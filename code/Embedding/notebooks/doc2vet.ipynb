{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "with open('../tokens', 'rb') as pickle_file:\n",
    "    tokenized_sent = pickle.load(pickle_file)\n",
    "\n",
    "random.shuffle(tokenized_sent)\n",
    "\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "test_tokens = tokenized_sent[:10000]\n",
    "train_tokens = tokenized_sent[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_train_data = [TaggedDocument(token[2], [i]) for i, token in enumerate(train_tokens)]\n",
    "tagged_test_data = [i[2] for i in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvector_size = Dimensionality of the feature vectors.\\nwindow = The maximum distance between the current and predicted word within a sentence.\\nmin_count = Ignores all words with total frequency lower than this.\\nalpha = The initial learning rate.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_train_data, vector_size = 50, min_count = 2, epochs = 30)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19637, 0.6353837251663208),\n",
       " (30930, 0.6341956853866577),\n",
       " (5478, 0.6312306523323059),\n",
       " (19140, 0.6294926404953003),\n",
       " (39313, 0.6060676574707031),\n",
       " (29072, 0.6038985848426819),\n",
       " (45961, 0.6007195115089417),\n",
       " (21924, 0.5802558064460754),\n",
       " (16527, 0.5774394869804382),\n",
       " (33303, 0.571479082107544)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = tagged_test_data[0]\n",
    "test_doc_vector = model.infer_vector(test_doc)\n",
    "model.dv.most_similar(positive = [test_doc_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (4628): «in this paper an improved design is presented to achieve a compact reconfigurable wideband antenna based on liquid crystal lc for millimeter wave mmw 5g networks the proposed design consists of two stacked patch antenna using aperture coupled feeding the lc is an anisotropic dielectric material used as substrate and it has a variable permittivity that can be controlled by a biasing voltage the dimensions of the designed stacked patch antenna are in mm the proposed antenna is suitable for radar satellite communications and for 5g applications such as theatres shopping malls convention centers and stadiums it operates at millimeter wave from to ghz with a bandwidth of ghz and a maximum antenna gain of db is achieved»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (22237, 0.8091264963150024): «a wideband hybrid dielectric antenna fitting into a low cost multilayer aip concept is proposed for 5g cellular handsets applications in this design a planar dielectric sheet with high permittivity and low loss is employed to achieve a low antenna profile and high radiation efficiency a rectangular metallic strip is introduced for bandwidth enhancement a antenna array is then further presented simulated results show that the array has an impedance bandwidth of 17 a peak gain of dbi a radiation efficiency of higher than 88 and a low profile of 0»\n",
      "\n",
      "MEDIAN (28308, 0.20951606333255768): «in this study we consider a hierarchically structured base station bs cellular system equipped with a backend distributed data storage in which nodes randomly arrive and depart the cell we numerically motivate and characterize the fundamental between the average repair bandwidth cost versus storage space where bs communication cost higher than that of local and link capacity constraints exist while the number of failed nodes can vary dynamically we establish the capacity region that is most relevant to 5g and beyond networks which are layered by design we hope that this study shall motivate novel regeneration code constructions that will be able to achieve the presented limits»\n",
      "\n",
      "LEAST (7946, -0.3257896900177002): «»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(tagged_test_data) - 1)\n",
    "inferred_vector = model.infer_vector(tagged_test_data[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(tagged_test_data[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(tagged_train_data[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "results = []\n",
    "for token in tokenized_sent:\n",
    "    class_token = token[1]\n",
    "    token_embedding = model.infer_vector(token[2])\n",
    "\n",
    "    row = np.append(token_embedding, [class_token])\n",
    "    results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('../dataframes/doc2vet_embedding.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
