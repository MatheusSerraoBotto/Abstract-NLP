id,title,abstract
9260345,Calculation of Chinese-Thai Cross-Language Similarity Based on Sentence Embedding,"In view of the poor accuracy, efficiency and scalability of the existing cross-language sentence similarity calculations, the Chinese- Thai cross-language sentence similarity is less studied. A new method is proposed. First, preprocess the corpus for Chinese-Thai parallel sentences, the sentence embedding model is used to obtain the Chinese-Thai sentence embedding matrix, and the sentence embedding is normalized. Then, the cross-language mapping model is used to embed the sentence. The conversion matrix is obtained through processing, and the orthogonal optimization of the conversion matrix is performed. Finally, the Chinese sentence embedding is mapped to the Thai sentence embedding space, and the Chinese- Thai cross-language sentence similarity is obtained by calculating the cosine of the two vectors, which provides a new idea for the Chinese-Thai cross-language sentence similarity calculation. The experimental results show that the method has good accuracy."
8611098,Jointly Learning Topics in Sentence Embedding for Document Summarization,"Summarization systems for various applications, such as opinion mining, online news services, and answering questions, have attracted increasing attention in recent years. These tasks are complicated, and a classic representation using bag-of-words does not adequately meet the comprehensive needs of applications that rely on sentence extraction. In this paper, we focus on representing sentences as continuous vectors as a basis for measuring relevance between user needs and candidate sentences in source documents. Embedding models based on distributed vector representations are often used in the summarization community because, through cosine similarity, they simplify sentence relevance when comparing two sentences or a sentence/query and a document. However, the vector-based embedding models do not typically account for the salience of a sentence, and this is a very necessary part of document summarization. To incorporate sentence salience, we developed a model, called CCTSenEmb, that learns latent discriminative Gaussian topics in the embedding space and extended the new framework by seamlessly incorporating both topic and sentence embedding into one summarization system. To facilitate the semantic coherence between sentences in the framework of prediction-based tasks for sentence embedding, the CCTSenEmb further considers the associations between neighboring sentences. As a result, this novel sentence embedding framework combines sentence representations, word-based content, and topic assignments to predict the representation of the next sentence. A series of experiments with the DUC datasets validate CCTSenEmb's efficacy in document summarization in a query-focused extraction-based setting and an unsupervised ILP-based setting."
8679121,Question Understanding Based on Sentence Embedding on Dialog Systems for Banking Service,"This paper introduce a question understanding system to respond appropriate answers in a dialog system for banking services. The question understanding system provides an automated response service in a specific domain (e.g. banking). This can increase response rate of a customer counseling service, and improve business efficiency and expertise. The question understanding system classify domains, specific categories, and speech acts of questions. Finally, the system analyze meanings and intents of the questions, and searching correct answers even various input sentences. In this paper, we describe methods of keyword tokenizing, pattern recognition, sentence embedding, analyzing dialogue intention, and searching similar FAQs. Through these methods, we have developed the question understanding unit in a real interactive system for financial services for real insurance companies and banks, and analyze the usefulness of the system through practical system implementation examples."
9749108,TA-SBERT: Token Attention Sentence-BERT for Improving Sentence Representation,"A sentence embedding vector can be obtained by connecting a global average pooling (GAP) to a pre-trained language model. The problem of such a sentence embedding vector using a GAP is that it is generated with the same weight for all words appearing in the sentence. We propose a novel sentence embedding-method-based model Token Attention-SentenceBERT (TA-SBERT) to address this problem. The rationale of TA-SBERT is to enhance the performance of sentence embedding by introducing three strategies. First, we convert the base form while preprocessing the input sentence to reduce misunderstanding. Second, we propose a novel Token Attention (TA) technique that distinguishes important words to produce more informative sentence vectors. Third, we increase stability of fine-tuning to avoid catastrophic forgetting by adding a reconstruction loss to the word embedding vector. Extensive ablation studies demonstrate that our TA-SBERT outperforms the original SentenceBERT (SBERT) in the sentence vector evaluation using semantic textual similarity (STS) tasks and the SentEval toolkit."
9071098,Empirical Study of Sentence Embeddings for English Sentences Quality Assessment,"Novel deep learning and machine translation techniques have greatly advanced the field of computational linguistics enabling us to find meaningful latent spaces for text analysis. While several embedding techniques exist for words, sentences, and entire documents, the potential applications are still being explored. In this paper we present the impact of top-performing sentence embedding methodologies on the accuracy of a neural model trained to assess the quality of English sentences. We focus our efforts in the methodologies called Language Agnostic SEntence Representation (LASER), Sentence to Vector (S2V), and Universal Sentence Encoder (USE) to observe their ability to capture information related to sentence quality. Our study suggests that these state-of-the-art sentence embeddings are unable to capture sufficient information regarding sentence correctness and quality in the English language."
8945681,News Title Classification Based on Sentence-LDA Model and Word Embedding,"Due to the severe data sparsity problem, conventional text classification methods are difficult to achieve good results in news title classification. In this paper, we design a novel news title classification method, referred as Word-Embedding-based Sentence-LDA (WESL) model. WESL employs the Sentence-LDA model to distill topic vectors from news titles, and FastText is used to learn word distributed representations. Furthermore, the proposed model expands the features of news titles by combining word and topic vectors. Finally, we utilize a Support Vector Machine (SVM) to classify news titles, and the classification results are evaluated using the precision, recall and F1-value. The experimental results illustrate that our method can significantly enhance classification performance."
9283681,An Improved Weighted-Removal Sentence Embedding Based Approach for Service Recommendation,"Currently, there is a large amount of information about user requirements and service in natural language. How to measure the semantic similarity between user requirements and service description is a critical issue in service recommendation and service solution construction. In this paper, we propose a service recommendation method based on the improved Weighted-Removal(WR) sentence embedding to solve the shortcomings of traditional information retrieval methods. After data preprocessing, we use the GloVe method to obtain the word vectors and use the improved WR sentence embedding method to obtain the sentence vectors. The similarity between the vectors can be better measured. The experimental results show that the proposed improved WR method is significantly better than the traditional methods in terms of recommendation accuracy, richness, and ranking."
9288330,Sentence Pair Similarity Modeling Based on Weighted Interaction of Multi-semantic Embedding Matrix,"In this paper, we focus on measuring the similarity of sentence pair. Noting that a single sentence vector may lose fine-grained semantic information which is important for sentence matching, we propose an embedding matrix to calculate a multi-granularity similarity matrix and find the true semantic alignment of two sentences. We also propose a semantic importance calculation and semantic decomposition that are simple but effective. The proposed model does not require any sparse features or external resources such as WordNet. Compared with other state-of-the-art models, we successfully train in a short time and achieve competitive results on similarity measurement and paraphrase identification tasks. Experiments and visual analysis show the good performance and interpretability of the model."
9660801,Clustering and Network Analysis for the Embedding Spaces of Sentences and Sub-Sentences,"Sentence embedding methods offer a powerful approach for working with short textual constructs or sequences of words. By representing sentences as dense numerical vectors, many natural language processing (NLP) applications have improved their performance. However, relatively little is understood about the latent structure of sentence embeddings. Specifically, research has not addressed whether the length and structure of sentences impact the sentence embedding space and topology. This paper reports research on a set of comprehensive clustering and network analyses targeting sentence and sub-sentence embedding spaces. Results show that one method generates the most clusterable embeddings. In general, the embeddings of span sub-sentences have better clustering properties than the original sentences. The results have implications for future sentence embedding models and applications."
9378337,Sent2Vec: A New Sentence Embedding Representation With Sentimental Semantic,"Text classification is considered as one of the primary task in many Natural Language Processing (NLP) applications. In industrial applications of NLP, sentimental analysis is a task to understand how satisfied a user is after receiving a service or buying a product. The traditional approach is to convert a text into a format of numeric vector before feeding into machine learning algorithm. This representation of a word refers to word embedding. However the traditional embedding methods often model the syntactic context of words but ignore the sentiment information of text [1]. This can impact on the accuracy of a classification model to predict the correct sentimental score for a text. In this paper, we present Sent2Vec, an alternative embedding representation that includes the sentimental semantic of a sentence in its embedding vector. We utilized the unsupervised Smoothed Inverse Frequency (uSIF) sentence embedding method in the Sent2Vec neural network over a multi million samples dataset. The new sentence embedding presented, can be used as features in downstream (un)supervised tasks, which also leads to better or comparable results compared to sophisticated methods. Furthermore, with a simple logistic regression classifier, Sent2Vec reaches competitive performance to state-of-the-art results on several datasets when combined with GloVe(6B)."
9164354,The Acquisition of Khmer-Chinese Parallel Sentence Pairs From Comparable Corpus Based on Manhattan-BiGRU Model,"Bilingual parallel sentence pairs are an important resource for cross-lingual processing. Aiming at the shortage of Khmer-Chinese parallel texts and the complex structure of research methods, a method of obtaining Khmer-Chinese parallel sentence pairs from comparable corpus based on Manhattan-BiGRU model is proposed. In this method, feature information and bilingual word embedding are concatenated together and then jointly used as input. Then the word embedding is coded into sentence embedding through the BiGRU network. Finally, the similarity of sentence embedding is calculated according to the Manhattan distance algorithm, it achieves the acquisition of parallel sentence pairs. Compared with other methods of obtaining parallel sentence pairs based on neural network model, the experimental results show that this method achieves good results, in which the accuracy of obtaining parallel sentence pairs reaches 82.2% and the recall rate reaches 70.7%."
9249247,A Novel Sentence Embedding Based Topic Detection Method for Microblogs,"Topic detection is a difficult challenging task, especially when the exact number of topics is unknown. In this article, we present a novel topic detection approach based on neural computing to detect topics in a microblogging dataset. We use an unsupervised neural sentence embedding model to map blogs to an embedding space. The proposed model is a weighted power mean sentence embedding model in which weights are calculated by a targeted attention mechanism. The experimental results show that our embedding model performs better than baseline in sentence clustering. In addition, we propose a clustering algorithm, referred to as Relationship-Aware DBSCAN (RADBSCAN), to discover topics from a microblogging dataset in which the number of topics is automatically determined by the characteristics of the dataset. Moreover, to provide parameter insensibility, we use the forwarding relationship in the blogs as a bridge of two independent clusters. Finally, we validate the proposed method on a dataset from the Sina microblog. The results show that our approach can detect all topics successfully and can extract the keywords of each topic."
8683085,Beyond Word-level to Sentence-level Sentiment Analysis for Financial Reports,"This paper attempts to conduct a sentence-level sentiment analysis with respect to financial risk on a collection of financial reports. Specifically, we first propose a simple yet efficient algorithm to generate financial sentiment phrases (senti-phrases), and then with the obtained senti-phrases, we utilize multiple sentence embedding models for better learning the representations of financial risk sentences. In order to verify the performance of the proposed approach, we conduct a risk classification task of financial sentences on a sentence-level labeled dataset of finance reports. Experimental results show that incorporating the obtained senti-phrases into the embedding-based models improves the classification performance."
9074312,A Comparative Sentiment Analysis Of Sentence Embedding Using Machine Learning Techniques,"Analyzing sentiment is a process to identify the opinion of a text. It is also known as opinion mining or emotion Artificial Intelligence (AI). People post comments in social media mentioning their experience about an event and are also interested to know if the majority of other people had a positive or negative experience on the same event. This classification can be achieved using Sentiment Analysis. Sentiment analysis takes unstructured text comments about a product reviews, an event, etc., from all comments posted by different users and classifies the comments into different categories as either positive or negative or neutral opinion. This is also known as polarity classification. Sentimental analysis can be performed by Text analysis and computational linguistics. This work aims at comparing the performance of different machine learning algorithms in performing sentiment analysis of Twitter data. The proposed method uses term frequency to find the sentiment polarity of the sentence. The performance of Multinomial Naive Bayes, SVM and Logistic regression algorithms in sentence classification were compared. From the results, it is inferred that logistic regression has achieved a greatest accuracy when it is used with n-gram and bigram model."
9230979,Sentence Generation using LSTM Based Deep Learning,"Sentence generation serves the process of predicting relevant words in a specific sequence. The purpose of this research is to come up with a method for generating sentences while maintaining proper grammatical structure. Here, we have implemented a sentence generation system based on Long Short-Term Memory (LSTM) architecture. Our system generally follows the basics of word embedding where words from the dataset get tokenized and turned into vector forms. These vectors are then processed and passed through a Long Short-Term Memory layer. Successive words get generated from the system after each iteration. This process winds up generating relevant words to form a sentence or a passage. The results of the system are pretty convincing compared to different existing methods."
8944630,SSE: Semantic Sentence Embedding for learning user interactions,"Semantics is an integral component in NLP. Semantics provides a meaningful view about the meaning of the language. The meaning of the text is susceptible to the negative words present in them. Thus the impact of semantics in NLP in turn increases the impact of negative words in the sentence. It plays crucial role in shaping the semantics of any sentence. Although all the sub-areas of natural language processing try to find out the impact of negative words on their implementation, the effect of these words are not considered in the word embedding process. Being the foremost step in NLP the omission of these words in embedding can affect the representation of the language. The aim is to propose a model which produces word embeddings by considering the negative words. Although most NLP implementing systems consider all negative words this paper focuses on the word `not' which is followed by an adjective. The antonym for the adjective is identified and is then replaced in the corpus. The model is achieved by replacing the corpus where negative words and this corpus is fed to a three layer neural network to form the embeddings. The proposed technique proves an accuracy of 93.6% in retrieving the relevant answer to the user."
9140343,SBERT-WK: A Sentence Embedding Method by Dissecting BERT-Based Word Models,"Sentence embedding is an important research topic in natural language processing (NLP) since it can transfer knowledge to downstream tasks. Meanwhile, a contextualized word representation, called BERT, achieves the state-of-the-art performance in quite a few NLP tasks. Yet, it is an open problem to generate a high quality sentence representation from BERT-based word models. It was shown in previous study that different layers of BERT capture different linguistic properties. This allows us to fuse information across layers to find better sentence representations. In this work, we study the layer-wise pattern of the word representation of deep contextualized models. Then, we propose a new sentence embedding method by dissecting BERT-based word models through geometric analysis of the space spanned by the word representation. It is called the SBERT-WK method. No further training is required in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and downstream supervised tasks. Furthermore, ten sentence-level probing tasks are presented for detailed linguistic analysis. Experiments show that SBERT-WK achieves the state-of-the-art performance. Our codes are publicly available."
9609125,Duplicate Bug Report Detection by Using Sentence Embedding and Fine-tuning,"Industrial software maintenance devotes much time and effort to find duplicate bug reports. In this paper, we propose an automated duplicate bug report detection system to improve software maintenance efficiency. Our system detects duplicate reports by vectorizing the contents of each report item by deep-learning-based sentence embedding and calculating the similarity of the whole report from those of the item vectors. The Sentence-BERT fine-tuned with report texts is used for sentence embedding. Finally, we verify that the combination of processing separately by item and Sentence-BERT fine-tuned with reports effectively detects duplicate bug reports in industrial experiments that compare the performance of existing methods."
9534215,Exploring Sentence Embedding Structures for Semantic Relation Extraction,"Sentence embeddings encode natural language sentences as low-dimensional, dense vectors and have improved NLP tasks, including relation extraction, which aims at identifying structured relations defined in a knowledge base from unstructured text. A promising and more efficient approach would be to embed both the text and structured knowledge in low-dimensional spaces and discover alignments between them. We develop such an alignment procedure and evaluate the extent to which sentences carrying similar senses are embedded in close proximity sub-spaces, using that structure to align them to a knowledge graph. Our experimental results show that embedding spaces generated from simple models outperform those from more complicated approaches for the alignment and relation extraction task. We also show that clusterability can serve as a proxy for alignment accuracy, leading us to conclude that better structured spaces drive better semantic applications."
9103797,Sentence Embedding Model Based on Feature Selection,"The method of calculating the word vector using the neural network method provides the motivation for generating the representation model of the sentence. For the smooth inverse frequency sentence vector model, only the word frequency information on the general data set is considered to calculate the word weight, but when it is specific to the task, the problem that the different words contribute differently to the task and its weight correction is not considered. According to the distribution of characteristic words in different categories in the dataset, the task contribution factor (TCF) is proposed by using the improved information gain feature selection method. Based on this factor, a sentence vector representation model (IIG-SIF) based on task contribution is proposed. By testing on the standard text classification dataset 20 Newsgroups and the text similarity calculation dataset SICK, the IIG-SIF model has a greater improvement in the two tasks of text categorization and Text similarity calculation than the original SIF model."
9054274,Self-Attentive Sentimental Sentence Embedding for Sentiment Analysis,"We propose the use of a word-level sentiment bidirectional LSTM in tandem with the self-attention mechanism for sentence-level sentiment prediction. In addition to the proposed model, we also present a finance report dataset for sentence-level financial risk detection. Experiments conducted on the proposed dataset together with two public review datasets attest the effectiveness of our model for sentence sentiment prediction."
9345618,A New Sentence Ordering Method using BERT Pretrained Model,"Building systems with capability of natural language understanding (NLU) has been one of the oldest areas of AI. An essential component of NLU is to detect logical succession of events contained in a text. The task of sentence ordering is proposed to learn succession of events with applications in AI tasks. The performance of previous works employing statistical methods is poor, while the neural networks-based approaches are in serious need of large corpora for model learning. In this paper, we propose a method for sentence ordering which does not need a training phase and consequently a large corpus for learning. To this end, we generate sentence embedding using BERT pre-trained model and measure sentence similarity using cosine similarity score. We suggest this score as an indicator of sequential events' level of coherence. We finally sort the sentences through brute-force search to maximize overall similarities of the sequenced sentences. Our proposed method outperformed other baselines on ROCStories, a corpus of 5-sentence human-made stories. The method is specifically more efficient than neural network-based methods when no huge corpus is available. Among other advantages of this method are its interpretability and needlessness to linguistic knowledge."
9102716,Phrase-Level Global-Local Hybrid Model For Sentence Embedding,"Latent structure models have drawn much attention due to the ability to learn an optimal latent hierarchical structures without explicit structure annotations. However, most existing models suffer from high computation complexity and hard training. To this end, this paper proposes a novel phrase-level global-local hybrid model, which inherits the advantages of existing latent structure models while requires less time complexity. Our model splits a sentence into multiple phrases by a category-selection module. Then, it encodes the context dependency by a phrase-level global encoding module, and encodes the task-specific information by a phrase-level local encoding module. Finally, sentence embedding is obtained by integrating the global encoding and task-specific encoding. Experiments on public benchmarks show that, our model achieves state-of-the-art performance on the tasks of sentence classification and natural language inference. Meanwhile, our model is at least 10 times faster than existing state-of-the-art method at the training stage."
8642425,An Efficient Framework for Sentence Similarity Modeling,"Sentence similarity modeling lies at the core of many natural language processing applications, and thus has received much attention. Owing to the success of word embeddings, recently, popular neural network methods achieved sentence embedding. Most of them focused on learning semantic information and modeling it as a continuous vector, yet the syntactic information of sentences has not been fully exploited. On the other hand, prior works have shown the benefits of structured trees that include syntactic information, while few methods in this branch utilized the advantages of word embeddings and another powerful technique-attention weight mechanism. This paper suggests to absorb their advantages by merging these techniques in a unified structure, dubbed as attention constituency vector tree (ACVT). Meanwhile, this paper develops a new tree kernel, known as ACVT kernel, which is tailored for sentence similarity measure based on the proposed structure. The experimental results, based on 19 widely used semantic textual similarity datasets, demonstrate that our model is effective and competitive, when compared against state-of-the-art models. Additionally, the experimental results validate that many attention weight mechanisms and word embedding techniques can be seamlessly integrated into our model, demonstrating the robustness and universality of our model."
9688008,Automatic Exam Answer Checker using Optical Character Recognition and Sentence Embedding,"The old-fashioned manual system of answer script correction is tiresome and time-consuming. Human resources are consumed and wasted. The automatic exam answer checker evaluates the answers and efficiently allocates the marks. Exam management authorities have access to post questions and provide an answer key based on which the system evaluates solutions. The system uses several technologies such as Cloud computing, Optical Character Recognition, and Web Development and Hosting. The handwritten answers are uploaded, or typed solutions are sent to the system. The responses are evaluated according to the answer key provided by the exam authorities. Solutions are uploaded on a webpage where students can login using unique credentials and view their results. The system is user friendly and efficient"
8806506,The Impact of Sentence Embeddings in Turkish Paraphrase Detection,"In recent studies, it is shown that word embeddings achieve in several natural language processing (NLP) tasks. Though paraphrase identification in Turkish is well-studied by traditional statistical NLP methods, to the best of our knowledge there exists no study where word and/or sentence embeddings are employed. In this paper, three methods, which are well-known as “using average vector for word embeddings” (AWE), “concatenated vectors for word embeddings” (CWE) and “word mover's distance word embeddings” (WMDWE) to build sentence embeddings from word embeddings are examined and their effect in performance of paraphrase identification is measured. The results are presented comparatively for English (MSRP) and Turkish (PARDER and TuPC) paraphrase corpora. The study doesn't cover the optimization of parameters used in training of word embeddings and also the features specific to Turkish langauge are not considered. Despite this naive approach, the test results obtained from PARDER corpus are inspiring that a more detailed study that involves such improvements may result with more convincing performance values."
8901317,An Improved Adaptive and Structured Sentence Embedding,"Recently, attention mechanism has aroused great interest in various fields of Natural Language Processing (NLP). In this paper, we propose a new model for extracting an interpretable sentence embedding by introducing an ""Adaptive self-attention"". Instead of using a vector, we use a 2-D matrix to represent the embedding and each valid row of the matrix represents a part of sentence. In addition, a length hierarchy mechanism with a unique loss function is applied to adaptively adjust the number of the valid rows of the matrix, which can solve the problem of attention redundancy in short sentences and lack of attention in long sentences. We evaluate our model on text classification tasks: news categorization, review categorization and opinion classification. The results show that our model, compared with other sentence embedding methods, achieve significant improvement in terms of performance when there exists a large amount of data and the length of the data is evenly distributed."
9412102,Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks,"Contextualized representations from a pre-trained language model are central to achieve a high performance on downstream NLP task. The pre-trained BERT and A Lite BERT (ALBERT) models can be fine-tuned to give state-of-the-art results in sentence-pair regressions such as semantic textual similarity (STS) and natural language inference (NLI). Although BERT-based models yield the [CLS] token vector as a reasonable sentence embedding, the search for an optimal sentence embedding scheme remains an active research area in computational linguistics. This paper explores on sentence embedding models for BERT and ALBERT. In particular, we take a modified BERT network with siamese and triplet network structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to create Sentence-ALBERT (SALBERT). We also experiment with an outer CNN sentence-embedding network for SBERT and SALBERT. We evaluate performances of all sentence-embedding models considered using the STS and NLI datasets. The empirical results indicate that our CNN architecture improves ALBERT models substantially more than BERT models for STS benchmark. Despite significantly fewer model parameters, ALBERT sentence embedding is highly competitive to BERT in downstream NLP evaluations."
9382471,Extraction of Question-related Sentences for Reading Comprehension Tests via Attention Mechanism,"Attention mechanism is now being widely used in machine translation, image segmentation, and many other neural network-based applications. In the attention mechanism, there is an index called ""attention score"" which could reflect the relevance degrees between the output tokens with the input tokens. In this paper, we proposed a method using the attention score in extracting the question-related sentences from the articles of reading comprehension tests. We investigated and compared the accuracies of the extracted sentences when in word-level embedding, sentence-level embedding, and using the traditional cosine similarity. Through the evaluation of the randomly selected results of the extracted sentences, the method using the attention mechanism with sentence-level embedding obtained the accuracy of 88.3%, which was 80.9% when using word-level embedding and 83.5% when using the general cosine similarity. Meanwhile, the sentence-level embedding method also obtained the highest precision and recall in the three methods. The results suggest that using the attention mechanism with sentence-level embedding could extract the question-related sentences more accurately than traditional cosine similarity."
8695382,ES-ESens: Detection of Event Sentences Based on Evaluation of the Explicitness and Significance of Information,"Extracting event sentences with explicit and significant information is a basic work of semantic analysis based retrieval of text information. Current methods of event extraction normally lack evaluation of the quality of information embedded in a sentence, thus the extracted event sentences usually contain inexplicit or insignificant information that is unusable for real-world applications. In this paper, we introduce ES-ESens, a deep learning based methodology for detecting event sentences with high-quality information. To evaluate the explicitness and significance of the information embedded in a sentence, ES-ESens adopts Recurrent Neural Network with attention mechanism to perform deep semantic analysis on the context of the sentence and the article that explains the background of the event. Based on datasets of practical applications, experiments are presented to show the performance of our methodology."
9412169,Efficient Sentence Embedding via Semantic Subspace Analysis,"A novel sentence embedding method built upon semantic subspace analysis, called semantic subspace sentence embedding (S3E), is proposed in this work. Given the fact that word embeddings can capture semantic relationship while semantically similar words tend to form semantic groups in a high-dimensional embedding space, we develop a sentence representation scheme by analyzing semantic subspaces of its constituent words. Specifically, we construct a sentence model from two aspects. First, we represent words that lie in the same semantic group using the intra-group descriptor. Second, we characterize the interaction between multiple semantic groups with the inter-group descriptor. The proposed S3E method is evaluated on both textual similarity tasks and supervised tasks. Experimental results show that it offers comparable or better performance than the state-of-the-art. The complexity of our S3E method is also much lower than other parameterized models."
9170315,A Sentence-RCNN embedding model for Knowledge Graph Completion,"Large-scale knowledge graphs are structured to represent real world facts, but they are far from completeness. A number of completion methods have been developed to fill missing facts. In this paper, a novel Sentence-RCNN embedding model is proposed for knowledge graph completion. This model represents knowledge facts as sentences, so that it can precisely capture long-term dependencies, local structure information and translational features simultaneously. In addition, we propose a new method to construct negative samples (CNS), which greatly reduces the number of false negative samples used in model training stage. The proposed model was validated by two challenging benchmark datasets without any extra information. Results showed Sentence-RCNN model had fairly good accuracy, robustness and convergence than the state-of-the-art embedding models. It improved MRR and H©N metrics by an average of over 11.8%, and obtained extra 5.4% improvement with CNS method."
9446068,Korean Erroneous Sentence Classification With Integrated Eojeol Embedding,"This paper attempts to analyze the Korean sentence classification system. Sentence classification is the task of classifying an input sentence based on predefined categories. However, spelling or space error contained in the input sentence causes problems in morphological analysis and tokenization. This paper proposes a novel approach of Integrated Eojeol (Korean syntactic word separated by space) Embedding to reduce the effect of poorly analyzed morphemes on sentence classification. The paper also proposes two noise insertion methods that further improve classification performance. Our evaluation results indicate that by applying the proposed methods on the existing sentence classifiers, the sentence classification accuracy on erroneous sentences is increased by 8% to 15%."
9582605,Predicting Software Defect Severity Level using Sentence Embedding and Ensemble Learning,"Bug tracking is one of the prominent activities during the maintenance phase of software development. The severity of the bug acts as a key indicator of its criticality and impact towards planning evolution and maintenance of various types of software products. This indicator measures how negatively the bug may affect the system functionality. This helps in determining how quickly the development teams need to address the bug for successful execution of the software system. Due to a large number of bugs reported every day, the developers find it really difficult to assign the severity level to bugs accurately. Assigning incorrect severity level results in delaying the bug resolution process. Thus automated systems were developed which will assign a severity level using various machine learning techniques. In this work, five different types of sentence embedding techniques have been applied on bugs description to convert the description comments to an n-dimensional vector. These computed vectors are used as an input of the software defect severity level prediction models and ensemble techniques like Bagging, Random Forest classifier, Extra Trees classifier, AdaBoost and Gradient Boosting have been used to train these models. We have also considered different variants of the Synthetic Minority Oversampling Technique (SMOTE) to handle the class imbalance problem as the considered datasets are not evenly distributed. The experimental results on six projects highlight that the usage of sentence embedding, ensemble techniques, and different variants of SMOTE techniques helps in improving the predictive ability of defect severity level prediction models."
8683367,Investigating the Effects of Word Substitution Errors on Sentence Embeddings,"A key initial step in several natural language processing (NLP) tasks involves embedding phrases of text to vectors of real numbers that preserve semantic meaning. To that end, several methods have been recently proposed with impressive results on semantic similarity tasks. However, all of these approaches assume that perfect transcripts are available when generating the embeddings. While this is a reasonable assumption for analysis of written text, it is limiting for analysis of transcribed text. In this paper we investigate the effects of word substitution errors, such as those coming from automatic speech recognition errors (ASR), on several state-of-the-art sentence embedding methods. To do this, we propose a new simulator that allows the experimenter to induce ASR-plausible word substitution errors in a corpus at a desired word error rate. We use this simulator to evaluate the robustness of several sentence embedding methods. Our results show that pre-trained neural sentence encoders are both robust to ASR errors and perform well on textual similarity tasks after errors are introduced. Meanwhile, unweighted averages of word vectors perform well with perfect transcriptions, but their performance degrades rapidly on textual similarity tasks for text with word substitution errors."
9724806,An Effective Crowdsourced Test Report Clustering Model Based on Sentence Embedding,"In the crowdsourced testing industry, efficient and automated classification of true bugs from test reports can greatly reduce the cost of software testing. Most of the existing methods are based on TF-IDF or machine learning methods to vectorize the test report and then construct a classifier. However, the document vector constructed by keywords more or less ignores the description information in the document, which affects the performance of classification and detection of real defects. In order to use the description information to construct an effective report clustering model, we propose a model called RCSE to encode test report description information at the sentence level, calculate the similarity between the test reports from the feature similarity of the description sentence, then cluster the test report. We evaluated the model on 3,442 reports. The experimental results show that the clustering model based on sentence embedding has an average purity of 12.3 % and an ARI of 22.0% higher than the keyword-based model on three report datasets."
9564148,Semantic Representation of Sentences Employing an Automated Threshold,"Semantic representations of sentences have become a challenging task for many fields of natural language processing. To represent the sentence, the semantic distributional representations of its words are being used. In this approach, all the features of the words are used to represent the sentence. The features of the words play a major role for all tasks including semantic representation of the sentence for the purpose of measuring semantic textual similarity (STS). In this paper, we study that all the features of the words are not necessary for semantic representation of the sentence to estimate the STS. Pre-trained word-embedding and BERT (Bidirectional Encoder Representation from Transformers) models are employed to get the semantic distribution of the words. An automated threshold is applied to the feature vectors of the words to get the sentence representation. Finally, the cosine similarity is employed on the sentence vectors to measure their STS. To validate the performance of our proposed approach, a wide range of experiments are carried out on four benchmark STS datasets. The results of the experiments concluded that our proposed approach is capable of semantic representation of the sentence and also outperforms some state-of-the-art methods."
9671614,Mufin: Enriching Semantic Understanding of Sentence Embedding using Dual Tune Framework,"With the advancements of Natural Language Understanding (NLU), diverse industrial applications like user intent classification, smart chatbots, sentiment analysis and question answering have be-come a primary paradigm. Transformers-based multi-lingual language models such as XLM have performed significantly well in diverse semantic understanding and classification tasks. However, fine-tuning such large pre-trained architectures is resource and compute intensive, limiting its wide adoption in enterprise environments.We present a novel efficient and light-weight frame-work based on sentence embeddings to obtain enhanced multi-lingual text representations for domain-specific NLU applications. Our framework combines the concepts of up-projection, alignment and meta-embeddings enhancing the textual semantic similarity knowledge of smaller sentence embedding architectures. Extensive experiments on diverse cross-lingual classification tasks showcase the proposed framework to be comparable to state-of-the-art large language models (in mono-lingual and zero-shot settings), even with lesser training and resource requirements."
9051342,Sentence-SFV Model: Make Full Use of Word Embedding Actively,"Word Embedding is a method that can learn and capture the grammatical and semantic features contained in the corpus through unsupervised training. Because of this superior characteristic, it is very common to use word vectors as inputs in the downstream tasks. However, little attention has been paid to the active mining of semantic information carried by word vectors. Therefore, we propose a Sentence based Scenario Feature Vector (Sentence-SFV) Model to represent the scenario related grammatical features of sentences quantitively in vector form. With the help of Sentence-SFV model, we extract scenario related features of different corpora and project them into a 3D feature space for visual analysis, by which means we demonstrate the interesting ability of our model like distinguishing grammatical features and explaining grammatical differences between different corpora. We also discuss on the methods to further improve our model, and verify the improvement effect by apply optimized SFV in CNN based sentiment classification experiments."
8824961,Implementing a Real-Time Image Captioning Service for Scene Identification Using Embedded System,"This work aims to implement a real-time scene identification system using an image captioning model on an embedded system. The image captioning model can translate the image captured by a webcam installed on the embedded system into a human-readable sentence immediately. Users can get the information quickly by reading only the sentences. There are two stages in the image captioning model. First, a deep neural network extracts features from images captured from the webcam. Second, a long-short term memory generates the corresponding sentence. Due to the portability of the embedded system, our scene identification system can be placed anywhere at home or in the company. We evaluate the execution time in different aspects on several embedded systems and demonstrate the generated sentences from the captured images by our scene identification system."
9698769,Third Order Polynomial Derived Sentence Embedding,"In this study, we adopt third order polynomials to derive sentence embeddings for Internet of Things (IoT) pattern definitions. The aim was to use pre-identified (or baseline) IoT pattern definitions, and use third order polynomial regression to embed similar sentences to these definitions in vector spaces. While artificial neural networks (ARRs) have previously been shown to be effective in producing sentence embeddings, such embeddings are prone to multicollinearity. Generally speaking, in ARRs, the problem of multicollinearity becomes worse with each successive layer. This, in turn, causes convergence issues across all known sentence embedding techniques based on ARRs training. These issues are evident in some of the well-known ARRs based sentence embedding techniques such as Doc2Vec, InferSent, Universal Sentence Encoder, and SentenceBERT. We do not claim that polynomial regression is better suited than artificial neural networks for sentence embedding tasks, but rather that artificial neural networks (ANNs) are, in essence, polynomial regression. Thus, we treat the generation of an analytic function used with an analytic activation as a polynomial. In this way, the core problem in this context then becomes the searching for the coefficients of the polynomial, which is essentially a regression problem. In our model, similar sentences are approximated around an operating point (baseline vector) by third order polynomials (3OPs). In other words, we used 3OPs derived from the ANN model. The resulting 3OPs derived sentence embeddings show an accuracy of 83.33% which is almost on par with the 83.7% achieved by the prior state-of-the-art on Feng et al. [1]."
9675278,Negative Siamese Network for Classifying Semantically Similar Sentences,"In semantically similar sentence-level classification, BERT often neglects the subtle semantic differences in sentences and performs poorly because of its inherent anisotropy regarding semantic space. This paper explores methods to enlarge the space distance between labels assigned to different sentences. First, the sentence embedding is transferred into the isotropic space through a message passing neural network (MPNN). Combined with structure encoding, semantic encoding can alleviate the anisotropy character of BERT. In addition, we propose the Negative Siamese Network (NSN) to analyze hard negative examples. The NSN focuses on widening the distance between hard negative samples and positive samples. After fine-tuning the NSN model, we conduct two tasks on four standard datasets. Experiment results show that our model achieves better performance than state-of-the-art models in capturing the subtle differences between semantically similar sentences."
9263191,Vector-to-Sequence Models for Sentence Analogies,"We solve sentence analogies by generating the solution rather than identifying the best candidate from a given set of candidates, as usually done. We design a decoder to transform sentence embedding vectors back into sequences of words. To generate the vector representations of answer sentences, we build a linear regression network which learns the mapping between the distribution of known and expected vectors. We subsequently leverage this pre-trained decoder to decode sentences from regressed vectors. The results of experiments conducted on a set of semantico-formal sentence analogies show that our proposed solution performs better than a state-of-the-art baseline vector offset method which solves analogies using embeddings."
9661426,A Simple Yet Robust Algorithm for Automatic Extraction of Parallel Sentences: A Case Study on Arabic-English Wikipedia Articles,"Parallel corpora are vital components in several applications of Natural Language Processing (NLP), particularly in machine translation. In this paper, we present a novel method for automatically creating parallel sentences from comparable corpora. The method requires a bilingual dictionary as well as an adequate word-vectorisation method. We use Arabic and English Wikipedia as a comparable corpus to apply our proposed method and construct a parallel corpus between Arabic and English. The created Arabic-English corpus consists of 105,010 parallel sentences with a total number of 4.6M words. During our study, we compared two methods of word vectorisation, word embedding and term frequency-inverse document frequency, in terms of their usefulness in computing similarities between well-formed and syntactically ill-formed sentences. We also quantitatively and qualitatively examined the parallel corpus produced by our proposed method and compared it with other available Arabic-English parallel corpora counterparts: GlobalVoices, TED, and Wiki-OPUS. We explored the main advantages and shortcomings of these corpora when used for NLP applications, such as word semantic similarity identification and Neural Machine Translation (NMT). The word semantic similarity models trained on our parallel corpus outperformed models trained on other corpora in the task of English non-similar word identification. Our parallel corpus also proved competitive when building Arabic-English NMT systems, yielding results comparable to those of the automatically created Wiki-OPUS corpus and of the manually created TED corpus, while achieving results superior to the smaller GlobalVoices corpus."
9673426,Web Service Clustering Technique based on Contextual Word Embedding for Service Representation,"Due to extensive use of Internet and IoT, the demand Web services and APIs are increasing day by day and there is a proliferation of services on internet in terms of quality and quantity both. It raises the need of service management. Web service clustering plays a vital role in service management as it reduces the search space and time. Word2vec word embedding is highly demanded in these days as it can capture the semantic similarity but it does not bother the context and it effects the clustering performance. In this paper, Sentence-BERT (Sentence Bidirectional Encoder Representations from Transformers) embedding is used in the vector space representation of services so that with the semantic meaning, context of the features can be also analyzed and services can be efficiently mapped in vector space. To analyze the performance of embedding, K-Means clustering is applied and results are compared with the different state-of-art techniques based on standard evaluation measures. The experimental results shows that accuracy of the proposed model is increased by approximately 49% in comparison of a model in which word2vec model is utilized."
8802975,Augmented Visual-Semantic Embeddings for Image and Sentence Matching,"The task of image and sentence matching has witnessed significant progress recently, but it is still challenging arising from the tremendous semantic gap between a pixel-level image and its matched sentences. Due to limited training data, it is rather challenging to optimize the visual-semantic embeddings. In this work, we propose to augment visual-semantic embeddings via enlarging the training dataset. With more data, models can learn discriminative features with high-quality semantic concepts. More specifically, we augment data by generating sentences for given images. Our method consists of two steps. At first, to enlarge the training dataset, given an image, we perform image captioning. Instead of introducing redundancy to our augmented dataset, we hope that our generated sentences are in diverse style and maintain its fidelity at the same time. Therefore, we consult to generative adversarial networks (GANs) which can produce more flexible expressions compared to methods based on the maximum likelihood principle. Then, we augment visual-semantic embeddings with the augmented training dataset and obtain the model for the task of image and sentence matching. Experiments on the popular benchmark demonstrate the effectiveness of our method by achieving superior results compared to our baseline."
9660657,Critical Sentence Identification in Legal Cases Using Multi-Class Classification,"Inherently, the legal domain contains a vast amount of data in text format. Therefore it requires the application of Natural Language Processing (NLP) to cater to the analytically demanding needs of the domain. The advancement of NLP is spreading through various domains, such as the legal domain, in forms of practical applications and academic research. Identifying critical sentences, facts and arguments in a legal case is a tedious task for legal professionals. In this research we explore the usage of sentence embeddings for multi-class classification to identify critical sentences in a legal case, in the perspective of the main parties present in the case. In addition, a task-specific loss function is defined in order to improve the accuracy restricted by the straightforward use of categorical cross entropy loss."
9545937,Chinese short text classification method based on word embedding and Long Short-Term Memory Neural Network,"The value of Internet short texts is increasingly prominent, and traditional classification methods cannot be applied to short texts with weak feature expression. In this regard, this paper proposes a Chinese Short Text Classification method based on Word Embedding and LSTM with feature enhancement (hereinafter called CSTCFE-WE-LSTM). This method uses word embedding learned from Wikipedia corpus as initial features for the model, and then uses category factors and TF-IDF to generate weights to enhance features. Finally, it uses a 6-layer neural network for classification, which includes a word embedding layer, two LSTM layers, a Dropout layer, and two fully connected layers. In order to verify the method CSTCFE-WE-LSTM, we collected short text sets on 3 topics, and reached following conclusions: 1. In the best model, P, F and other indicators are better than the classifier of using Wikipedia word embedding and KNN. 2. For texts with sentences average length less than 10 words, the effect of two-layer LSTM is better than that of single-layer LSTM, and the effect is better when the number of single-layer nodes is 50. 3. The effect of feature enhancement is better for the health category than the commercial category."
9533903,Semantic Extraction for Sentence Representation via Reinforcement Learning,"Many modern Natural Language Processing(NLP) systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Several attempts of learning unsupervised sentence representations have not achieved satisfactory performance and have not been widely adopted. In this work, we present a Semantic Extraction Reinforcement learning Model(SERM) for encoding sentences into embedding vectors, which transforms the learning process into intent detection and named entity recognition tasks. Intent detection is mainly related to the semantics of the whole sentence, while named entity recognition pays more attention to local entities. Given a sentence, SERM builds a sentence representation by extracting the most important words and removing irrelevant words in a sentence. Unlike the mainstream approach, SERM compresses sentences from semantic and entity perspectives and this allows us to efficiently learn different types of encoding functions. The experimental results show that SERM can learn high quality sentence representation. This paper demonstrates that our sentence representations have sufficient competitiveness with the best performing model on text classification task."
9390614,Person-Job Fit model based on sentence-level representation and theme-word graph,"Previous studies on Person-Job Fit mainly focused on the use of large amounts of data to train word-level embedding to model with the Joint Feature Extractor. Therefore, in order to reduce the computational load and text truncation problems caused by word-level representation and better integrate the idea of Click-through Rate prediction, a Person-Job Fit model based on sentence vector and theme-words knowledge graph (KG-DPJF) is proposed. The model uses a Bi-directional Long Short-term Memory Neural Network to extract theme-words from long text of recruitment resume and requirements and construct a knowledge graph. The model uses the Bert pre-training model to code the input sentences, and uses a multi-level attention mechanism to calculate the correlation among candidate resumes, historical resumes and recruitment requirements features, then inputs the correlation and weighted output to the classifier to predict the matching degree. Finally, the experiment is tested in the industry data set, and the KG-DPJF method achieves nearly 9% and 5% performance improvement compared with method based on logistic regression and single convolution neural network feature extraction method, and shortens training time by about four times compared with word-level embedding method."
9739828,SBSim: A Sentence-BERT Similarity-Based Evaluation Metric for Indian Language Neural Machine Translation Systems,"Machine translation (MT) outputs are widely scored using automatic evaluation metrics and human evaluation scores. The automatic evaluation metrics are expected to be easily computable and a reflection of human evaluation. Traditional string-based metrics such as BLEU, ChrF++ scores, are widely used to evaluate MT systems, but fail to account for synonyms that appear in the state-of-the-art neural machine translation (NMT) systems, owing to their inability to evaluate paraphrases. While similarity-based metrics such as Yisi, BERTScore address this issue, these metrics need to be modified to better evaluate morphologically rich Indian languages such as, Tamil and Hindi. The current work proposes a novel and individual sentence-BERT based similarity (SBSim) metric, that makes use of a paraphrase-BERT model and sentence-level embedding to evaluate NMT outputs. The effectiveness of the BLEU, ChrF++, Yisi, BERTScore, and the proposed SBSim are evaluated on English-to-Tamil and English-to-Hindi NMT outputs. The sentence-level metric correlation of the proposed SBSim metric with respect to human scores is observed to outperform the existing metrics with a correlation of 0.9123 and 0.9052 for English-to-Tamil and English-to-Hindi NMT systems, respectively. Further, the average metric correlation of the SBSim metric is also observed to be the highest with a value of 0.9801 and 0.9836 for these NMT systems, respectively. The proposed metric is also evaluated on WMT2020 dataset and reports the highest correlation of 0.7129 with the human scores."
9073750,Emotional State Estimation by Dialogue History and Sentence Distributed Representation,"The paper presents a novel approach to predict human's emotion in the dialogue. We target the scenario dialogue corpora. The corpora are annotated by some subjects with emotion tag. We use sentence embedding by Bidirectional Encoder Representations from Transformers as versatile and rich feature. By acquiring multi-context information from dialogue text, the accuracy of emotion prediction can be improved. As an experiment by machine learning algorithm such as deep neural networks for the proposed method, we achieved higher accuracy than the method using embedding feature from utterance itself."
9382430,A Method to Extract Knowledge Explanation Sentences from Conversations in Comics with Combination of Contents and Expressions,"Comics are entertainment contents that conversations of characters drive stories. Since people can obtain new knowledge from conversations in real lives, the conversations in comics could be useful to acquire new knowledge. The conversations in comics contain various utterance sentences of characters. Some of the utterance sentences contain knowledge, and the others not. We define the utterance sentence as a knowledge explanation sentence. If the knowledge explanation sentences are extracted automatically, those can be used for knowledge acquisition with comics and comics recommendation.In this paper, we propose an extraction method of the knowledge explanation sentences from characters' conversations in comics. We assume that the knowledge explanation sentences include content words and expressions that are special for knowledge explanation. The proposed method represents a sentence as a combination vector of an averaged word vector and an expression vector. The averaged word vector is an average of each content word's embedding. The expression vector has factors of expression meanings that are used for knowledge explanation. The combination vectors are trained by Support Vector Machine to obtain a classifier. The classification outputs a set of knowledge explanation sentences that will be an extraction result.We tested the proposed method by comparing other methods with 4,611 sentences from five comics. The proposed method obtained 0.80 accuracy, 0.77 precision, and 0.88 recall that were the highest among the prepared methods."
9331057,"Detection of Compatibility, Proximity and Expectancy of Bengali Sentences using Long Short Term Memory","Text classification is known to be a supervised machine learning technique used in one or more predefined categories to classify sentences or text archives. To be a perfect phrase to convey one's feelings or to be significant, a Bengali sentence must have three properties i.e. Compatibility, Proximity and Expectancy. In this paper, we have proposed a method that is able to detect whether a Bengali sentence has compatibility, proximity and expectancy using Long Short Term Memory network. Our model is trained with word embedding layer and LSTM layer for the detection of Compatibility of a sentence but POS tagging is included for assuring the syntactic structure in case of Proximity and Expectancy detection. The model is tested on around 75000 Bengali simple sentences. The proposed framework achieves an accuracy of 97.5 percent, 85.5 percent and 97 percent for Compatibility, Proximity and Expectancy respectively. The result analysis proves that our model gives better performance."
8995429,Detecting Sentences that May be Harmful to Children with Special Needs,"Children and adults with special needs may find it difficult to recognize danger and threats as well as socially complex situations. Thus, they are under the risk of being victims of exploitation, violence and attacks. In addition, they may find themselves unintendedly insulting their friends, relatives or caregivers. In this paper, we propose an autonomous agent to assist the special needs person (child or adult) in the goal of recognizing risky or insulting situations. The autonomous agent will detect these situations and will signal them to the user (by text, speech, or other signaling forms). We composed a dataset containing 13,490 sentences, categorized into one of four classes: a 'normal' sentence, an insulting sentence, a negative sentence about a different person, or a risky sentence that may indicate a dangerous situation for the special needs person, which requires immediate intervention. We used several machine learning methods, and we found that the most accurate methods were the random forest method with 100 estimators, a voting method using several classifiers, and a convolutional neural network (CNN) with embedding. All of these mechanisms reached an accuracy close to 70% in classifying the sentences in the test set. Finally, using an ensemble method comprising a panel of the 5 best CNN based methods, improves the accuracy of the results and the F1-score. Our results demonstrate the feasibility of building an assisting agent that will accompany the special needs children and adults, and assist them in their daily social interactions."
8918473,Sentence Simplification Based on Multi-Stage Encoder Model,"Sentence simplification aims to simplify a complex sentences while retaining its main idea. It is one of the most important tasks in natural language processing. Recent works addressed the task with sequence-to-sequence(Seq2seq) model. However, these conventional Seq2seq models usually based on a single-stage encoder, which only read the source complex sentence once, as a result, it was hard to extract the representational features of the source sentence precisely. To resolve the problem, we proposed a multi-stage encoder based Seq2seq model for sentence simplification. Specificly, there were three stages in the encoder of proposed model, namely N-gram reading stage, glance-over stage and final encoding stage. The N-gram reading stage catched N-gram feature embedding for other stage and the glance-over stage extracted local and global information about the source sentence. The final encoding stage took advantage of the information extracted by the former two stage to encode source sentence better. Then, it introduced a novel attention connection method which could help the decoder to make full use of the information of encoder. Experiments on three public datasets demonstrated the proposed model that outperforms state-of-the-art baseline simplification systems."
9204644,A Quantum Entanglement-Based Approach for Computing Sentence Similarity,"It is important to learn directly from original texts in natural language processing (NLP). Many deep learning (DP) models needing a large number of manually annotated data are not effective in deriving much information from corpora with few annotated labels. Existing methods using unlabeled language information to provide valuable messages consume considerable time and cost. Our provided sentence representation based on quantum computation (called Model I) needs no prior knowledge except word2vec. To reduce some semantic noise caused by the tensor product on the entangled words vector, two improved models (called Model II and Model III) are proposed to reduce the dimensions of the sentence embedding stimulated by Model I. The provided models are evaluated in the STS tasks of 2012, 2014, 2015 and 2016, for a total of 21 corpora. Experimental results show that using quantum entanglement and dimensionality reduction in sentence embedding yields state-of-the-art performances on semantic relations and syntactic structures. Compared to the Pearson correlation coefficient (Pcc) and mean squared error (MSE), the results of 16 out of 16 corpora are better than the results of the comparative methods."
9289342,Learning a Video-Text Joint Embedding using Korean Tagged Movie Clips,"For intelligent multimedia services, video contents understanding is a major challenge. In the existing video retrieval approaches, manual descriptive sentence data is necessary for retrieving desired videos against user's search intent. To overcome these limitations, modeling visual concepts included in video and sentence is necessary to learn a mapping of video and text into a common vector space, where relevant videos and texts are close to each other. In this study, we construct a new dataset containing 250 Korean movies with manual text description in Korean. Also, video-text joint embedding model and its quantitative and qualitative search results are introduced. With our proposed model, video manual tagging is no longer necessary for video retrieval services."
9594674,Efficient Estimate of Sentence's Representation Based on the Difference Semantics Model,"Sentence representation is an important research hotspot in natural language processing (NLP) since it can map the semantics of sentences into semantics vectors, thereby effectively solving complex semantics computing problems. Recently, sentence representations are mainly obtained by indirect means. Specifically, for sentence representations obtained by unsupervised means, they are often calculated by the weighted sum of embeddings of tokens in sentences; for sentence representations obtained by self-supervised or supervised means, they are often derived from intermediate encodings of sentences in prediction tasks. For example, Google's BERT and MUSE respectively use the embedding of [CLS] in the next sentence prediction task and intermediate encodings of sentences in the translation bridge task as sentence representations. In this paper, we use the observed semantics increment feature of sentences to directly model the semantics function of sentences. To be able to use the existing neural network language model to approximate the semantics function, we first implement the first-order Taylor expansion on the semantics function to obtain a difference semantics model and then add it to BERT as a subtask to perform self-supervised fine-tuning. Finally, we get a new sentence representation model S-BERT. S-BERT achieves the state-of-the-art performance on many datasets in Chinese, English, and Vietnamese."
9640295,Improving the Performance of the Extractive Text Summarization by a Novel Topic Modeling and Sentence Embedding Technique using SBERT,"Given the limitations of human reading abilities and the massive amount of text data available in modern times, there is a need for an automatic text summarization system. One automatic text summarization method that produces a satisfactory summary is extractive text summarization based on density peaks clustering. Previous research that applied this method has become state-of-the-art for the DUC 2004 dataset. However, there is still an opportunity for further development, specifically by applying the artificial neural network-based sentence embedding technique to replace the embedding vector space model and LDA topic modeling that was previously used. This research proposes a cluster-based automatic text summarization system using Sentence-BERT (SBERT) to perform sentence embedding and topic modeling processes to improve the summarization technique proposed by previous research. SBERT was chosen because it has state-of-the-art performance on sentence embedding tasks, so it is expected to represent the semantic meaning of sentences better than the techniques used in previous studies. This research is the first research that applied SBERT for text summarization. This research also proposes several improvements for the sentence selection techniques used in previous studies. Based on the assessment using the ROUGE toolkit, the text summarization system built in this study succeeded in creating a better summary than the previous research."
9443135,A method Based on an Attention Mechanism to Measure the Similarity of two Sentences,"Bidirectional LSTMs and the attention mechanism play an essential role in many areas of natural language processing. Many studies give equal importance to words, which leads to a flawed model. This research offers a method based on Attention-Based Bidirectional Long-Short Term Memory (BLSTM) to solve the problem of plagiarism at the sentence level. For this purpose, word embedding is first made with Glove and Word2Vec methods and is considered as initial embedding. Then the two BLSTM networks are used separately for sentence embedding. Finally, the embedding of sentences and their differences are connected and passed through a classifier. We evaluate our model on two datasets of Persian and English. The evaluation results show the superiority of the proposed model over other compared methods."
9074379,Named Entity Recognition for Code-Mixed Indian Corpus using Meta Embedding,"In this paper, we utilize the pre-trained embedding, sub-word embedding and closely related languages of languages in the code mixed corpus to create a meta-embedding. We then use the Transformer to encode the code mixed sentence and use Conditional Random Field to predict the Named Entities in the code-mixed text. In contrast to classical Named Entity recognition where the text is monolingual, our approach can predict the Named Entities in code-mixed corpus written both in the native script as well as Roman script. Our method is a novel method to combine the embeddings of closely related languages to identify Named Entity from Code-Mixed Indian text written using native script and Roman script in social media."
9196423,Enhancing Short Text Topic Modeling with FastText Embeddings,"Over the past few years, we have experienced the rapid development of online social media, which produced a variety of short texts. It is important to understand the topic patterns of these short texts. Because of data sparsity, traditional topic models are not suitable for short text topic analysis. In this paper, we proposed a novel topic model, referred as FastText-based Sentence-LDA (FSL) model, which extends the Sentence-LDA topic model for short texts. We first utilize the FastText model to train a word embedding replacement model, which can alleviate the problem of lacking word co-occurrence information over short texts. Secondly, we propose a new latent feature topic model which integrates latent feature word embeddings into Sentence-LDA. Experimental results demonstrate that our new model has produced significant improvements in topic coherence by using information from external corpora."
9443970,Twi Corpus: A Massively Twi-to-Handful Languages Parallel Bible Corpus,"This paper presents detailed modeling of massively parallel Bible corpus based on Twi, a common Ghanaian language, to a handful of languages. We discussed some of the common issues we encountered in obtaining, processing, converting, and formatting the corpus and the latent desire for success in NLP. We stored the sentence aligned data in various files based on the Twi to the selected language pairs with a tab-delimited separation. Verses with the same line number in a line pair are mappings of each other. It is often challenging to learn what a “clean” corpus looks like in lower-resource situations, especially where the target corpus is the only sample of the language's parallel text. We, therefore, performed unsupervised measurements on each sentence pair. We engage the squared Mahalanobis distances that predicted parallelism on the dataset. Eventually, we perform a statistical analysis of the corpora collected based on selected text categorization models for text classification by leveraging vector embedding (like Word2vec). Finally, we trained the Twi vocabs for a 2D representation. Similar words find their vectors closer by engaging t-Distributed Stochastic Neighbor embedding (t-SNE)."
8712424,Matching Image and Sentence With Multi-Faceted Representations,"In this paper, we propose a novel multimodal matching model for the image and sentence based on their multiple representations. Each representation of the image or sentence undergoes an independent neural network, consisting of multiple layers of nonlinear mappings to yield the corresponding embedding. Besides exploiting the image and sentence relationship based on their embeddings, we propose one novel loss to further exploit the relationship within each single modality, namely, image and sentence based on the yielded multiple embeddings, which is used to train the neural networks simultaneously. The experimental results demonstrate that multiple representations can help to capture the image contents and the sentence semantic meaning more precisely, thus making comprehensive exploitations of the complicated image and sentence matching relationship. More concretely, the proposed matching model significantly outperforms the state-of-the-art approaches in bidirectional image-sentence retrieval on the Flickr8K, Flickr30K, and Microsoft COCO datasets."
9384567,Enhanced Word Embedding Method in Text Classification,"For the task of natural language processing (NLP), Word embedding technology has a certain impact on the accuracy of deep neural network algorithms. Considering that the current word embedding method cannot realize the coexistence of words and phrases in the same vector space. Therefore, we propose an enhanced word embedding (EWE) method. Before completing the word embedding, this method introduces a unique sentence reorganization technology to rewrite all the sentences in the original training corpus. Then, all the original corpus and the reorganized corpus are merged together as the training corpus of the distributed word embedding model, so as to realize the coexistence problem of words and phrases in the same vector space. We carried out experiment to demonstrate the effectiveness of the EWE algorithm on three classic benchmark datasets. The results show that the EWE method can significantly improve the classification performance of the CNN model."
9154162,Deep LSTM-RNN with Word Embedding for Sarcasm Detection on Twitter,"In the world full of sarcastic people, it's becoming challenging for the people of 21 st century to detect sarcasm using sentiment analysis efficiently. Sarcasm detection helps us to understand the bitter truth under the sugar coated sentences. It is widely used in various networking sites for understanding the true reviews and taking appropriate actions on the same if needed. Various methods, techniques and algorithms have been applied, although there's little or much drawback of using the same. For instance, algorithms like logistic regression has been used to detect sarcasm, which has a drawback, can't be used for continuous datasets. In our paper, we will be discussing about the approach we found appropriate and also provides an increased accuracy. Self designed dataset of sarcastic and non-sarcastic tweets is used for training of proposed model. Use of Recurrent Neural Network, Long Short Term Memory (LSTM) and Word Embeddings can make the sarcasm detection efficient and thereby making the statements from twitter easily classifiable."
9727996,Multi-View Metrics Enhanced Heterogeneous Graph Neural Network for Extractive Summarization,"Currently, many works regarded extractive summarization as a binary classification problem. However, the metric of the summary classification is always too singular and cannot fully utilize sentence features. To address this issue, we propose a multi-view metrics enhanced heterogeneous graph neural network for extractive summarization (MHGS). Firstly, the text is modeled as a heterogeneous graph, which contains two types of nodes, namely, words and sentences. Secondly, in the graph update layer, cross-sentence relationships and long-distance associations are obtained through the indirect connection of words and the direct connection between sentences. Then, we design multi-view metrics including relevance, redundancy, new information, and Rouge to help the model improve the quality of the summary. Finally, to evaluate the effectiveness of the method, some experiments are conducted on the CNN & DailyMail dataset. The experimental results show a generally higher performance of MHGS compared with related methods."
9352546,Emotion Expression With Fact Transfer for Video Description,"Translating a video into natural language is a fundamental but challenging task in visual understanding, since there is a great gap between visual content and linguistic sentence. More attention has been paid to this research field and a number of state-of-the-art results are achieved in recent years. However, the emotions in videos are usually overlooked, leading to the generated description sentences being boring and colorless. In this work, we construct a new dataset for video description with emotion expression, which consists of two parts: a re-annotated subset of the MSVD dataset with emotion embedded and another subset annotated with long sentences and rich emotions based on a video emotion recognition dataset. A fact transfer based framework is designed, which incorporates a fact stream and an emotion stream to generate sentences with emotion expression for video description. In addition, we propose a novel approach for sentence evaluation by balancing facts and emotions. A group of experiments are conducted, and the experimental results demonstrate the effectiveness of the proposed methods, including the idea of dataset construction for video description with emotion expression, model training and testing, and the emotion evaluation metric. The project page (including the code and dataset) can be found in https://mic.tongji.edu.cn/ce/70/c9778a183920/page.htm."
8590814,Topic-Oriented Image Captioning Based on Order-Embedding,"We present an image captioning framework that generates captions under a given topic. The topic candidates are extracted from the caption corpus. A given image's topics are then selected from these candidates by a CNN-based multi-label classifier. The input to the caption generation model is an image-topic pair, and the output is a caption of the image. For this purpose, a cross-modal embedding method is learned for the images, topics, and captions. In the proposed framework, the topic, caption, and image are organized in a hierarchical structure, which is preserved in the embedding space by using the order-embedding method. The caption embedding is upper bounded by the corresponding image embedding and lower bounded by the topic embedding. The lower bound pushes the images and captions about the same topic closer together in the embedding space. A bidirectional caption-image retrieval task is conducted on the learned embedding space and achieves the state-of-the-art performance on the MS-COCO and Flickr30K datasets, demonstrating the effectiveness of the embedding method. To generate a caption for an image, an embedding vector is sampled from the region bounded by the embeddings of the image and the topic, then a language model decodes it to a sentence as the output. The lower bound set by the topic shrinks the output space of the language model, which may help the model to learn to match images and captions better. Experiments on the image captioning task on the MS-COCO and Flickr30K datasets validate the usefulness of this framework by showing that the different given topics can lead to different captions describing specific aspects of the given image and that the quality of generated captions is higher than the control model without a topic as input. In addition, the proposed method is competitive with many state-of-the-art methods in terms of standard evaluation metrics."
9652410,Comparative Analysis of Word Embeddings in Assessing Semantic Similarity of Complex Sentences,"Semantic textual similarity is one of the open research challenges in the field of Natural Language Processing. Extensive research has been carried out in this field and near-perfect results are achieved by recent transformer-based models in existing benchmark datasets like the STS dataset and the SICK dataset. In this paper, we study the sentences in these datasets and analyze the sensitivity of various word embeddings with respect to the complexity of the sentences. In this article, we build a complex sentence dataset comprising of 50 sentence pairs with associated semantic similarity values provided by 15 human annotators. Readability analysis is performed to highlight the increase in complexity of the sentences in the existing benchmark datasets and those in the proposed dataset. Further, we perform a comparative analysis of the performance of various word embeddings and language models on the existing benchmark datasets and the proposed dataset. The results show the increase in complexity of the sentences has a significant impact on the performance of the embedding models resulting in a 10-20% decrease in Pearson’s and Spearman’s correlation."
9338635,Duplicate record detection approach based on sentence embeddings,"Duplicate record detection is a crucial task for data cleaning. Records representation is among the main challenges of this task. Word embeddings models have been widely applied in an attempt to improve records representation. However, despite the improvements made by word embeddings to enhance the semantic aspect, duplicate record detection results is still insufficient In this paper, we present a duplicate record detection approach based on sentence embeddings, where each attribute is viewed as a sentence. First, universal sentence encoder model is used to embed the values of records' attributes into embeddings vectors. Afterwards, based on the created vectors, similarity vectors between the record pairs are computed. Finally, support vector machine algorithm is used to classify the similarity vectors. Experiments on two datasets (Cora and Restaurant) show that our proposal outperforms state-of-the-art baselines and leads to significant improvements in duplicate record detection effectiveness."
9055679,Embedding Framework for Identifying Ambiguous Words in Code-Mixed Social Media Text,"Now a day's text on social media contains codeswitched and code-mixed contents. These contents are widely used by people to express their opinions on any topic in the languages known to them. Her code-mixing technique is analyzed to find the words which can be used both in Hindi and in English, having different contexts. This leads to word sense ambiguity problem as one word can have a different meaning when it used in context to other words in a sentence. As Hindi Roman and English language exhibit word sense ambiguity, and resolving this ambiguity is a current research issue using the machine learning model. Here character embedding features are used for the representation of each word written in code-mixed content. The proposed method was developed for identifying context words by classifying the intent for using the ambiguous word in code mixed sentence. A well-known hierarchical LSTM model is used in the paper for context-based sub-word-level ambiguity detection to identify the language of the word. The work on Language Identification in the code-mixed text using character-based embedding for processing ambiguous word is a novel approach and shows promising results."
9007316,Domain Specific word Embedding Matrix for Training Neural Networks,"The text represents one of the most widespread sequential models and as such is well suited to the application of deep learning models from sequential data. Deep learning through natural language processing is pattern recognition, applied to words, sentences, and paragraphs. This study describes the process of creating a pre-trained word embeddings matrix and its subsequent use in various neural network models for the purposes of domain-specific texts classification. Embedding words is one of the popular ways to associate vectors with words. Creating a word embedding matrix maps imply well semantic relationship between words, which can vary from task to task."
9269483,Deep Relation Embedding for Cross-Modal Retrieval,"Cross-modal retrieval aims to identify relevant data across different modalities. In this work, we are dedicated to cross-modal retrieval between images and text sentences, which is formulated into similarity measurement for each image-text pair. To this end, we propose a Cross-modal Relation Guided Network (CRGN) to embed image and text into a latent feature space. The CRGN model uses GRU to extract text feature and ResNet model to learn the globally guided image feature. Based on the global feature guiding and sentence generation learning, the relation between image regions can be modeled. The final image embedding is generated by a relation embedding module with an attention mechanism. With the image embeddings and text embeddings, we conduct cross-modal retrieval based on the cosine similarity. The learned embedding space well captures the inherent relevance between image and text. We evaluate our approach with extensive experiments on two public benchmark datasets, i.e., MS-COCO and Flickr30K. Experimental results demonstrate that our approach achieves better or comparable performance with the state-of-the-art methods with notable efficiency."
9738841,I2Transformer: Intra- and Inter-Relation Embedding Transformer for TV Show Captioning,"TV show captioning aims to generate a linguistic sentence based on the video and its associated subtitle. Compared to purely video-based captioning, the subtitle can provide the captioning model with useful semantic clues such as actors’ sentiments and intentions. However, the effective use of subtitle is also very challenging, because it is the pieces of scrappy information and has semantic gap with visual modality. To organize the scrappy information together and yield a powerful omni-representation for all the modalities, an efficient captioning model requires understanding video contents, subtitle semantics, and the relations in between. In this paper, we propose an Intra- and Inter-relation Embedding Transformer (I2Transformer), consisting of an Intra-relation Embedding Block (IAE) and an Inter-relation Embedding Block (IEE) under the framework of a Transformer. First, the IAE captures the intra-relation in each modality via constructing the learnable graphs. Then, IEE learns the cross attention gates, and selects useful information from each modality based on their inter-relations, so as to derive the omni-representation as the input to the Transformer. Experimental results on the public dataset show that the I2Transformer achieves the state-of-the-art performance. We also evaluate the effectiveness of the IAE and IEE on two other relevant tasks of video with text inputs, i.e., TV show retrieval and video-guided machine translation. The encouraging performance further validates that the IAE and IEE blocks have a good generalization ability. The code is available at https://github.com/tuyunbin/I2Transformer."
8268651,Learning Two-Branch Neural Networks for Image-Text Matching Tasks,"Image-language matching tasks have recently attracted a lot of attention in the computer vision field. These tasks include image-sentence matching, i.e., given an image query, retrieving relevant sentences and vice versa, and region-phrase matching or visual grounding, i.e., matching a phrase to relevant regions. This paper investigates two-branch neural networks for learning the similarity between these two data modalities. We propose two network structures that produce different output representations. The first one, referred to as an embedding network, learns an explicit shared latent embedding space with a maximum-margin ranking loss and novel neighborhood constraints. Compared to standard triplet sampling, we perform improved neighborhood sampling that takes neighborhood information into consideration while constructing mini-batches. The second network structure, referred to as a similarity network, fuses the two branches via element-wise product and is trained with regression loss to directly predict a similarity score. Extensive experiments show that our networks achieve high accuracies for phrase localization on the Flickr30K Entities dataset and for bi-directional image-sentence retrieval on Flickr30K and MSCOCO datasets."
9070506,Emotion Recognition from Text Stories Using an Emotion Embedding Model,"In this paper, we analyze emotions in a story text using an emotion embedding model. Firstly, we collected 144,701 tweets, and each tweet is given an emotional hashtag. Using the emotion hashtag as an emotion label, we built a CNN model for emotion classification. We then extracted the embedding model created during the learning process. We then extracted word embedding layer created during the emotion classification learning process. We defined this as an `Emotion embedding model', and applied it to classify story text emotions. The story text used in emotion analysis was ROC story data, and those story sentences are classified into eight emotions based on plutchik's emotion model."
9002824,Sequential Sentence Embeddings for Semantic Similarity,"Sentence embeddings are distributed representations of sentences intended to be general features to be effectively used as input for deep learning models across different natural language processing tasks. State-of-the-art sentence embeddings for semantic similarity are computed with a weighted average of pretrained word embeddings, hence completely ignoring the contribution of word ordering within a sentence in defining its semantics. We propose a novel approach to compute sentence embeddings for semantic similarity that exploits a linear autoencoder for sequences. The method can be trained in closed form and it is easy to fit on unlabeled sentences. Our method provides a grounded approach to identify and subtract common discourse from a sentence and its embedding, to remove associated uninformative features. Unlike similar methods in the literature (e.g. the popular Smooth Inverse Frequency approach), our method is able to account for word order. We show that our estimate of the common discourse vector improves the results on two different semantic similarity benchmarks when compared to related approaches from the literature."
9790926,Estimating Semantic Relationships between Sentences Using Word Embedding with BERT,"In this study, we focus on conjunctions between sentences to estimate the semantic relations between sentences. As a method for estimating the types of hidden conjunctions, we propose a method using a word embedding with bidirectional encoder representations from the transformer (BERT), which has shown high accuracy in various natural language processing tasks. By using Japanese newspaper articles, we have confirmed the effectiveness of the proposed method in estimating the presence or absence of conjunctions and the types of conjunctions. There was a difference in the accuracy by changing the estimator used to input word embedding. The result varied greatly depending on the conjunction."
8921386,Extractive Myanmar News Summarization Using Centroid Based Word Embedding,"Nowadays, many researches are going on for text summarization because there are a lot of data on the internet and it is required to process, store and manage. Text summarization is a process of distilling important information from the original text and presents that information in the form of summary. The system is proposed to summarize Myanmar news with centroid based method. Centroid based method ranks the sentences based on their similarity to the centroid. Centroid based method uses the bags of words model to represent sentences. Bags of words representation does not capture the semantic relationship between words. To overcome this problem, centroid based method is combined with word embedding representation instead of bags of words in this paper. Experiments were done on Myanmar news dataset. Centroid based on word embedding method gets better performance than centroid based on bags of words method."
9680181,Identifying Duplicate Police Reports,"Several crimes occur every day, and the first step in investigating these crimes begins with a police report. Victims report the criminal facts, which in turn must be detailed and contain accurate information about the incident or crime (e.g., factual, accurate, clear, concise, complete, and timely). In addition, the bulletin helps to safeguard the police operation itself, showing where the series of investigative operations that police agencies have been carrying out began. In cities with high crime rates, it is unfeasible to require the police to read and analyze all reported crime narratives. However, it would be helpful if employees could identify reports with similar modus operandi. Priority legal document retrieval is an information retrieval task used to retrieve past case documents related to specific cases and guide the police on how to act. Given a police report, the main objective of this work is to determine the most similar or duplicate police report. Another method is to encode the narrative as an embedded vector. In this article, we experimented with different pre-trained representations at the sentence level. We found the one that most effectively captures the semantic attributes of police report vocabulary and recognizes repeated reports. We are also investigating whether the summarized sentences identify duplicate police reports. Finally, we compare the effectiveness of the duplicated police report with the available sentence incorporation model trained in a large corpus. Our goal is to evaluate the performance of these embedding models (the one trained with our corpus and the pre-trained) to capture duplicate narratives."
8859430,Progress of Word Embedding in Code Generation,"How to use natural language to generate program code directly has become the focus of researchers, and realizing machine's understanding of natural language is an important foundation to solve this problem. In the natural language processing system based on deep learning, word embedding model can encode words and sentences, which significantly improves the capability of a neural network to process text data. Through the analysis of word embedding model, this paper describes its vital role in code generation, elaborates the process of using word embedding and neural network to automatically generate program code, and finally looks forward to its development trend."
9489253,Intelligence Embedded Image Caption Generator using LSTM based RNN Model,"Humans tend to extract information from everything they see be it living or non-living. This whole phenomenon motivated us to move in this direction and explore the field of computer vision and how this can be used with recurrent neural networks to generate captions from any image. By witnessing the recent increase in natural language processing-based applications; various other researchers have also worked on this concept and produced commendable results. Describing an image is not an easy task to implement, the structure and semantics of a sentence hold an important weight age in sentence formation. This paper approaches the problem of caption generation with an LSTM (Long-Short Term Memory) based RNN model and builds architecture based on the same to generate efficient and meaningful captions by training the dataset effectively. Flicker8k dataset is used to train our model and worked well. The accuracy of the model is evaluated based on standard evaluation metrics."
9329439,A Semi-Supervised Feature Fusion Method for Sentence Similarity Matching,"Ahstract-The semantic textual similarity plays an important role in the NLP fields and it aims to find whether the semantic between two sentences is similar or not. The methods for processing the features would have a significant impact on the performance of the model. However, most of the previous work was focused on modeling with a complex model and some handcrafted feature like syntax tree and positional information is ignored, and the mathematical feature coding model is lack of flexibility. In this paper, a semi-supervised feature fusion method is proposed to utilized handcrafted features. A method for generating embedded representations of dependency trees is designed to utilized the syntactic features. The syntactic with the positional feature are fused by a set of auto-encoder operations. It enables the feature representation more effective. Experiments are performed on the STS tasks datasets and demonstrated the effectiveness of our approach. It is found that our proposed approach get the best performance in 5 datasets compared to previous work."
9740087,Region-level arabic dialect identification using deep learning models,"The automatic classification of the Arabic language dialects is the preliminary step towards establishing many dialectal-sensitive Arabic natural language processing tasks. Arabic dialect identification entails predicting the dialects associated with specific textual inputs and classifying them under their respected labels. In this paper, we propose a novel approach by merging several distinct datasets to obtain a large, diverse, and bias-free dialectal corpus. Our dataset is a collection of parallel sentences translated into multiple dialects (MADAR), as well as tweets gathered from Twitter users (NADI-2020, NADI-2021, QADI, & ARAP-Tweet 2.0). The collected dataset is classified into seven labels: Gulf, Levant, Iraq, Maghreb, Nile Basin, Yemen, and Modern Standard Arabic. The merged dataset was cleansed to produce Arabic sentences free of punctuation, non-Arabic characters, numbers, repeating characters, empty lines, and elongation characters. We obtained a high dialectal classification accuracy using a new Word2vec embedding model on the merged dialectal dataset. Seven deep learning systems were trained on a balanced subset of the dataset. These models are Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional-LSTM (BILSTM), Bidirectional-GRU (BIGRU), Convolutional-LSTM (CLSTM), and Convolutional-GRU (CGRU). Single-label classification tests ran on these trained models acquired a minimum accuracy of 77.90% (CNN) and a maximum accuracy of 81.52% (BILSTM). We further evaluated the accuracy of tests ran for short and long sentences with 87.52% attained accuracy for classifying short sentences with CGRU and 94.06% for long sentences using BIGRU, both of which are indications of proposed approach efficacy to move forward."
9377958,Textual Evidence Mining via Spherical Heterogeneous Information Network Embedding,"Scientific literature, as one of the major knowledge resources, provides abundant textual evidence that has great potential to support high-quality scientific hypothesis validation. In this paper, we study the problem of textual evidence mining in scientific literature: given a scientific hypothesis as a query triplet, find the textual evidence sentences in scientific literature that support the input query. A critical challenge for textual evidence mining in scientific literature is to retrieve high-quality textual evidence without human supervision. Because it is non-trivial to obtain a large set of human-annotated articles containing evidence sentences in scientific literature. To tackle this challenge, we propose EvidenceMiner, a high-quality textual evidence retrieval method for scientific literature without human-annotated training examples. To achieve high-quality textual evidence retrieval, we leverage heterogeneous information from both existing knowledge bases and massive unstructured text. We propose to construct a large heterogeneous information network (HIN) to build connections between the user-input queries and the candidate evidence sentences. Based on the constructed HIN, we propose a novel HIN embedding method that directly embeds the nodes onto a spherical space to improve the retrieval performance. Quantitative experiments on a huge biomedical literature corpus (over 4 million sentences) demonstrate that EvidenceMiner significantly outperforms baseline methods for unsupervised textual evidence retrieval. Case studies also demonstrate that our HIN construction and embedding greatly benefit many downstream applications such as textual evidence interpretation and synonym meta-pattern discovery."
9410213,Leveraging Grammatical Roles for Measuring Semantic Similarity Between Texts,"Semantic similarity between texts can be defined based on their meaning. Assessing the textual similarity is a prerequisite in almost all applications in the field of language processing and information retrieval. However, the diversity in the sentence structure makes it formidable to estimate the similarity. Some sentences pairs are lexicographically similar but semantically dissimilar. That is why the trivial lexical overlapping is not enough for measuring the similarity. To attain the semanticity of sentences, the context of the words and the structure of the sentence should be considered. In this paper, we propose a new method for capturing the semantic similarity between sentences based on their grammatical roles through word semantics. First, the sentences are divided grammatically into different parts where each part is considered as a grammatical role. Then multiple new measures are introduced to estimate the role-based similarity exploiting word semantics considering the sentence structure. The proposed similarity measures focus on inter-role and intra-role similarity between the sentence-pair. The word-level semantic information is extracted from a pre-trained word-embedding model. The performance of the proposed method was verified by conducting a wide range of experiments on the SemEval STS dataset. The experimental results indicated the effectiveness of the proposed method in terms of different standard evaluation metrics and outperformed some known related works."
9319154,Citation Intent Classification Using Word Embedding,"Citation analysis is an active area of research for various reasons. So far, statistical approaches are mainly used for citation analysis, which does not look into the internal context of the citations. Deep analysis of citation may reveal interesting findings by utilizing deep neural network algorithms. The existing scholarly datasets are best suited for statistical approaches but lack citation context, intent, and section information. Furthermore, the datasets are too small to be used with deep learning approaches. For citation intent analysis, the datasets must have a citation context labeled with different citation intent classes. Most of the datasets either do not have labeled context sentences, or the sample is too small to be generalized. In this study, we critically investigated the available datasets for citation intent and proposed an automated citation intent technique to label the citation context with citation intent. Furthermore, we annotated ten million citation contexts with citation intent from Citation Context Dataset (C2D) dataset with the help of our proposed method. We applied Global Vectors (GloVe), Infersent, and Bidirectional Encoder Representations from Transformers (BERT) word embedding methods and compared their Precision, Recall, and F1 measures. It was found that BERT embedding performs significantly better, having an 89% Precision score. The labeled dataset, which is freely available for research purposes, will enhance the study of citation context analysis. Finally, It can be used as a benchmark dataset for finding the citation motivation and function from in-text citations."
8923485,Joint Embedding of Emoticons and Labels Based on CNN for Microblog Sentiment Analysis,"Microblog sentiment analysis is a technique to analyze the emotion types of microblog texts, which is beneficial to control the direction of the public's attitudes or emotions. Different researches are being developed to guide sentiment analysis. However, these methods do not consider or do not give enough attention to emoticons and labels present in the microblog sentences, which cause a loss of potential information. To solve this problem, the research proposes a novel model, called joint embedding of Emoticons and Labels based on CNN (EL-CNN). EL-CNN applies an input encoder to convert input sequences into fix-length semantic vectors, a label encoder to convert labels into fix-length semantic vectors and a matcher to calculate a matching score between the fix-length semantic vectors obtained from the input encoder and the label encoder, which turns the sentiment classification tasks into vector matching tasks. The work maintains the interpretability of word embedding and enjoys a built-in ability to capture sentiment information from emoticons and labels. Experimental results on two public microblog benchmark corpora (NLPCC2013 and NLPCC2014) show that the EL-CNN model achieves significant improvements as compared to all start-of-the-art methods."
9760564,Word Translation using Cross-Lingual Word Embedding: Case of Sanskrit to Hindi Translation,"Sanskrit is a low resource language for which large parallel data sets are not available. Large parallel data sets are required for Machine Translation. Cross-Lingual word embedding helps to learn the meaning of words across languages in a shared vector space. In the present work, we propose a translation technique between Sanskrit and Hindi words without a parallel corpus-base. Here, fastText pre-trained word embedding for Sanskrit and Hindi are used and are aligned in the same vector space using Singular Value Decomposition and a Quasi bilingual dictionary. A Quasi bilingual dictionary is generated from similar character string words in the monolingual word embeddings of both languages. Translations for the test dictionary are evaluated on the various retrieval methods e.g. Nearest neighbor, Inverted Sofmax approach, and Cross-domain Similarity Local Scaling, in order to address the issue of hubness that arises due to the high dimensional space of the vector embeddings. The results are compared with the other Unsupervised approaches at 1, 10, and 20 neighbors. While computing the Cosine similarity, we observed that the similarity between the expected and the translated target words is either close to unity or equal to unity for the cases that were even not included in the Quasi bilingual dictionary that was used to generate the orthogonal mapping. A test dictionary was developed from the Wikipedia Sanskrit-Hindi Shabdkosh to test the translation accuracy of the system. The proposed method is being extended for sentence translation."
9172840,A Knowledge Graph Embedding Method Based on Neural Network,"As the basis of many knowledge graph completion tasks, the embedding representation of entities and relations in knowledge graph (KG) is an important task in the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI). While most of the existing knowledge graph embedding (KGE) models based on convolutional neural network (CNN) can obtain abundant feature embedding, they may ignore an important fact that the triples in the KG come from the text, as they simply learn about the feature embedding of entities and relations without considering contextual information. Therefore, in this paper, we propose an effective KGE model based on neural network. First of all, we convert the triple (h,r,t) of the KG into a sentence [h r t]. Then, the LSTM neural network is used to learn the long-term dependence of sentences from the input feature vectors. Then, on this basis, the two-layer convolutional neural network with several different filters is used to extract different local features. Finally, the obtained feature vectors are connected together, and the inner product is carried out with the weight vectors to obtain the score of the triple, so as to judge the validity of the given triple. We evaluate our model on two benchmark datasets FB15k-237 and WN18RR, the experimental results show that the model can effectively improve the accuracy of link prediction, achieving better results compared with other baseline models."
9023215,Modeling Content Interaction in Information Diffusion with Pre-trained Sentence Embedding,"Social networks have become indispensable parts of our daily life, and therefore understanding the process of information diffusion over social networks is a meaningful research topic. Usually, multiple pieces of information do not spread in isolation; rather, they interact with each other throughout the diffusion process. This paper aims to quantify these interactions by modeling users' forwarding behavior after reading a series of information. Inspired by several successful components prevalent in recent research of deep learning, i.e., long short term memory (LSTM) network and bi-directional encoder representation from transformers (BERT), we designed IMM Enhanced model and InfoLSTM model. In our experiments on real-world Weibo dataset, both models significantly outperform baselines such as the prior IMM model and IP model, with IMM Enhanced model improving 23.52% and InfoLSTM model improving 32.56% in F1 score (absolute value) compared to that of baseline IMM model. In addition, we visualize the dataset and the parameters learned in IMM Enhanced model, which further enables us to discuss the relationship between text similarity and information diffusion interaction with case studies."
9350744,Quantum Entanglement Based Sentence Similarity Computation,"Sentence representation is one of the foundations of natural language processing. A quantum entanglement-based approach is provided to determine the sentence similarity. The sentence embedding based on quantum computation considers the semantic structure of a sentence, tensor product and attentive mechanism. Main components of modifiers of a complete sentence are obtained by using the attentive mechanism combining with quantum entanglement of two consecutive notional words. The semantic features and syntactic structures of sentences are extracted by the tensor product of two consecutive notional words without any big-corpus or prior knowledge. Experimental results implemented on 17 datasets with an excellent effect show that taking the quantum entanglement into the sentence embedding yields improvements on mining semantic relations and syntactic structures. These results shed new light on sentence representation with limited resources, and represent a new step towards the quantum entanglement-based application in natural language processing."
9147767,Prob2Vec: Mathematical Semantic Embedding for Problem Retrieval in Adaptive Tutoring,"We propose a novel mathematical semantic embedding for problem retrieval in adaptive tutoring. The goal is to retrieve problems with similar mathematical concepts. There are two challenges: First, problems conducive to tutoring are never exactly the same in terms of underlying concepts: those problems often mix concepts in innovative ways. Second, it is difficult for human to determine a consistent similarity score across a large enough training set. To address these two challenges, we develop a hierarchical problem embedding algorithm, Prob2Vec, which consists of abstraction and embedding steps. Prob2Vec is able to distinguish very finegrained differences among problems, an ability humans need time and effort to acquire. In addition, the associated concept labeling is a multi-label problem with imbalanced training data set suffering from dimensionality explosion. Robust concept labeling is achieved with a novel negative pre-training algorithm that dramatically reduces false negative and positive ratios for classification. Experimental results show that Prob2Vec achieves 96.88% accuracy on a problem similarity test, in contrast to 75% from directly applying state-of-the-art sentence embedding methods."
9364434,On-Device Sentence Similarity for SMS Dataset,"Determining the sentence similarity between Short Message Service (SMS) texts/sentences plays a significant role in mobile device industry. Gauging the similarity between SMS data is thus necessary for various applications like enhanced searching and navigation, clubbing together SMS of similar type when given a custom label or tag is provided by user irrespective of their sender etc. The problem faced with SMS data is its incomplete structure and grammatical inconsistencies. In this paper, we propose a unique pipeline for evaluating the text similarity between SMS texts. We use Part of Speech (POS) model for keyword extraction by taking advantage of the partial structure embedded in SMS texts and similarity comparisons are carried out using statistical methods. The proposed pipeline deals with major semantic variations across SMS data as well as makes it effective for its application on-device (mobile phone). To showcase the capabilities of our work, our pipeline has been designed with an inclination towards one of the possible applications of SMS text similarity discussed in one of the following sections but nonetheless guarantees scalability for other applications as well."
9791503,Vector Model Based Information Retrieval System With Word Embedding Transformation,"Vector based information retrieval system has been one of the trending methods in Natural Language Processing. The embeddings vector generated from a document helps in identifying most relevant document related to the query. There is various approach were embedding vectors can be generated and some of them which have implemented are Word2vec, Glove2vec and Sentence BERT. For information retrieval system also used word embedding transformation like PCA and Factor Analysis to improvise the model's performance. Most of information retrieval system involves getting query from the user, preprocessing of the query and generating most relevant information to the query. Results obtained by post processing methods such as PCA and Factor Analysis shows a comparatively better results with an increase of 2–3% of Mean average precision."
9669319,Document-level DDI relation extraction with document-entity embedding,"DDI is an important part of drug-related research and pharmacovigilance. Extracting DDI information from scientific literature has become a low-cost and highly reliable way. Currently, existing works are all sentence-level DDI relation extraction. In fact, the entity relationship is often expressed by multiple sentences. Moreover, the sentence-level DDI relation extraction also causes a large amount of redundancy in the whole dataset with increasing in negative instance data. In this study, we propose a document-level DDI relation extraction method based on document-entity embedding. Our method performs special processing on the DDI Extraction 2013 for the first time, in order to calculate document-level relation extraction. For obtaining document-level entity information, we propose a document-entity embedding method to integrate the information of all same drugs in the same article. The experimental results show that the processing of DDI Extraction 2013 dataset is reasonable. In addition, the proposed method has achieved good performance on document-level DDI dataset, and the best F1 score is 62.51%. This is the first time that DDI Extraction 2013 has been processed into a document-level dataset, and document-level DDI relation extraction has been realized."
9548088,AltibbiVec: A Word Embedding Model for Medical and Health Applications in the Arabic Language,"In recent years, the utilization of natural language processing (NLP) and Machine Learning (ML) techniques in clinical decision support systems have shown their ability in improving and automating the diagnosis process, and reducing potential clinical errors. NLP in the Arabic language is more intricate due to several limitations, such as the lack of datasets and analytical resources compared to other languages like English. However, a clinical decision support system in the Arabic context is of significant importance. A fundamental process in NLP is extracting features from text-based data via text embedding. Word embedding is a representation of words in a numeric format that encodes the statistic, semantic, or context information. Building a neural word embedding model requires hundreds of thousands of data instances to find hidden patterns of relationships within sentences. Essentially, extracting relevant and informative features promotes the performance of the learning algorithms. The objective of this paper is to propose an Arabic neural-based word embedding model in the medical and healthcare context (called “AltibbiVec”). Around 1.5 million medical consultations and questions written in different dialects are obtained from Altibbi telemedicine company and used to train the embedding model. Three different embedding models are developed and compared, which are Word2Vec, fastText, and GloVe. The trained models were evaluated by different criteria, including the word clustering and the similarity of words. Besides, performing a specialty-based question classification. The results show that Word2Vec and fastText capture sufficiently the semantics of text more than GloVe. Hence, they are recommended for healthcare NLP-based applications."
9534250,A Comparative Study of Methods for Visualizable Semantic Embedding of Small Text Corpora,"Text embedding has recently emerged as a very useful and successful method for semantic representation. Following initial word-level embedding methods such as Latent Semantic Analysis (LSA) and topic-based bag-of-words approaches like Latent Dirichlet Allocation (LDA), the focus has turned to language models and text encoders implemented as neural networks - ranging from word-level models to those embedding whole documents. The distinctive feature of these models is their ability to infer semantic spaces at all levels based purely on data, with no need for complexities such as syntactic analysis or ontology building. Many of these models are available pre-trained on enormous amounts of data, providing downstream applications with general-purpose semantic spaces. In particular, embedding models at the sentence level or higher are most useful in applications because the meaning of text only becomes clear at that level. Most text embedding methods produce text embeddings in high-dimensional spaces, with a dimensionality ranging from a few hundred to thousands. However, it is often useful to visualize semantic spaces in very low dimension, which requires the use of dimensionality reduction methods. It is not clear what language models and what method of dimensionality reduction would work well in these cases. In this paper, we compare four text embedding methods in combination with three methods of dimensionality reduction to map three related real-world datasets comprising textual descriptions of items in a particular domain (sports) to a 2-dimensional semantic visualization space. The results provide several insights into the utility of these methods for data of this type."
8855333,Fusing Syntax and Word Embedding Knowledge for Measuring Semantic Similarity,"The explosive growth of information makes it an important issue to effectively mine useful information from massive information. Text is an important carrier of information, so the processing and analysis of text has become one of the hot spots of data mining and information retrieval. Sentence similarity is the basis of most text-related tasks. The majority of current approaches leverage pairwise similarity characteristics to represent text pairs. Unlike the current approaches, we propose a new method to analyze and quantify the semantic textual similarity between sentences by encoding semantic knowledge based on word embedding into the syntax tree of sentences. We use SemEval-2012 task to test our approach and evaluate the performance with two widely used benchmarks:the Spearman and Pearson correlations, the experimental results show that compared with the best systems of semantic textual similarity (STS) task, our method can effectively improve the accuracy of sentence similarity judgment."
8716664,Person Entity Attribute Extraction Based on Siamese Network,"Entity attribute extraction, which converts chaotic text data to structured knowledge, plays an important role in natural language processing (NLP). Many previous studies proved that the representation of text has a significant impact on the results of attribute extraction. In this paper, we propose a novel model to obtain the discriminative representation of sentences by applying the Siamese architecture. Specifically, we simultaneously input two variable-length sentences in the training stage of our model, namely, the main sentence and its similar or dissimilar partner. This scheme of pair is beneficial for entity attribute extraction. First, the entity attribute extraction community suffers the insufficient but expensive labeled data, the two input sets produce much more samples for the representation learning and can be treated as a useful data augmentation method. More importantly, the co-learning by the Siamese architecture achieves more interesting embedding than the separate way, since the informative relation between the focused sentence and its partner help the representation learning to explore more essential semantics and keep stable to the variation of wording and syntax of a sentence. The experiments on the Wikipedia data show that our model takes advantage of the Siamese architecture for sentence embedding and achieves significant improvements on attribute extraction as compared with baselines. To the best of our knowledge, we are the first to introduce the Siamese network into the person entity attribute extraction, which we proved to achieve the state of the art."
9810295,MAI-CBert: Multidirectional Attention Interaction Construction-Bert Sentiment Sentence Representation,"Considering that when the traditional network structure is connected with the pre-trained language model for sentence embedding representation, it relies on simple word weights to generate sentence vectors, which easily ignores the global and contextual semantic details of sentences, reducing the accuracy of sentence representation. To address these issues, we develop a Multidirectional Attention Interaction Construction-Bert Sentence Representation Framework (MAI-CBert). First, the model uses deep attention to transform the input emotional sentences to reduce misunderstanding caused by the unbalanced distribution of word weights in the sentences; at the same time, it applies horizontal and vertical attention to emotional constructions, which are generated from two different directions. Construction vectors, aim to focus on words with salient features in sentences and establish close dependencies. Second, the dynamic interaction strategy is adopted to realize the interaction between attention in different directions, so that the information flow forms a complementary relationship, resulting in more effective sentence vectors. Notably, the loss function is refactored to improve the representation robustness of the model to avoid catastrophic forgetting. Experimental results on the SemEval-14 and ACL-14 baseline datasets demonstrate that the MAI-CBert sentence representation framework is robust and competitive."
8989352,Generating Text using Generative Adversarial Networks and Quick-Thought Vectors,"Generative Adversarial Networks (GANs) have been shown to perform very well with the image generation tasks. Many advancements have been made with GANs over the past few years, which are making them more and more accurate in their generation tasks. State-of-the-art methods of natural language processing (NLP) involve word embeddings such as global vectors for word representation (GLoVe) and word2vec. These word embeddings help apply text data to a neural network by converting the textual data to numbers that the networks could use. The main focus for GANs has been image generation and in the past few years there have been research works to apply GANs to the text generation task. This paper presents a Quick-Thought GAN (QTGAN) to generate sentences by incorporating the Quick-Thought model. Quick-Thought vectors offer richer representations than prior unsupervised and supervised methods and enable a classifier to distinguish context sentences from other contrastive sentences. The proposed QTGAN is trained on a portion of the BookCorpus dataset that has been converted to Quick-Thought embeddings. The embeddings generated from the generator are then classified and used to pick a generated sentence. BLEU scores are used to test the results of the training and compared to the Skip-Thought GAN. The increases in BLEU-3 and BLEU-4 scores were achieved with the QTGAN."
8919565,"Identification of Expectancy, Proximity, and Compatibility of the Bengali Language","Semantic errors are portrayed as in border on upon of the law of sentences. In Bengali language, we will identify errors depend on three sorts of sentence rules. They are Expectancy (n/a), Proximity (n/a), Compatibility (n/a). We are assessing every single Bengali sentence as indicated by these three sorts of techniques. Cause the accompanying way will function as significance by importance word order and will discover which sentence is sans mistake. Presently, we will place sentences into certain classes with the goal that sentences will be gathered as right or off-base. We picked a Bengali sentence comprising of completed fundamental structure of the structure Subject, Object, Verb. These sentences are broken into subject, object and verb term parts with the end goal of arrangement. From thing and action term order we classify the subject and action term into a progressively broad class and henceforth we build up the relationship. In light of the order of action terms what is more, things, we built up a relationship for subject and action term as well as action term and object of the sentence. In our tenets and terms we are demonstrating that our framework works for expectancy is 70 percent, Proximity is 80 percent, compatibility is 80 percent. Some are blunders however in most extreme sentences it can discover the mistakes."
9728370,A Korean sentence similarity calculation method based on sub-word level information,"Sentence similarity is the key technology in text summary and machine translation. In this paper, a Korean sentence similarity calculation model based on local inference and difference analysis(Kor-Sim) is presented, Kor-Sim resolves the problem of insufficient capture of potential semantic information by traditional similarity calculation method.First, the Korean word vector can be obtained by using GlowVe, secondly the embedding vector can be effectively represented by BiLSTM, thirdly the semantic expression of the sentence is strengthened by local inference.Finally, by using difference analysis, the minute semantic differences between sentences can be captured by the interaction of sentences.The experimental results show that the Kor-Sim model can improve the accuracy of the similarity calculation in Korean sentences."
9591122,Entity Relation Extraction Model Based Negative Feedback Attention,"The relationship between entity pairs is the key to understanding an entire sentence or paragraph in a natural language, so entity relationship extraction is particularly important in natural language processing tasks. When we understand the information of a sentence, we often need to combine the information into the context. The entity relationship of a sentence is expressed as a relationship, but combined with the next sentence, the entity represents another relationship,and It called negative feedback. In this paper, to solve these problems, we propose NFBATT.a entity relationship extraction model based negative feedback attention.which is based on improved attention and pre-training of BERT.The main is highlighted the negative feedback in attention of Distant Supervision Relation extraction,and in order to get a better vector representation get a better vector representation of semantics, the sentence vector we represent is a combination of word Embedding, entity Embedding and positional embedding of solid and non-solid entity words by deep learning, which we call sentence multi-information representation. We use the negative feedback attention to reduce the influence of negative feedback on sentence relationship discrimination. At the same time, we have also done a lot of experiments to prove it. Through extensive experiments on Sogou Person Knowledge Map dataset.we demonstrate NFBATT's effectiveness.and the accuracy rate is 82.0%, which is better than the best method currently available."
9556136,Graph-Based Multimodal Sequential Embedding for Sign Language Translation,"Sign language translation (SLT) is a challenging weakly supervised task without word-level annotations. An effective method of SLT is to leverage multimodal complementarity and to explore implicit temporal cues. In this work, we propose a graph-based multimodal sequential embedding network (MSeqGraph), in which multiple sequential modalities are densely correlated. Specifically, we build a graph structure to realize the intra-modal and inter-modal correlations. First, we design a graph embedding unit (GEU), which embeds a parallel convolution with channel-wise and temporal-wise learning into the graph convolution to learn the temporal cues in each modal sequence and cross-modal complementarity. Then, a hierarchical GEU stacker with a pooling-based skip connection is proposed. Unlike the state-of-the-art methods, to obtain a compact and informative representation of multimodal sequences, the GEU stacker gradually compresses the channel d with multi-modalities m rather than the temporal dimension t. Finally, we adopt the connectionist temporal decoding strategy to explore the entire video's temporal transition and translate the sentence. Extensive experiments on the USTC-CSL and BOSTON-104 datasets demonstrate the effectiveness of the proposed method."
9378621,Tag recommendation model using feature learning via word embedding,"Tag recommendation models serve as extracting metadata for target objects like images, videos and Web pages. However, these models tackle cold start problem due to absence of initial tags. To improve tag quality in tag recommendation services, most of previous works exploit the statistical properties such as co-occurrence patterns or term frequency to predict the candidate tags to a target object. Yet, these tag recommendation methods fail to be effective when initial tags are absent or low quality texts are available for objects. Recently, sentence modeling via word embeddings achieves successes in many natural language processing tasks. Therefore, this paper aims to introduce a novel tag recommendation algorithm that can analyze the relation between words in a text associated with target object using word embedding. In fact, we involve grammatical relations between words in a text or sentence with focus on feature learning methods. Skip-gram model is used to optimize feature values and learn the representation vector of words for tag recommendation. Our method shows improvements to previous research methods with gains of up to 10 percent in precision using real data from Movielens dataset."
9719473,Question Classification Using Universal Sentence Encoder and Deep Contextualized Transformer,"One of the most vital steps in automatic Question Answer systems is Question classification. The Question classification is also known as Answer type classification, identification, or prediction. The precise and accurate identification of answer types can lead to the elimination of irrelevant candidate answers from the pool of answers available for the question. High accuracy of Question Classification phase means highly accurate answer for the given question. This paper proposes an approach, named Question Sentence Embedding(QSE), for question classification by utilizing semantic features. Extracting a large number of features does not solve the problem every time. Our proposed approach simplifies the feature extraction stage by not extracting features such as named entities which are present in fewer questions because of their short length and features such as hypernyms and hyponyms of a word which requires WordNet extension and hence makes the system more external sources dependent. We encourage the use of Universal Sentence Embedding with Transformer Encoder for obtaining sentence level embedding vector of fixed size and then calculate semantic similarity among these vectors to classify questions in their predefined categories. As it is the time of the Global pandemic COVID-19 and people are more curious to ask questions about COVID. So, our experimental dataset is a publicly available COVID-Q dataset. The acquired result highlights an accuracy of 69% on COVID questions. The approach outperforms the baseline method manifesting the efficacy of the QSE method."
9315474,Phrase Embedding Based Multi Document Summarization with Reduced Redundancy using Maximal Marginal Relevance,"In the Internet Era of Information due to the exponential increase of textual data, Multi Document Summarization (MDS) is becoming an inevitable NLP task that aims to produce a concise representation of the main idea of multiple related documents. MDS becomes difficult and challenging to produce a non-redundant summary because of the lexical diversity of multiple authors. This paper proposes a new multi-document summarization system based on phrase embedding and greedy Maximal Marginal Relevance (MMR) algorithm. This approach considers phrases as the basic meaningful semantic unit of the sentences to understand and summarize documents. Embedding techniques are employed to learn the vector representation of phrases to identify similar phrases semantically. Finally, an MMR based greedy algorithm is used to select sentences with important phrases while reducing the redundancy among similar phrases. Experimental results on the benchmark dataset DUC 2004 show better performance gains compared with the state-of-the-art baselines."
8768045,Video Storytelling: Textual Summaries for Events,"Bridging vision and natural language is a longstanding goal in computer vision and multimedia research. While earlier works focus on generating a single-sentence description for visual content, recent works have studied paragraph generation. In this paper, we introduce the problem of video storytelling, which aims at generating coherent and succinct stories for long videos. Video storytelling introduces new challenges, mainly due to the diversity of the story and the length and complexity of the video. We propose novel methods to address the challenges. First, we propose a context-aware framework for multimodal embedding learning, where we design a residual bidirectional recurrent neural network to leverage contextual information from past and future. The multimodal embedding is then used to retrieve sentences for video clips. Second, we propose a Narrator model to select clips that are representative of the underlying storyline. The Narrator is formulated as a reinforcement learning agent, which is trained by directly optimizing the textual metric of the generated story. We evaluate our method on the video story dataset, a new dataset that we have collected to enable the study. We compare our method with multiple state-of-the-art baselines and show that our method achieves better performance, in terms of quantitative measures and user study."
9632886,Sarcasm Detection of Tweets in Indonesian Language Using Long Short-Term Memory,"Twitter is a massive source of information that can potentially be used to obtain valuable insights about public opinions, public ideas, and public circumstances. Extracting accurate information from tweets, however, is often challenging due to the use of informal, non-standard, and figurative languages including sarcasm. Sarcasm itself conveys messages using words with opposite literal meaning. Sarcasm detection, therefore, becomes an important task during information extraction from public tweets. This research proposes the use of LSTMs to detect sarcastic tweets in Indonesian language through the extraction of sentence-embedding features. LSTMs have been known to be able to learn sequential patterns in input data so that features extracted by LSTMs are more representative than those manually hand-crafted by human. The proposed LSTMs are combined with the Word2Vec model that serves as a word encoder that preserves semantic meaning. The proposed method is evaluated on tweets that are scrapped from the Web using some keywords related to popular topics. The experimental results demonstrate that the proposed method is able to achieve an accuracy of 82.13% and an f1-score of 61.31% outperforming the conventional TF-IDF + naïve Bayes sarcasm detector. These results thus prove that sentence-embedding is able to extract features that are more accurate and more discriminative for sarcasm detection."
9415694,Learning Video Moment Retrieval Without a Single Annotated Video,"Video moment retrieval has progressed significantly over the past few years, aiming to search the moment that is most relevant to a given natural language query. Most existing methods are trained in a fully-supervised or a weakly-supervised manner, which requires a time-consuming and expensive manually labeling process. In this work, we propose an alternative approach to achieving video moment retrieval that requires no textual annotations of videos and instead leverages the existing visual concept detectors and a pre-trained image-sentence embedding space. Specifically, we design a video-conditioned sentence generator to produce a suitable sentence representation by utilizing the mined visual concepts in videos. We then design a GNN-based relation-aware moment localizer to reasonably select a portion of video clips under the guidance of the generated sentence. Finally, the pre-trained image-sentence embedding space is adopted to evaluate the matching scores between the generated sentence and moment representations with the knowledge transferred from the image domain. By maximizing these scores, the sentence generator and moment localizer can enhance and complement each other to achieve the moment retrieval task. Experimental results on the Charades-STA and ActivityNet Captions datasets demonstrate the effectiveness of our proposed method."
9606584,Joint Embedding of Deep Visual and Semantic Features for Medical Image Report Generation,"Medical image report generation (MeIRG) aims at generating associated diagnosis descriptions with natural language sentences from medical images, which is essential in the computer-aided diagnosis system. Nevertheless, this task remains challenging in that medical images and linguistic expressions should be understood jointly which however show great discrepancies in the modality. To fill this visual-to-semantic gap, we propose a novel framework that follows the encoder-decoder pipeline. Our framework is characterized by encoding both deep visual and semantic embeddings through a triple-branch network (TriNet) during the encoding phase. The visual attention branch captures attended visual embeddings from medical images with the soft-attention mechanism. The medical report (MeRP) embedding branch predicts semantic report embeddings. The embedding branch of medical subject headings (MeSH) obtains semantic embeddings of related medical tags as complementary information. Then, outputs of these branches are fused and fed into a decoder for the report generation. Experimental results on two benchmark datasets have demonstrated the excellent performance of our method."
8904821,Long distance entity relation extraction with article structure embedding and applied to mining medical knowledge,"As a central work in medical knowledge graph construction, relation extraction has gained extensive attention in the fields of natural language processing and artificial intelligence. Conventional works on relation extraction share a common assumption: a sentence can express a relation of an entity pair only if both entities appear in this sentence. Under this assumption, plenty of informative sentences are precluded. In this paper, we break the assumption and propose a new relation extraction model that incorporates article structure information, which not only provide additional information, but also allows extracting long distance relations. We apply the model to online medical relation extraction and demonstrate its advantage over conventional models."
9321308,Learning-Free Unsupervised Extractive Summarization Model,"Text summarization is an information condensation technique that abbreviates a source document to a few representative sentences with the intention to create a coherent summary containing relevant information of source corpora. This promising subject has been rapidly developed since the advent of deep learning. However, summarization models based on deep neural network have several critical shortcomings. First, a large amount of labeled training data is necessary. This problem is standard for low-resource languages in which publicly available labeled data do not exist. In addition, a significant amount of computational ability is required to train neural models with enormous network parameters. In this study, we propose a model called Learning Free Integer Programming Summarizer (LFIP-SUM), which is an unsupervised extractive summarization model. The advantage of our approach is that parameter training is unnecessary because the model does not require any labeled training data. To achieve this, we formulate an integer programming problem based on pre-trained sentence embedding vectors. We also use principal component analysis to automatically determine the number of sentences to be extracted and to evaluate the importance of each sentence. Experimental results demonstrate that the proposed model exhibits generally acceptable performance compared with deep learning summarization models although it does not learn any parameters during the model construction process."
9142175,Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers,"Financial and economic news is continuously monitored by financial market participants. According to the efficient market hypothesis, all past information is reflected in stock prices and new information is instantaneously absorbed in determining future stock prices. Hence, prompt extraction of positive or negative sentiments from news is very important for investment decision-making by traders, portfolio managers and investors. Sentiment analysis models can provide an efficient method for extracting actionable signals from the news. However, financial sentiment analysis is challenging due to domain-specific language and unavailability of large labeled datasets. General sentiment analysis models are ineffective when applied to specific domains such as finance. To overcome these challenges, we design an evaluation platform which we use to assess the effectiveness and performance of various sentiment analysis approaches, based on combinations of text representation methods and machine-learning classifiers. We perform more than one hundred experiments using publicly available datasets, labeled by financial experts. We start the evaluation with specific lexicons for sentiment analysis in finance and gradually build the study to include word and sentence encoders, up to the latest available NLP transformers. The results show improved efficiency of contextual embeddings in sentiment analysis compared to lexicons and fixed word and sentence encoders, even when large datasets are not available. Furthermore, distilled versions of NLP transformers produce comparable results to their larger teacher models, which makes them suitable for use in production environments."
9770772,Toward the Development of Large-Scale Word Embedding for Low-Resourced Language,"Word embedding is possessed by Natural language processing as a key procedure for semantically and syntactically manipulating the unlabeled text corpus. While this process represents the extracted features of corpus on vector space that enables to perform the NLP tasks such as summary generation, text simplification, next sentence prediction, etc. There exist some approaches for word embedding that consider co-occurrence and word frequency, such as Matrix Factorization, skip-gram, hierarchical-structure regularizer, and noise contrastive estimation. These approaches have created mature word vectors for most spoken languages in the world, on the other hand, the research community turned their minor attention towards the Urdu language having 231.3 million speakers. This paper focuses on creating Urdu word embedding. To perform this task, we used a dataset covering different categories of News such as Business, Sports, Health, Politics, Entertainment, Science, world, and others. This dataset was tokenized while creating 288 million tokens. Further, for word vector formation we utilized skip-gram also known as the word2vec model. The embedding was performed while limiting the vector dimensions to 100, 200, 300, 400, 500, 128, 256, and 512. For evaluation Wordsim-353 and Lexsim-999 annotated datasets were utilized. The proposed work achieved a 0.66 Spearman correlation coefficient value for wordsim-353 and 0.439 for Lexsim-999. The results were compared with state-of-the-art and were observed better."
9392431,News Event Prediction using Causality Approach on South China Sea Conflict,"South China Sea (SCS) generates huge economic value in fishing and shipping lane as well as a high amount of natural resources. Due to its strategic location and high revenue generated, SCS became a place where several nearby countries competed for its territorial claims. Famous territorial disputes such as Spratly islands, Paracel island, Scarborough Shoal happened due to claim on SCS wealth. Newspapers are the main medium that disseminate the message to the public and update whenever SCS conflict happens. News related to SCS events or conflicts usually contain causal relationships between cause and effect. This causal relationship can be extracted and analyzed to obtain the trends of events and conflicts that have happened. In order to avoid any inevitable conflict among countries in SCS region, event prediction is important as it gives a better insight and foresee future events that might happen. In this paper, phrase similarity is used as important metrics for prediction models. First, it extracts news articles based on causality connectors such as ""because"", ""after"", ""lead to"", etc. into <; cause, effect> tuple. Then, three different embedding techniques, Doc2vec, InferSent and BERT were evaluated based on their best similarity score. The selected embedding technique is used to construct the prediction model and predict South China Sea conflict related events. A crude prediction is done based on similarity of past causes. The result shows that BERT has the highest average accuracy of 50.85% in getting the most similar phrase. By using the causal prediction model, a future possible event can be predicted and this helps to increase the awareness of national security among SCS nearby countries."
9321857,Learning Knowledge Graph Embedding with Entity Descriptions based on LSTM Networks,"Knowledge inference and knowledge prediction is widely used in the intelligent fault diagnosis that is very important to the product safety. Most learning knowledge graph embedding methods represent entities and relations only with fact triples of knowledge graphs (KGs) through translating embedding models without integrating the rich semantic information in entity descriptions. However, entity description-based method DKRL only takes high frequency words of entity descriptions as input data in training using CNN encoder model, which loss the word order feature in entity descriptions and the relevance in context. In this paper, we propose a novel learning knowledge graph embedding method with entity descriptions named as Learning Knowledge Graph Embedding with Entity Descriptions based on LSTM Networks(KGDL), which can integrate word order features of each sentence in entity descriptions and the semantic information in fact triples of KGs, to enrich the semantic representations of KGs for promoting knowledge acquisition and inference. More specifically, we explore LSTM encoder model to encode all semantic information of entity descriptions based on each sentence without losing the independent feature of each sentence and the semantic associations between sentences of entities descriptions, then encode these sentence embeddings into the entity descriptions embeddings, and further learn knowledge graph embeddings from fact triples with entity descriptions embeddings. The experiment results show that KGDL gets better performance than state-of-the-art method DKRL, in terms of mean rank value and HITS@K with highly accurate, fast and robust. Moreover, KGDL based on two-steps relational path of KGs with entity descriptions has promising abilities for relation prediction and entity prediction, which gets better performance than state-of-the-art method Path-based TransE in knowledge inference."
9251897,Semantic Classification of Scientific Sentence Pair Using Recurrent Neural Network,"One development of Natural Language Processing is the semantic classification of sentences and documents. The challenge is finding relationships between words and between documents through a computational model. The development of machine learning makes it possible to try out various possibilities that provide classification capabilities. This paper proposes the semantic classification of sentence pairs using Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Each couple of sentences is turned into vectors using Word2Vec. Experiments carried out using CBOW and Skip-Gram to get the best combination. The results are obtained that word embedding using CBOW produces better than Skip-Gram, although it is still around 5%. However, CBOW slows slightly at the beginning of iteration but is stable towards convergence. Classification of all six classes, namely Equivalent, Similar, Specific, No Alignment, Related, and Opposite. As a result of the unbalanced data set, the retraining was conducted by eliminating a few classes member from the data set, thus providing an accuracy of 73 % for non-training data. The results showed that the Adam model gave a faster convergence at the start of training compared to the SGD model, and AdaDelta, which was built, gave 75% better accuracy with an F1-Score of 67%."
9356287,Cell Type Identification from Single-Cell Transcriptomic Data via Gene Embedding,"Single-cell RNA sequencing (scRNAseq) enables the profiling of the transcriptomes of individual cells, thus characterizing the heterogeneity of biological samples since scRNAseq experiments are able to yield high volumes of data. Analyzing scRNAseq data will be beneficial for obtaining knowledge on cancer drug resistance, gene regulation in embryonic development, and mechanisms of stem cell differentiation and reprogramming. One common goal of scRNAseq data analytics is to identify the cell type of each individual cell that has been profiled. However, data sparsity is the main challenge due to limitations of current single-cell RNA sequencing techniques. In this paper, a novel method of representing the genes as gene embeddings is proposed to reduce data sparsity of scRNAseq data for cell type identification, which is inspired by similarities between gene system and natural language system. It contains two steps: 1) transform gene sequences into gene sentences by ranking genes in terms of their expression values; 2) employ the word2vec technique to learn gene embeddings on these gene sentences. Then we build three deep learning models, namely RNNs, Attention RNNs, and Bi-directional LSTM RNNs, for cell type classification. The proposed method is evaluated on macosko2015, a large scale scRNAseq dataset with ground truth of individual cell types. Experimental results show that the proposed method performs effectively and efficiently on identifying cell types on scRNAseq data, and it can achieve promising performance even learning on limited number of genes."
9454463,Answer Selection Based on Aligned Local Composite Features and Global Features,"With the rapid increase of text information on the Internet, intelligent question answering has attracted considerable attention in many different domains. Many studies use intelligent question answering as an answer selection task and use local or global features to encode sentence representation, which has been proven to be very effective. However, due to the different characteristics of the corpora in different domains or languages, these methods usually suffer a great reduction in performance and cannot produce a sentence representation with good qualities. In this paper, we introduce Hybrid Model combining Local Composite features and Global features into a Siamese network (HM-LCGS) to alleviate the issue. The framework consists of a novel convolution neural network like architecture called local composite features convolution neural network to extract sufficient semantic information of the text from different granularity, shortcut connection to combine local composite features into pre-trained embeddings, alignment layer to mine the correlation between question and answer and bidirectional Long Short-Term Memory to encode the final sentence representation. The experimental results on InsuranceQA and cMedQA datasets show that with suitable granularity selection and embedding method, our proposed model can achieve competitive performance compared with other state-of-the-art models."
8908707,A Fusion Model-Based Label Embedding and Self-Interaction Attention for Text Classification,"Text classification is a pivotal task in NLP (Natural Language Processing), which has received widespread attention recently. Most of the existing methods leverage the power of deep learning to improve the performance of models. However, these models ignore the interaction information between all the sentences in a text when generating the current text representation, which results in a partial semantics loss. Labels play a central role in text classification. And the attention learned from text-label in the joint space of labels and words is not leveraged, leaving enough room for further improvement. In this paper, we propose a text classification method based on Self-Interaction attention mechanism and label embedding. Firstly, our method introduce BERT (Bidirectional Encoder Representation from Transformers) to extract text features. Then Self-Interaction attention mechanism is employed to obtain text representations containing more comprehensive semantics. Moreover, we focus on the embedding of labels and words in the joint space to achieve the dual-label embedding, which further leverages the attention learned from text-label. Finally, the texts are classified by the classifier according to the weighted labels representations. The experimental results show that our method outperforms other state-of-the-art methods in terms of classification accuracy."
9060280,A Novel Relationship Extraction Scheme Based on Negative Feedback Attention,"Entity relationship extraction is an important task in natural language processing (NLP). Extracting sentences expressing entity relationship from large knowledge bases (KBs) by distance supervision will cause excessive noise. The existing attention model cannot filter such noise since it is difficult to express well when turning sentences from text such as novels and fictions. We propose a novel entity relationship extraction scheme, in which we add a new negative feedback mechanism to the attention model. The involved negative feedback mechanism can improve the accuracy by considering the contextual information of the corpus to highlight the misleading effect of the turn in sentences. Moreover, in order to retain the semantic information in sentences and optimize the effect of sentence embedding, a variety of features, such as words, parts of speech, distance between non-entity words and entities, are adopted to encode sentences. Besides, the prevalent Bidirectional Long Short-Term Memory (BILSTM) is used as the sentence encoder in the proposed scheme. Experimental results show that our proposed scheme outperforms the traditional entity relationship extraction methods, the traditional attention model and the single characteristic encoder using convolutional neural network (CNN)."
9288115,On Using Composite Word Embeddings To Improve Biomedical Term Similarity,"Representation learning has gained prominence over last few years and has shown that for all underlying learning algorithm, results are vastly improved if feature representation of input is improved to capture the domain knowledge. Word embedding approaches like Word2Vec or sub-word information technique like Fast-text have shown to improve multiple NLP tasks in biomedical domain. These techniques mostly capture indirect relationships but often fail to capture deeper contextual relationships. This can be attributed to the fact that such techniques capture only short-range context defined via a co-occurrence window. In this paper we propose a novel contextual embedding for a “wide sentential context” We then generate composite word embedding achieving a multi-scale word representation. We further prove that the composite embedding performs better than the present individual state of art techniques on both intrinsic and extrinsic evaluations."
9754614,An Enhanced Method for Entity Trigger Named Entity Recognition Based on POS Tag Embedding,"In the task of Named Entity Recognition, plenty of human annotations are required in deed. However, a large number of annotations in articles are time-consuming and labor-intensive. In order to solve these problems above, an enhanced method for entity trigger named entity recognition based on POS tag embedding is proposed in this paper. Firstly, by employing lexical annotation tool, it can not only obtain the POS tag of the word, but also connect the word embedding with the POS tag embedding. Secondly, train the attention representation of sentences and triggers, and learn the semantic relationship between entity triggers and sentences based on the attention model. Lastly, the model is instructed with a new sentence attention representation as the input of the CRF (Conditional Random Fields) network. The simulation experiments explicate that the proposed can expand the semantic information of words, so as to improve the recognition ability of entities in a relatively small amount of labeled training data."
9376628,Zero-Shot Audio Classification Via Semantic Embeddings,"In this paper, we study zero-shot learning in audio classification via semantic embeddings extracted from textual labels and sentence descriptions of sound classes. Our goal is to obtain a classifier that is capable of recognizing audio instances of sound classes that have no available training samples, but only semantic side information. We employ a bilinear compatibility framework to learn an acoustic-semantic projection between intermediate-level representations of audio instances and sound classes, i.e., acoustic embeddings and semantic embeddings. We use VGGish to extract deep acoustic embeddings from audio clips, and pre-trained language models (Word2Vec, GloVe, BERT) to generate either label embeddings from textual labels or sentence embeddings from sentence descriptions of sound classes. Audio classification is performed by a linear compatibility function that measures how compatible an acoustic embedding and a semantic embedding are. We evaluate the proposed method on a small balanced dataset ESC-50 and a large-scale unbalanced audio subset of AudioSet. The experimental results show that classification performance is significantly improved by involving sound classes that are semantically close to the test classes in training. Meanwhile, we demonstrate that both label embeddings and sentence embeddings are useful for zero-shot learning. Classification performance is improved by concatenating label/sentence embeddings generated with different language models. With their hybrid concatenations, the results are improved further."
9003802,Neural Machine Translation with Acoustic Embedding,"Neural machine translation (NMT) has successfully redefined the state of the art in machine translation on several language pairs. One popular framework models the translation process end-to-end using attentional encoder-decoder architecture and treats each word in the vectors of intermediate representation. These embedding vectors are sensitive to the meaning of words and allow semantically similar words to be near each other in the vector spaces and share their statistical power. Unfortunately, the model often maps such similar words too closely, which complicates distinguishing them. Consequently, NMT systems often mistranslate words that seem natural in the context but do not reflect the content of the source sentence. Incorporating auxiliary information usually enhances the discriminability. In this research, we integrate acoustic information within NMT by multi-task learning. Here, our model learns how to embed and translate word sequences based on their acoustic and semantic differences by helping it choose the correct output word based on its meaning and pronunciation. Our experiment results show that our proposed approach provides more significant improvement than the standard text-based transformer NMT model in BLEU score evaluation."
9706965,Is An Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching,"The task of image-text matching aims to map representations from different modalities into a common joint visual-textual embedding. However, the most widely used datasets for this task, MSCOCO and Flickr30K, are actually image captioning datasets that offer a very limited set of relation-ships between images and sentences in their ground-truth annotations. This limited ground truth information forces us to use evaluation metrics based on binary relevance: given a sentence query we consider only one image as relevant. However, many other relevant images or captions may be present in the dataset. In this work, we propose two metrics that evaluate the degree of semantic relevance of retrieved items, independently of their annotated binary relevance. Additionally, we incorporate a novel strategy that uses an image captioning metric, CIDEr, to define a Semantic Adaptive Margin (SAM) to be optimized in a standard triplet loss. By incorporating our formulation to existing models, a large improvement is obtained in scenarios where available training data is limited. We also demonstrate that the performance on the annotated image-caption pairs is maintained while improving on other non-annotated relevant items when employing the full training set. The code for our new metric can be found at github.com/furkanbiten/ncs_metric and the model implementation at github.com/andrespmd/semantic_adaptive_margin."
9574076,Bangla-German Language Translation Using GRU Neural Networks,"Machine translation relates to highly autonomous software which is capable of translating source sentences into different languages. Previously some work was done in this sector where the result was comparatively low. Most of the researchers worked on common languages and none of them gave satisfactory Bilingual Evaluation Understudy (BLEU) score. Depending on these factors, we build a system of Bangla-German translator. This system can be used in various areas (i.e. reliable interpreters, business conduction, e-commerce merchandising, etc.). The system is built based on Gated Recurrent Unit (GRU) which is a gating mechanism of Recurrent Neural Network (RNN). Here, total five types of different RNN algorithms were used like Simple RNN, RNN with Embedding, Encoder-Decoder RNN, Bidirectional RNN, Hybrid RNN. All of them gave good accuracy. But the best result we got from the Hybrid model which was the combination of Embedded and Bidirectional algorithm. The accuracy was 85.69%. For further evaluation, BLEU score was used. The result of BLEU score of unigram to four gram was respectively increasing from 54.40% to 85.88%. Also the comparison between machine translated sentences and Google translated sentences showed that the system works very efficiently."
8889722,Orientation Analysis for Chinese News Based on Word Embedding and Syntax Rules,"In this era of Internet and big data, there is billions of news generated every day, and the traditional manual methods are insufficient for public opinion orientation analysis. Especially for Chinese, which has more complicated syntax and semantic structure, and there is no space between words as separator. This greatly increases the difficulty of analyzing opinion orientation. In this paper, a novel approach is proposed aiming at solving the problem of public opinion orientation analysis based on Chinese news. The approach combines word2vec, sentiment dictionaries and syntax rules, where the word2vec can map words into different vectors with finite dimensions. Through it we can calculate the cosine similarity between the words and sentiment dictionaries to get the orientation value of target words, which is helpful for calculating the orientation value of key sentences and full text. Specifically, the process consists of three steps. First, word2vec is used to train word embedding, and every word in corpus is mapped into a given vector space. Then, key sentences are extracted from news content. Finally, pre-defined syntax rules with word vector similarity are used to analyze document orientation based on key sentences. Several experiments are conducted on both closed and open datasets, and the results validate the effectiveness of the proposed approach."
9283495,An N-ary Tree-based Model for Similarity Evaluation on Mathematical Formulae,"Accurate and efficient measurements for evaluating the similarity between mathematical formulae play an important role in mathematical information retrieval. Most previous studies have focused on representing formulae in different types to catch their features and combining the traditional structure matching algorithms. This paper presents a new unsupervised model called N-ary Tree-based Formula Embedding Model (NTFEM) for the task of mathematical similarity evaluation. Using an n-ary tree structure to represent the formula, we convert the formula into a linear sequence that can be viewed as the input sentence and then embed the formula by using a word embedding model. Based on the characteristics of mathematical formulae, a weighting function is also used to get the final weighted average embedding vector. Through some experiments on NTCIR-12 Wikipedia Formula Browsing Task, our model can outperform previous formula search engines in Bpref prediction metrics. In addition, compared with traditional tree-based models, NTFEM not only improves the retrieval effect, but also greatly reduces the training time and improves training efficiency."
8993771,Topic Modeling for Short Texts via Word Embedding and Document Correlation,"Topic modeling is a widely studied foundational and interesting problem in the text mining domains. Conventional topic models based on word co-occurrences infer the hidden semantic structure from a corpus of documents. However, due to the limited length of short text, data sparsity impedes the inference process of conventional topic models and causes unsatisfactory results on short texts. In fact, each short text usually contains a limited number of topics, and understanding semantic content of short text needs to the relevant background knowledge. Inspired by the observed information, we propose a regularized non-negative matrix factorization topic model for short texts, named TRNMF. The proposed model leverages pre-trained distributional vector representation of words to overcome the data sparsity problem of short texts. Meanwhile, the method employs the clustering mechanism under document-to-topic distributions during the topic inference by using Gibbs Sampling Dirichlet Multinomial Mixture model. TRNMF integrates successfully both word co-occurrence regularization and sentence similarity regularization into topic modeling for short texts. Through extensive experiments on constructed real-world short text corpus, experimental results show that TRNMF can achieve better results than the state-of-the-art methods in term of topic coherence measure and text classification task."
8865646,Easy-to-Deploy API Extraction by Multi-Level Feature Embedding and Transfer Learning,"Application Programming Interfaces (APIs) have been widely discussed on social-technical platforms (e.g., Stack Overflow). Extracting API mentions from such informal software texts is the prerequisite for API-centric search and summarization of programming knowledge. Machine learning based API extraction has demonstrated superior performance than rule-based methods in informal software texts that lack consistent writing forms and annotations. However, machine learning based methods have a significant overhead in preparing training data and effective features. In this paper, we propose a multi-layer neural network based architecture for API extraction. Our architecture automatically learns character-, word- and sentence-level features from the input texts, thus removing the need for manual feature engineering and the dependence on advanced features (e.g., API gazetteers) beyond the input texts. We also propose to adopt transfer learning to adapt a source-library-trained model to a target-library, thus reducing the overhead of manual training-data labeling when the software text of multiple programming languages and libraries need to be processed. We conduct extensive experiments with six libraries of four programming languages which support diverse functionalities and have different API-naming and API-mention characteristics. Our experiments investigate the performance of our neural architecture for API extraction in informal software texts, the importance of different features, the effectiveness of transfer learning. Our results confirm not only the superior performance of our neural architecture than existing machine learning based methods for API extraction in informal software texts, but also the easy-to-deploy characteristic of our neural architecture."
9129095,An Approach Towards Enhancement of Review Summarization Using Sequence Model And Word Embedding,"Traditionally, extraction-based text summarization techniques have simply relied upon statistical measures for extraction of major points in a document. With popularization of deep learning in solving problems in the field of Natural Language Processing, there is a need to incorporate deep learning to improve the effectiveness of summarization methods. In this paper, we explore the results and findings of our approach to improve review summarization as well as to delineate the capabilities of these language models in improving Natural Language Understanding. The aim of our summarization approach is to generate a gist of customers' feedback of a product which would provide vendors and other customers with the necessary understanding in order to make informed purchases. Our proposed summarization approach is split into four major steps: Keyword Extraction, Sentiment Analysis, Adjective Mining and Sentence Extraction. Afterwards, all the outputs of these steps are combined to provide the user with summary of all reviews about a product."
8351927,PathEmb: Random Walk Based Document Embedding for Global Pathway Similarity Search,"Pathway analysis is a cornerstone of system biology. In particular, pathway similarity search plays a key role in establishing structural, functional, and evolutionary relationships between different biological entities. Given a query pathway as well as a database, a pathway similarity search aims to identify novel pathways that are homologous to the query pathway. Unfortunately, the pathway similarity search is computationally inefficient due to the NP-complete graph isomorphism problem. In this study, we introduce a novel algorithmic framework for pathway similarity search, named PathEmb (Pathway Embedding), which is analogous to the Skip-gram model where each pathway is represented as a “document.” PathEmb exploits a second order random walk strategy to explore diverse pathway patterns. All signaling paths traversed from random walks are regarded as “sentences,” which are constituted as a “document” afterwards. Then, the “document” pattern for the individual pathway is mapped into a low-dimensional feature space for downstream tasks. Furthermore, PathEmb is a topology-free pathway similarity search algorithm, which is feasible to handle any pathway with arbitrary structure. We have extensively evaluated PathEmb and other cuttingedge methods on three pathway datasets. The experimental results demonstrate that PathEmb outperforms the existing methods in terms of computational efficiency and search accuracy. The source code of PathEmb are freely available online https://github.com/zhangjiaobxy/PathEmb."
9207554,Deep Embedding for Relation Extraction on Insufficient Labelled Data,"Many recently proposed relation extraction methods are based on distantly supervised learning. They use data from existing knowledge bases as training data. Although the methods solve the problem of insufficient labelled data and are highly scalable, they suffer from a large amount of incorrectly labelled data. Instead of using these distantly supervised approaches, this paper proposes an alternative relation extraction method. It firstly performs unsupervised learning to train relation embeddings by a neural network. As the relation embeddings encode the semantic information of the original sentences and their entity pairs, these embeddings can be efficiently classified by supervised learning. Since the relation embedding phase is based on unsupervised learning, labelled data is only required in the classification phase. Experiments show that our proposed approach significantly outperforms the state-of-the-art baselines when labelled training data is insufficient."
9335022,Automatic Short Answer Grading With SemSpace Sense Vectors and MaLSTM,"Automatic assessment of exams is widely preferred by educators than multiple-choice exams because of its efficiency in measuring student performance, lack of subjectivity when evaluating student response, and faster evaluation time than the time consuming manual evaluation. In this study, a new approach for the Automatic Short Answer Grading (ASAG) is proposed using MaLSTM and the sense vectors obtained by SemSpace, a synset based sense embedding method built leveraging WordNet. Synset representations of the Student's answers and reference answers are given as input into parallel LSTM architecture, they are transformed into sentence representations in the hidden layer and the vectorial similarity of these two representation vectors are computed with Manhattan Similarity in the output layer. The proposed approach has been tested using the Mohler ASAG dataset and successful results are obtained in terms of Pearson (r) correlation and RMSE. Also, the proposed approach has been tested as a case study using a specific dataset (CU-NLP) created from the exam of the “Natural Language Processing” course in the Computer Engineering Department of Cukurova University. And it has achieved a successful correlation. The results obtained in the experiments show that the proposed system can be used efficiently and effectively in context-dependent ASAG tasks."
9045404,Semantic Enhancement and Multi-level Label Embedding for Chinese News Headline Classification,"News headline classification is a specific example of short text classification, which aims to extract semantic information from the short text and classify it accurately. It can provide a fast classification method for data of various kinds of news media, thus arousing the common concern of academia and industry. Most short text classification methods are based on the semantic expansion of external knowledge, which is unable to expansion dynamically in real time and make full use of label information. To overcome these problems, we propose a novel method which consists of three parts: semantic enhancement, multi-dimensional feature fusion network and multi-level label embedding. Firstly, the word-level semantic information are embedded into the character encoding from pre-train model to enhance semantic features. Secondly, both of Bi-GRU and multi-scale CNN are used to extract sequence and local features of text to enhance the semantic representation of the sentence. Furthermore, the multi-level label embedding is used to filter textual vector and assist classification in the word and sentence level respectively. Experimental results on NLPCC 2017 Chinese news headline classification task show that our model achieves 84.74% of accuracy and 84.75% of F1, improves over the best baseline model by 1.5% and 1.6%, respectively, and reaches the state-of-the-art performance."
9347584,Keyphrase Extraction in Russian and English Scientific Articles Using Sentence Embeddings,"Keyphrases provide an overview of the articles, making it a powerful tool for categorizing scientific articles. This paper introduces and describes our supervised machine learning model for automatic keywords extraction. The model calculates features from traditional statistical metrics and new state-of-the-art sentence embeddings to predict a confidence score annotating conformity of keyphrase candidate. The model is tested on corpora of Russian as well as English scientific articles. When compared to the chosen baseline methods of the experiment, our model achieved a comparable F1 score when applied to the Russian corpora; and outperformed them when applied to the English corpora. Using F1-score as the evaluation metric, we also experimented with the model's parameters, such as the embedder and the set of features used as input. We found the pre-trained embedder that provides the best possible outcome for our task and confirmed that our model works best with the full set of features - non of the input to the model is redundant. For future works, we set our goal on deploying the model on existing system. Moreover, we suggest training a delicated embedding module to improve the model performance when working with articles written in Russian."
9031480,Automatic Image Recommendation for Economic Topics using Visual and Semantic Information,"This paper proposes an image recommendation system for automatic report generation on economic topics. For a given specific headline query on daily economic events, the proposed system collects candidate images through a public search engine and choose the most appropriate one for summary report on the event. The proposed system is composed of two deep learning-based modules of different modalities: image filtering module and text matching module. In the image filtering module, an image classifier network is adopted to filter out non-photo images such as graph and tables. In the text matching module, a sentence embedding network is adopted to get text query vector and image caption vector and calculate their matching scores. By analyzing image and text information together, the proposed system can recommend suitable images both visually and semantically. Through computational experiments using a number of recent economic topics, we confirm that recommended images of the proposed system are more appropriate than that of conventional search engine."
8953306,Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval,"Visual-semantic embedding aims to find a shared latent space where related visual and textual instances are close to each other. Most current methods learn injective embedding functions that map an instance to a single point in the shared space. Unfortunately, injective embedding cannot effectively handle polysemous instances with multiple possible meanings; at best, it would find an average representation of different meanings. This hinders its use in real-world scenarios where individual instances and their cross-modal associations are often ambiguous. In this work, we introduce Polysemous Instance Embedding Networks (PIE-Nets) that compute multiple and diverse representations of an instance by combining global context with locally-guided features via multi-head self-attention and residual learning. To learn visual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in the multiple instance learning framework. Most existing work on cross-modal retrieval focus on image-text pairs of data. Here, we also tackle a more challenging case of video-text retrieval. To facilitate further research in video-text retrieval, we release a new dataset of 50K video-sentence pairs collected from social media, dubbed MRW (my reaction when). We demonstrate our approach on both image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our new MRW dataset."
8976951,Paraphrase Detection Using Manhattan's Recurrent Neural Networks and Long Short-Term Memory,"Natural Language Processing (NLP) is a part of artificial intelligence that can extract sentence structures from natural language. Some discussions about NLP are widely used, such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) to summarize papers with many sentences in them. Siamese Similarity is a term that applies repetitive twin network architecture to machine learning for sentence similarity. This architecture is also called Manhattan LSTM, which can be applied to the case of detecting paraphrase sentences. The paraphrase sentence must be recognized by machine learning first. Word2vec is used to convert sentences to vectors so they can be recognized in machine learning. This research has developed paraphrase sentence detection using Siamese Similarity with word2vec embedding. The experimental results showed that the amount of training data is dominant to the new data compared to the number of times and the variation in training data. Obtained data accuracy, 800,000 pairs provide accuracy reaching 99% of training data and 82.4% of new data. These results are better than the accuracy of the new data, with half of the training data only yielding 64%. While the amount of training data did not effect on training data."
9313452,Cross2Self-attentive Bidirectional Recurrent Neural Network with BERT for Biomedical Semantic Text Similarity,"Estimating the similarity of biomedical sentence pair is an important component in such natural language processing (NLP) tasks as text retrieval and text summarization with great amount of biomedical information growing. Deep learning-based approaches have been successfully applied to the task, but they often rely on traditional pre-trained context-independent word embedding. Bidirectional Encoder Representations from Transformers (BERT) is recently employed to pre-train contextualized word/sentence representation models via bidirectional Transformers, outperforming the state-of-the-art for many NLP tasks. The mutual semantic influence between sentences is important for estimating semantic textual similarity, which is neglected in existing methods including BERT. On the other hand, biomedical corpora mainly consist of syntactic complex and long sentences. Owing to the above-mentioned issues, we proposed a hybrid architecture, integrating the pre-trained BERT and downstream bidirectional recurrent neural network (bi-RNN). The proposed model enhanced the sentence semantic representation via employing the self-attention instead of global attention to perform cross attention between sentences. Meanwhile, bi-RNN reduced redundant information in the output of BERT. Experimental results show that the best fine-tuned models consistently outperform previous methods and advance the state-of-the-art for clinical semantic textual similarity in OHNLP 2018 task 2, with up to 0.6% increase in Pearson correlation coefficient."
9050793,Enabling Intelligent Environment by the Design of Emotionally Aware Virtual Assistant: A Case of Smart Campus,"With the advent of the 5G and Artificial Intelligence of Things (AIoT) era, related technologies such as the Internet of Things, big data analysis, cloud applications, and artificial intelligence have brought broad prospects to many application fields, such as smart homes, autonomous vehicles, smart cities, healthcare, and smart campus. At present, most university campus app is presented in the form of static web pages or app menus. This study mainly developed a Deep Neural Network (DNN) based emotionally aware campus virtual assistant. The main contributions of this research are: (1) This study introduces the Chinese Word Embedding to the robot dialogue system, effectively improving dialogue tolerance and semantic interpretation. (2) The traditional method of emotion identification must first tokenize the Chinese sentence, analyze the clauses and part of speech, and capture the emotional keywords before being interpreted by the expert system. Different from the traditional method, this study classifies the input directly through the convolutional neural network after the input sentence is converted into a spectrogram by Fourier Transform. (3) This study is presented in App mode, which is easier to use and economical. (4) This system provides a simple voice response interface, without the need for users to find information in complex web pages or app menus."
8924017,Automating News Summarization with Sentence Vectors Offset,"Text summaries consist of short versions of texts that convey their key aspects and help readers understand the gist of such texts without reading them in full. Generating such summaries is important for users who must sift through ever-increasing volumes of the content generated on the web. However, generating high-quality summaries is time-consuming for humans and challenging for automated systems, since it involves understanding the semantics of the underlying texts in order to extract key information. In this work, we develop an extractive text summarization method using vector offsets, which we show empirically to be able to summarize texts from an Internet news corpus with an effectiveness competitive with state-of-the-art extractive techniques."
9517671,WRS: A Novel Word-embedding Method for Real-time Sentiment with Integrated LSTM-CNN Model,"Artificial Intelligence (AI) is a research-focused technology in which Natural Language Processing (NLP) is a core technology in AI. Sentiment Analysis (SA) aims to extract and classify the people's opinions by NLP. The Machine Learning (ML) and lexicon dictionaries have limited competency to efficiently analyze massive live media data. Recently, deep learning methods significantly enrich the accuracy of recent sentiment models. However, the existing methods provide the aspect-based extraction that reduces individual word accuracy if a sentence does not follow the aspect information in real-time. Therefore, this paper proposes a novel word embedding method for the real-time sentiment (WRS) for word representation. The WRS's novelty is a novel word embedding method, namely, Word-to-Word Graph (W2WG) embedding that utilizes the Word2Vec approach. The WRS method assembles the different lexicon resources to employ the W2WG embedding method to achieve the word feature vector. Robust neural networks leverage these features by integrating LSTM and CNN to improve sentiment classification performance. LSTM is utilized to store the word sequence information for the effective real-time SA, and CNN is applied to extract the leading text features for sentiment classification. The experiments are conducted on Twitter and IMDB datasets. The results demonstrate our proposed method's effectiveness for real-time sentiment classification."
8793299,A preliminary study on topical model for multi-domain speech recognition via word embedding vector,"In this paper, we suggest a basic topical model(TM) framework to adapt speech recognition system to multi-domain and prevent topical errors. This paper employs the cosine similarities between target and context words at a spoken utterance as the topical model parameters. The TM is applied to frames having a large number of candidate words at lattice network, and it adjusts the ranking of candidate words by adding it to total cost estimated from acoustic model(AM) and language model(LM). To cover multidomain, the word embedding was trained with 5.5 billion text corpus from multi-domain. As an acoustic model and a language model, DNN-HMM and N - gram were selected. 501 sentences (10,054 words) includes 35 topics were used as an evaluation data set. As a result, the best performances were obtained by our approach, and the performance of WERR was increased up to about 4% compared with N-gram based model. The WERR increased above 10% when the word errors were correctly detected. The results show this suggestion has a possibility of adapting a model to multi-domains without sub-topic models."
9263691,Sentence Modeling via Graph Construction and Graph Neural Networks for Semantic Textual Similarity,"Recently, using graph neural networks to model the hidden features of natural language has achieved success. In this paper, a novel sentence modeling method named TextSimGNN based on graphical representation is proposed to measure the semantic textual similarity. For embedding sentences into a graphical structure, we first construct a semantic textual graph which combines textual structure information and semantic information together. Then an end-to-end graph neural network is used to measure the similarity between graph pairs. The experiments show that our method has achieved good performance in semantic textual similarity task, which proves the advantage and effectiveness of graphical representation on natural language sentence modeling."
9482260,Distant Supervised Relation Extraction based on Dense Neural Network with 2-Level Attention Mechanism,"Dense connected convolutional neural network (Dense Net) is a new architecture of deep convolutional neural network, which ensures the maximum information transmission between network layers by establishing the connection relationship between different layers. In the task of text remote supervised relation extraction, aiming at the limitation of existing neural network methods using shallow network to extract features, a deep convolution neural network model with dense connection based on attention mechanism is designed. In this model, the dense connection module and maximum pooling layer composed of 5 layer convolutional neural network are used as the sentence encoder. By combining the lexical, syntactic and semantic features of different levels, it helps the network to learn the features, so as to obtain more abundant semantic information of the input sentence. At the same time, it reduces the gradient vanishing phenomenon of deep neural network, and makes the network more capable of representing natural language Strong. Secondly, the word level attention is introduced to calculate the relevance between two entities and context words, so as to fully capture the semantic information of the entity context in the sentence; then the sentence level attention is constructed on multiple instances to reduce the problem of label error labeling; finally, the dependency inclusion relationship between different relationships is automatically learned through the attention of the relationship level. The average accuracy of the model on the NYT freebase dataset is 83.5%. Experimental results show that the model can effectively use features and improve the accuracy of remote supervised relationship extraction."
8904533,Biomedical Semantic Embeddings: Using hybrid sentences to construct biomedical word embeddings and its applications,"word embeddings is a useful method that has shown enormous success in various NLP tasks, not only in open domain but also in biomedical domain. The biomedical domain provides various domain specific resources and tools that can be exploited to improve performance of these word embeddings. However, most of the research related to word embeddings in biomedical domain focuses on analysis of model architecture, hyper-parameters and input text. In this paper, we use SemMedDB to design new sentences called `Semantic Sentences'. Then we use these sentences in addition to biomedical text as inputs to the word embedding model. This approach aims at introducing biomedical semantic types defined by UMLS, into the vector space of word embeddings. The semantically rich word embeddings presented here rivals state of the art biomedical word embedding in both semantic similarity and relatedness metrics up to 11%. We also demonstrate how these semantic types in word embeddings can be utilized."
9243610,Study of Extractive Text Summarizer Using The Elmo Embedding,"In recent times, data excessiveness has become a major problem in the field of education, news, blogs, social media, etc. Due to an increase in such a vast amount of text data, it became challenging for a human to extract only the valuable amount of data in a concise form. In other words, summarizing the text, enables human to retrieves the relevant and useful texts, Text summarizing is extracting the data from the document and generating the short or concise text of the document. One of the major approaches that are used widely is Automatic Text summarizer. Automatic text summarizer analyzes the large textual data and summarizes it into the short summaries containing valuable information of the data. Automatic text summarizer further divided into two types 1) Extractive text summarizer, 2) Abstractive Text summarizer. In this article, the extractive text summarizer approach is being looked for. Extractive text summarization is the approach in which model generates the concise summary of the text by picking up the most relevant sentences from the text document. This paper focuses on retrieving the valuable amount of data using the Elmo embedding in Extractive text summarization. Elmo embedding is a contextual embedding that had been used previously by many researchers in abstractive text summarization techniques, but this paper focus on using it in extractive text summarizer."
8714976,Automatic Keyword Extraction Using TextRank,"Summarizing and extracting keywords from textual documents is a fundamental task involving in many applications in natural language processing and related fields. This work presents an automatic keyword extraction algorithm based primarily on a weighted TextRank model. In this model, word embedding vectors are used to compute a similarity measure as an edge weight. Incorporating sentence importance scores derived from the TextRank model at a sentence level enhances an overall performance. The proposed algorithm is experimented and compared with the traditional TextRank algorithm as well as the weighted TextRank algorithm with word embedding-based weights."
8930002,Thai Dependency Parsing with Character Embedding,"Dependency parsing (DP) becomes an important part of natural language processing (NLP) applications. However, most of DP methods have been developed for English language, but not for Thai language. In addition, the existing DP methods were still unsolved the problems of long and complex sentences. Therefore, this paper proposes seven Thai DP algorithms. Five different Thai DP algorithms was developed from transition-based parsing and the other two was developed from graph-based parsing. Based on Thai-PUD and English-PUD datasets, containing both long and complex sentences, the experimental results showed that all Thai DP algorithms bundled with character embedding can outperform the baselines."
9350403,Design and Implementation of Automated Image Handwriting Sentences Recognition using Hybrid Techniques on FPGA,"The validation of documents such as recognition of optical character, the sign which is written by hand are the main drawbacks involved in the identification of human and their addresses, codes of the post written on the envelops, manuscript evaluation, understanding the transactions of money and documents of the bank that are written in the English language. The conceptual model was written by hand for the real-time application that deals with the handwritten identification enables a comprehensive computerized system to identify the data written by hand which is more efficient and is free from noise. The proposed framework consists of filters based on Probabilistic Patch (PPB), identification, and analysis of the Canny edge. With the application of a Probabilistic Patch-based filter, the recursive speckle noise and additive Gaussian noise are processed. The words in the document are obtained by using the structure of Lifting transformation, the edges of the word are identified with help of Canny edge recognition. At last, the database validates the text as correct or incorrect. With the application of the Embedded Development Kit (EDK) and Software Development Kit (SDK), the entire framework is developed. The hardware used is in this work is Virtex-5 FPGA board which is the integration of SDK and EDK with XC5VLX50T as the part name."
9262734,Text Classification Using a Bidirectional Recurrent Neural Network with an Attention Mechanism,"Text classification is an important semantic processing task in the field of natural language processing (NLP). Traditional text-classifying systems often rely on human-designed features, and most of existing methods fail to obtain the sentence information completely. In this paper, we design a recurrent structure to represent the input sentence, and apply attention mechanism to capture the most important semantic information in a sentence. We conduct experiments on the Internet Movie Database (IMDB). The experimental result shows that our method outperforms most of methods of text classification."
9160935,Deep Learning-Based Context-Sensitive Spelling Typing Error Correction,"This study aims to solve the context-sensitive spelling error problem for English documents. There are two types of spelling errors in English: non-word spelling errors and context-sensitive spelling errors. Non-word spelling errors are simple to correct because they can only be detected by matching the words in sentences with those in a dictionary; however, context-sensitive spelling errors entail increased difficulty of correction because the relationship between the word to be corrected and the surrounding context must be known. Spelling errors are considered noise in every field that uses text information, and preprocessing via document correction is necessary to minimize this problem. Context-sensitive spelling errors include homophone errors (which arise from the incorrect use of words that sound the same but are spelled differently), typographical errors (caused by striking an incorrect key on a keyboard), grammatical errors (which occur when the user does not know the correct grammatical rules), and cross word boundary errors (which arise from incorrect spacing between words). This study focuses on typographical errors. The context-sensitive spelling error problem is solved using the deep learning method, which is not an existing statistical method. The deep learning language model-based correction approach is divided into four parts, namely, correction based on word embedding information, contextual embedding information, an auto-regressive (AR) language model, and an auto-encoding (AE) language model. In this study, the best correction performance was obtained for the AE language model-based approach, and we verified its performance through a detailed correction test."
9395976,Method Of Text Summarization Using Lsa And Sentence Based Topic Modelling With Bert,"Document summarization is one such task of the natural language processing which deals with the long textual data to make its concise and fluent summaries that contains all of document relevant information. The Branch of NLP that deals with it, is automatic text summarizer. Automatic text summarizer does the task of converting the long textual document into short fluent summaries. There are generally two ways of summarizing text using automatic text summarizer, first is using extractive text summarizer and another abstractive text summarizer. This paper has demonstrated an experiment in contrast with the extractive text summarizer for summarizing the text. On the other hand topic modelling is a NLP task that extracts the relevant topic from the textual document. One such method is Latent semantic Analysis (LSA) using truncated SVD which extracts all the relevant topics from the text. This paper has demonstrated the experiment in which the proposed research work will be summarizing the long textual document using LSA topic modelling along with TFIDF keyword extractor for each sentence in a text document and also using BERT encoder model for encoding the sentences from textual document in order to retrieve the positional embedding of topics word vectors. The algorithm proposed algorithm in this paper is able to achieve the score greater than that of text summarization using Latent Dirichlet Allocation (LDA) topic modelling."
8924757,Disease Prediction and Early Intervention System Based on Symptom Similarity Analysis,"With the development of computer technology, the electronic of medical data has become a reality. Now, how to analyze the data sufficiently to predict patient's disease and conduct early intervention has become a focused research direction. The patient's intuitive expression of feelings is also an aspect that cannot be ignored. Doctors record the pathological characteristics of patients in system. In the paper, we proposed a sentence similarity model to carry out symptom similarity analysis to achieve elementary disease prediction and early intervention, which makes use of word embedding and convolutional neural network (CNN) to extract a sentence vector that contains keyword information about the patient's feelings and symptoms. In order to increase the accuracy of sentence similarity computation, this model integrated syntactic tree and neural network into the computation process. Our main innovation is to use symptom similarity analysis model for disease prediction and early intervention. In addition, the SPO kernel is also one of the innovations. Finally, the results of experiment on Microsoft research paraphrase identification (MSRP) indicated that our model can achieve an excellent performance reached 83.9% in the terms of F1 and accuracy. Furthermore, we also conducted experiments on the data of the Semantic Textual Similarity task. Pearson correlation coefficient indicates that our result is closer to the gold standard scores, which illustrates that it can extract the key information of sentence well to realize the prediction of disease and carry out early intervention."
9316527,A Character Aware Gated Convolution Model for Cloze-style Medical Machine Comprehension,"The machine comprehension research in the medical field has not received significant attention despite its importance in practical applications. This paper is concerned with the cloze-style problem which is one of the tasks of machine comprehension on the medical data. We propose a new model that combines character-level embedding, pre-trained biomedical word embedding, and convolution with attention. The model adopted by this paper achieves better performance than the state-of-the-art results on the BioMedical Knowledge Comprehension Title (BMKC_T) and BioMedical Knowledge Comprehension Last Sentence (BMKC_LS) datasets. We report 84.7% and 77.4% accuracy on BMKC_T and BMKC_LS datasets, respectively."
9565713,Self-supervised extractive text summarization for biomedical literatures,"In this study, we propose a self-supervised approach to extractive text summarization for biomedical literature. The approach uses abstracts to find the most informative content in the article, then generate a summary for training a classification model. The Sentences in the abstract and literature were first embedded using BERT. A similarity-based model was then applied to label the informative sentences for training the classifier. We used logistic regression as our classification model and used the features of sentence embedding for the classification. The results showed the feasibility of employing the abstract to perform self-supervised training of a classification model to generate extractive summarization. This approach can enable automatic generation of one or two-page executive summaries of biomedical literature to keep clinicians and biomedical researchers up to date with the latest development"
9198445,Semantic similarity between short paragraphs using Deep Learning,"Textual semantic similarity plays an increasingly important role in tasks such as information retrieval, text mining and text-based searches. Multiple approaches have been presented to enhance methods for information retrieval by understanding the underlying meaning of sentences. However, most of these focus on single line sentences. In this paper, we try to evaluate the effectiveness of these approaches to understand the semantic meaning of short paragraphs. We use an existing recurrent neural network architecture and train it using document embedding vectors to try and infer the meaning of small paragraphs consisting of one, two or three sentences. We use three different methods - Manhattan distance, Euclidean distance and cosine distance - to evaluate the performance and effectiveness of measuring the semantic similarity. The conclusion compares the performance of all three methods."
9140132,Word Embedding-based Text Processing for Comprehensive Summarization and Distinct Information Extraction,"In this paper, we propose two automated text processing frameworks specifically designed to analyze online reviews. The objective of the first framework is to summarize the reviews dataset by extracting essential sentence. This is performed by converting sentences into numerical vectors and clustering them using a community detection algorithm based on their similarity levels. Afterwards, a correlation score is measured for each sentence to determine its importance level in each cluster and assign it as a tag for that community. The second framework is based on a question-answering neural network model trained to extract answers to multiple different questions. The collected answers are effectively clustered to find multiple distinct answers to a single question that might be asked by a customer. The proposed frameworks are shown to be more comprehensive than existing reviews processing solutions."
9002783,An Automated Fact Checking System Using Deep Learning Through Word Embedding,"The increasing concern with false information has stimulated research in joint Fact Extraction and VERification (FEVER). Now we propose a system by deep learning which can help people identify the authenticity of most claims as well as providing evidences selected from knowledge source like Wikipedia. In this paper, we examine how to use deep learning method to improve the performance of the automatic fact verification system. Firstly, the inverted index of the knowledge base is established by using a Python package named Whoosh. Secondly, the claim is regularized by the Named Entity Recognition (NER) tool, and the most relevant documents are filtered based on the relevance ranking algorithm. Thirdly, top 20 relevant sentences for each claim are filtered by word embeddings. Finally, the effectiveness of each sentence and the label of claim is judged based on the two-level pre-training model. Our approach achieved a 0.89 document."
9750346,Electrophysiological Responses to Prose-Embedded Linguistic Anomalies: An EEG Study,"Language regularities build linguistic knowledge and expectations, so humans comprehend language despite various peculiarities. It has been shown that linguistic anomalies elicit electrophysiological responses, but the specific effect of different types of anomalies on brain signals is rarely studied. In this paper, we examined the electrophysiological responses caused by different kinds of prose-embedded linguistic anomalies. To this end, we recorded Electroencephalogram in two different multi-tasking experiments in the Persian language: 1. reading prose while listening to a recorded version of the prose, and 2. writing prose while listening to a recorded version of that prose. Then, we evaluated how the brain responds to linguistic peculiarities by analyzing the power of recorded EEG in different bands. We found out that grammatical errors changed the power distribution between different frequency bands and increased the percentage of the theta band. Other anomalies affected the power of the delta and theta bands; in the first experiment, an increase occurred in the right and midline central, frontal and parietal channels, whereas in the second experiment, the increase happened in almost all channels. Moreover, in the first experiment, in anomalies except for grammatical ones, a decrease was observed in the beta power of the occipital, left central, right parietal, and right temporal channels. Finally, we compared the two experiments; the writing task causes an increase in the beta and gamma power and a decrease in the delta and theta power of the right frontal, central and temporal channels in comparison to the reading task."
9172834,Context-aware Event Type Identification Based on Context Fusion and Joint Learning,"Automatic Event Type Identification from a text document is a particularly challenging task in event extraction. Consideration of the event triggers' context has been shown effective in this task. However, existing methods suffer from insufficient consideration of the semantic information in the event trigger context and the dependencies between event triggers. To fill this gap, we propose a novel joint learning model called CAED for event type identification. Given a text document, first, CAED carries out event context fusion by sending the sentence embeddings to BiLSTMs for capturing the weighted average of its hidden states and aggregates them using the self-attention mechanism to obtain the document-level context embedding. Then, CAED introduces a dynamic memory vector to record the occurrences of event triggers and their dependencies in each sentence. Finally, CAED concatenates the documentlevel context embedding, the dynamic memory vector, and the pre-trained word embedding into a joint word-level embedding. CAED carries out joint learning by sending the joint word-level embeddings to a LSTM based event type classifier that iteratively uses the dynamic memory vector to achieve Context-aware Event Type Identification. Experiment results on the CEC benchmark dataset and a case study demonstrate the superiority of CAED over six state-of-the-art event type identification models."
8684200,Knowledge Base Question Answering With Attentive Pooling for Question Representation,"This paper presents a neural network model for a knowledge base (KB)-based single-relation question answering (SR-QA). This model is composed of two main modules, i.e., entity linking and relation detection. In each module, an embedding vector is computed from the input question sentence to calculate its similarity scores with entity candidates or relation candidates. This paper focuses on attention-based question representation in SR-QA. In the entity linking module, two attentive pooling methods, inner-sentence attention and structure attention, are employed to derive question embeddings, and their performances are compared in experiments. In the relation detection module, a new attentive pooling structure, named multilevel target attention (MLTA), is proposed to utilize the multilevel descriptions of relations. In this structure, the attention weights for aggregating the hidden states of question sentences are calculated using relation candidates as queries at the relation level, word level, and character level. Then, the similarity scores for relation detection are computed by matching questions to relation candidates at all three levels. The experimental results show that our proposed model achieves a state-of-the-art accuracy of 82.29% on the simple questions dataset. Furthermore, the results of ablation tests demonstrate the effectiveness of our proposed MLTA method for question representation."
9763115,An Ensemble Model for detecting Sarcasm on Social Media,"Nowadays, sarcasm is extensively utilized in regular conversations over social media sites. There have been cases where sarcastic statements have significantly reduced the accuracy in automatically detecting sentiment analysis or fake news. The paper presents natural language processing based text embedding techniques utilizing the ensemble approach in this study. The framework proposes the generation of text embeddings from Word2Vec, Global vectors for word representation (GloVe), and long short-term memory (LSTM) models. Word2Vec and GloVe are word embeddings based generators, while LSTM considers full text. Thus, the proposed framework encapsulates small tweets and sentences scenarios to detect sarcasm. Various natural language processing approaches have been developed, but each has textual context and proximity restrictions. Thus, the proposed framework overcomes this restriction by using word and sentence embeddings and ensembling the output. Two social media datasets, the Headlines dataset and the Twitter dataset, are used to test the proposed model. Headlines datasets have long sentences, while Twitter datasets have short tweets. The accuracy of 81.4% and 88.9% are achieved, respectively. The accuracy metric values are better than previous state-of-art frameworks."
9672072,NodeSense2Vec: Spatiotemporal Context-Aware Network Embedding for Heterogeneous Urban Mobility Data,"The problem of learning latent representations of heterogeneous networks with spatial and temporal attributes has been gaining traction in recent years, given its myriad of real-world applications. Most systems with applications in the field of transportation, urban economics, medical information, online e-commerce, etc., handle big data that can be structured into Spatiotemporal Heterogeneous Networks (SHNs), thereby making efficient analysis of these networks extremely vital.In this paper, we propose a spatiotemporal context-aware network embedding framework that jointly captures the spatial regularities between objects and the sequential transition patterns of human mobility. First, we model the heterogeneous urban mobility data collected from multiple sources as an SHN using a probabilistic weighted degree centrality measure. To learn the sequential transition patterns of human mobility in urban regions, we perform meta-path constrained random walks (MPCRWs) on the constructed SHN, which captures the proximities between multi-typed objects via their rich spatiotemporal links. By treating the generated meta-path instances as sentences, we capture multiple contrastive context senses associated with nodes in an SHN produced due to multiplex of spatial and temporal dependencies between objects in urban mobility data by performing spectral graph clustering. We then map the learned contrastive contextual node senses with respective meta-path instances. Finally, we learn latent embeddings of the mapped meta-path instances by using the word2vec model Skip-gram. We evaluate the performance of our proposed model on real-world application problems. Experimental results demonstrate the effectiveness of our model over state-of-the-art alternatives."
9669595,Aspect-Level Sentiment Classification of Chinese Patient Comments Based on Pre-trained Sentiment Embedding,"With the development of information technology, online health care service platforms have collected a large amount of patient comment information. Through fine-grained sentiment classification of this information, we can provide references for patients to seek medical treatment and help doctors understand their work. Therefore, we proposed a model that integrated pre-trained emotional information and semantic information at the word and character level for aspect-level sentiment classification. Specifically, we employed adversarial learning for training sentiment word embeddings and the two-layer bidirectional long short-term memory network to extract the sentiment embedding of the entire sentence in a specific aspect. We also combined the pre-trained sentiment feature vector with the structured semantic information by linear weighting and the multi-head self-attention mechanism, enabling the model to pay more attention to the information most relevant to a given aspect category. We performed experiments on the Chinese patient comments data set constructed by our research team and the proposed model outperformed the state-of-the-art methods, which proved the effectiveness of the proposed model for aspect-level sentiment classification of Chinese patient comments."
8757726,Multi-modal Remote Sensing Image Description Based on Word Embedding and Self-Attention Mechanism,"Traditional multi-modal models are relatively weak in describing complex image content when describing and identifying objects to be identified in microwave images, the generated sentences by which are relatively simple. In this paper, a multimodal remote sensing semantic description and recognition method based on self-attention mechanism is proposed, which combined with the Ngram2vec word embedding technique. Firstly, Ngram2ve is used to mine the semantic information and context features between the pixels to be identified in the domain window and adjacent pixels. Secondly, a self-attention mechanism is introduced to further learn the internal structure information of all pixels in the neighborhood window to generate a multidimensional representation. Finally, in order to avoid the loss of information transmitted between layers, Dense nets are used to implement information flow integration, and a multi-layered independent recurrent neural network is added between each densely connected module to solve the gradient disappearance. Experimental results show that this method is superior to traditional deep learning methods in image description and recognition."
8925070,Opinion Words Extraction and Sentiment Classification with Character Based Embedding,"In recent years, sentiment analysis from customer comments has received widespread attention in deep learning and recognition computing area. In the field of fine-grained sentiment analysis, aspect level sentiment classification aims to detect the sentiment polarity towards a particular aspect in a sentence. Most of previous research in this task focus on sentiment polarity, and ignore the importance of opinion words. As the specific embodiment of sentiment, opinion words provide diversified representation of the aspect and contribute to sentiment polarity analysis. In this work, character level word embedding is applied to our model for enhanced semantic expression, and additional position attention based on opinion words is used in sentiment classification. Our work shows considerable improvement in opinion words extraction and acquires comparable results in sentiment polarity classification on SemEval 2014 datasets."
9680078,Survey of Visual-Semantic Embedding Methods for Zero-Shot Image Retrieval,"Visual-semantic embedding is an interesting research topic because it is useful for various tasks, such as visual question answering (VQA), image-text retrieval, image captioning, and scene graph generation. In this paper, we focus on zero-shot image retrieval using sentences as queries and present a survey of the technological trends in this area. First, we provide a comprehensive overview of the history of the technology, starting with a discussion of the early studies of image-to-text matching and how the technology has evolved over time. In addition, a description of the datasets commonly used in experiments and a comparison of the evaluation results of each method are presented. We also introduce the implementation available on github for use in confirming the accuracy of experiments and for further improvement. We hope that this survey paper will encourage researchers to further develop their research on bridging images and languages."
9441897,Emoji Prediction from Sentence,"Emojis are small pictures typically used in text messages via social media. The synthesis of the visual and textual quality of the same message creates a new way of conversation. Despite being commonly used in social media, from the prespective of natural language processing, the underlying semantics of emojis have gained limited attention. We explore the relationship between words and emojis in this project, researching the challenging task of predicting the emojis when conveyed by textual twitter posts. We experimented variant of word embedding techniques, and train several models such as SVC, LinearSVC, Random Forest Classifier and Decision Tree Classifier."
8907490,Visual Relationship Embedding Network for Image Paragraph Generation,"Image paragraph generation aims to produce a complete description of a given image. This task is more challenging than image captioning, which only generates one sentence to describe the entire image. Traditional paragraph generation methods usually produce paragraph descriptions based on individual regions that are detected by a Region Proposal Network (RPN). However, relationships among visual objects are either ignored or utilized in an implicit manner in previous work. In this paper, we attempt to explore more visual information through a novel paragraph generation network that explicitly incorporates visual relationship semantics when producing descriptions. First, a novel Relation Pair Generative Adversarial Network (RP-GAN) is designed to locate regions that may cover subjective or objective elements. Then, their relationships are inferred through an attention-based network. Finally, the visual features and relationship semantics of valid relation pairs are taken as inputs by a Long Short-Term Memory (LSTM) network for generating sentences. The experimental results show that by explicitly utilizing the predicted relationship information, our proposed method obtains more accurate and informative paragraph descriptions than previous methods."
8991445,Recurrent Convolutional Attention Neural Model for Sentiment Classification of short text,"Nowadays neural attention model doing good in many tasks of natural language processing (NLP). More specifically attention mechanism has been used widely with convolutional neural network (CNN) and recurrent neural network (RNN) for many task of NLP. But the sentiment classification of short text is a challenging task because it contains limited contextual information. Thus, we proposed a new recurrent convolutional attention neural model for sentiment classification of the short text by using attention mechanism with RCNN (recurrent convolutional neural network). In proposed model attention score is calculated by averaging hidden units (feature maps) generated from LSTM (long short-term memory). Then we combined this attention score with recurrent convolution-based encoded text features to get final sentence representation. Here attention will focus on important text features and recurrent convolution makes full use of limited contextual information by processing sentence representation through different windows sizes with specialized recurrent convolution operation. Validation of the proposed model is done through experimentation with three benchmark datasets i.e. MR, SSTl, and SST2. Achieved results exhibit that our model performs better than many existing baselines works on all three datasets."
8852376,Deep Reinforcement Learning for Chatbots Using Clustered Actions and Human-Likeness Rewards,"Training chatbots using the reinforcement learning paradigm is challenging due to high-dimensional states, infinite action spaces and the difficulty in specifying the reward function. We address such problems using clustered actions instead of infinite actions, and a simple but promising reward function based on human-likeness scores derived from human-human dialogue data. We train Deep Reinforcement Learning (DRL) agents using chitchat data in raw text-without any manual annotations. Experimental results using different splits of training data report the following. First, that our agents learn reasonable policies in the environments they get familiarised with, but their performance drops substantially when they are exposed to a test set of unseen dialogues. Second, that the choice of sentence embedding size between 100 and 300 dimensions is not significantly different on test data. Third, that our proposed human-likeness rewards are reasonable for training chatbots as long as they use lengthy dialogue histories of ≥10 sentences."
9423097,Improving Video Captioning with Temporal Composition of a Visual-Syntactic Embedding,"Video captioning is the task of predicting a semantic and syntactically correct sequence of words given some context video. The most successful methods for video captioning have a strong dependency on the effectiveness of semantic representations learned from visual models, but often produce syntactically incorrect sentences which harms their performance on standard datasets. In this paper, we address this limitation by considering syntactic representation learning as an essential component of video captioning. We construct a visual-syntactic embedding by mapping into a common vector space a visual representation, that depends only on the video, with a syntactic representation that depends only on Part-of-Speech (POS) tagging structures of the video description. We integrate this joint representation into an encoder-decoder architecture that we call Visual-Semantic-Syntactic Aligned Network (SemSynAN), which guides the decoder (text generation stage) by aligning temporal compositions of visual, semantic, and syntactic representations. We tested our proposed architecture obtaining state-of-the-art results on two widely used video captioning datasets: the Microsoft Video Description (MSVD) dataset and the Microsoft Research Video-to-Text (MSR-VTT) dataset."
9288209,Integrating Text Embedding with Traditional NLP Features for Clinical Relation Extraction,"Recently, text embedding techniques such as Word2Vec and BERT have produced state-of-the-art results in a wide variety of NLP tasks. As a result, traditional NLP features frequently used in Information Extraction (IE) such as POS tags, dependency relations and semantic types have received less attention. In this paper, we investigate whether traditional NLP features can be combined with word and sentence embeddings to improve relation extraction. We have explored diverse feature sets and different neural network architectures and evaluated our models on a benchmark clinical text dataset. Our new models significantly outperformed all the baselines on the same dataset."
9604738,Multitype Learning via Multimodal Data Embedding,"This paper creates a multimodal retrieval system for image and text data in a multi-type learning approach that enables text-to-image, image-to-text, text-to-text, and image-to-image retrievals. As a practical solution, a mobile application is developed in which the users can upload their images to search a description sentence for the images. The user system is created on the application, which is done with React Native, and crucial features like e-mail authentication and reset password options are added to the application. An essential database system is designed with PostgreSQL to store user information and search for the user. The multimodal embedding study is worked, and the model that recognizes multitype retrievals is formed. The image-to-text retrieval model, which is our application’s idea, is applied to the mobile application."
9070691,Temporal Relationship Extraction for Natural Language Texts by Using Deep Bidirectional Language Model,"In general, documents contain temporal information and recognizing that information is crucial in understanding the overall content of documents written in natural language. To find the temporal information, there are three tasks that capturing the time representation itself, finding out the event associated with the time representation, and extracting the temporal relationship between times or events. As inherent linguistic characteristics of the multiple languages, it is hard to capture every time information from a given sentence without considering the context of temporal relationships. In this paper, we design an artificial neural network model that extracts temporal relations, one of the tasks that extract temporal information from natural language sentences. Our proposed model is based on a deep bidirectional architecture to design temporal relationships learning from given sentences. The model separates an input single sentence into individual word tokens and converts them into embedding vectors, and then learns whether each token is a subject or an object of temporal relationship information in the given sentence. Before using models and datasets that target multiple languages, we first conduct our research on English and Korean."
9097176,Enhancing Cross-Modal Retrieval Based on Modality-Specific and Embedding Spaces,"A new approach that drastically improves cross-modal retrieval performance in vision and language (hereinafter referred to as “vision and language retrieval”) is proposed in this paper. Vision and language retrieval takes data of one modality as a query to retrieve relevant data of another modality, and it enables flexible retrieval across different modalities. Most of the existing methods learn optimal embeddings of visual and lingual information to a single common representation space. However, we argue that the forced embedding optimization results in loss of key information for sentences and images. In this paper, we propose an effective utilization of representation spaces in a simple but robust vision and language retrieval method. The proposed method makes use of multiple individual representation spaces through text-to-image and image-to-text models. Experimental results showed that the proposed approach enhances the performance of existing methods that embed visual and lingual information to a single common representation space."
8885540,Language2Pose: Natural Language Grounded Pose Forecasting,"Generating animations from natural language sentences finds its applications in a a number of domains such as movie script visualization, virtual human animation and, robot motion planning. These sentences can describe different kinds of actions, speeds and direction of these actions, and possibly a target destination. The core modeling challenge in this language-to-pose application is how to map linguistic concepts to motion animations. In this paper, we address this multimodal problem by introducing a neural architecture called Joint Language-to-Pose (or JL2P), which learns a joint embedding of language and pose. This joint embedding space is learned end-to-end using a curriculum learning approach which emphasizes shorter and easier sequences first before moving to longer and harder ones. We evaluate our proposed model on a publicly available corpus of 3D pose data and human-annotated sentences. Both objective metrics and human judgment evaluation confirm that our proposed approach is able to generate more accurate animations and are deemed visually more representative by humans than other data driven approaches."
9306250,Module Comparison Of Transformer-Tts For Speaker Adaptation Based On Fine-Tuning,"End-to-end text-to-speech (TTS) models have achieved remarkable results in recent times. However, the model requires a large amount of text and audio data for training. A speaker adaptation method based on fine-tuning has been proposed for constructing a TTS model using small scale data. Although these methods can replicate the target speaker s voice quality, synthesized speech includes the deletion and/or repetition of speech. The goal of speaker adaptation is to change the voice quality to match the target speaker ' s on the premise that adjusting the necessary modules will reduce the amount of data to be fine-tuned. In this paper, we clarify the role of each module in the Transformer-TTS process by not updating it. Specifically, we froze character embedding, encoder, layer predicting stop token, and loss function for estimating sentence ending. The experimental results showed the following: (1) fine-tuning the character embedding did not result in an improvement in the deletion and/or repetition of speech, (2) speech deletion increases if the encoder is not fine-tuned, (3) speech deletion was suppressed when the layer predicting stop token is not fine-tuned, and (4) there are frequent speech repetitions at sentence end when the loss function estimating sentence ending is omitted."
8887401,Finding Rats in Cats: Detecting Stealthy Attacks using Group Anomaly Detection,"Advanced attack campaigns span across multiple stages and stay stealthy for long time periods. There is a growing trend of attackers using off-the-shelf tools and pre-installed system applications (such as powershell and wmic) to evade the detection because the same tools are also used by system administrators and security analysts for legitimate purposes for their routine tasks. Such a dual nature of using these tools makes the analyst's task harder when it comes to spotting the difference between attack and benign activities. To start investigations, event logs can be collected from operational systems; however, these logs are generic enough and it often becomes impossible to attribute a potential attack to a specific attack group. Recent approaches in the literature have used anomaly detection techniques, which aim at distinguishing between malicious and normal behavior of computers or network systems. Unfortunately, anomaly detection systems based on point anomalies are too rigid in a sense that they could miss the malicious activity and classify the attack, not an outlier. Therefore, there is a research challenge to make better detection of malicious activities. To address this challenge, in this paper, we leverage Group Anomaly Detection (GAD), which detects anomalous collections of individual data points. Our approach is to build a neural network model utilizing Adversarial Autoencoder (AAE-alpha) in order to detect the activity of an attacker who leverages off-the-shelf tools and system applications. In addition, we also build Behavior2Vec and Command2Vec sentence embedding deep learning models specific for feature extraction tasks. We conduct extensive experiments to evaluate our models on real-world datasets collected for a period of two months. Our method discovered 2 new attack tools used by targeted attack groups and multiple instances of malicious activity. The empirical results demonstrate that our approach is effective and robust in discovering targe...
(Show More)"
9404110,Code Similarity Detection Based on Siamese Network,"At present, with the continuous expansion of software scale, problems such as plagiarism, clone and reuse in software code become increasingly prominent, and the study of code similarity plays an important role. The existing studies have problems such as inaccurate representation of code semantic information and insufficient acquisition of word vector feature information. To solve the above problems, this paper proposes a code similarity calculation model based on a deep learning framework. This method firstly expresses the source code semantics. Secondly, it uses the Siamese network to extract semantic feature information. Finally, it utilizes the cosine distance to calculate the similarity of feature vector in high-dimensional space. Experiments have proved that our method has better performance in terms of precision, recall and F 1 compared with the baseline method. For this reason, our method can effectively obtain code semantic information and improve the performance of code similarity measurement."
9794844,RuBERT Embeddings in the Task of Classifying User Posts on a Social Media,"This paper presents models for solving the problem of multiclass classification of user posts in a social media. These models are based on embeddings extracted from messages using the RuBert language model and a fully connected neural network built over it. The models presented are compared to a baseline model using long-term short-term memory neurons (LSTM). The results will improve the accuracy of the classification posts, which in turn will improve the accuracy of assessing the psychological characteristics users."
9439960,Behavior2vector: Embedding Users’ Personalized Travel Behavior to Vector,"We investigate how to effectively and efficiently embed users’ personalized travel behaviors to vectors in this paper. Based on an example scenario of travel mode choice in intelligent transportation system, three data structures representing users’ travel behaviors are defined, namely heterogeneous graph of users’ travel behaviors, user travel behavior
k
-partite graph, and personalized user travel behavior sentence set. This paper systematically analyzes the principle of existing methods and provides intuitions for the problem of learning travel behavior representation in intelligent transportation system. Then we propose the Behavior2vector, which is an improved method tailored for embedding users’ personalized travel behaviors to vectors. In our experiments, we design a travel mode choice model based on machine learning, which uses both hand-crafted basic features and embedded vector features. We further quantify the impact of various factors on travel mode choice and use travel big data to test the hypothesis of traffic assignment models, e.g., travelers always choose the path with the shortest path. In addition, we also compared with the existing graph embedding methods and essentially discussed their advantages and disadvantages."
9492136,Biomedical Text Similarity Evaluation Using Attention Mechanism and Siamese Neural Network,"It is a crucial component to estimate the similarity of biomedical sentence pair. Siamese neural network (SNN) can achieve better performance for non-biomedical corpora. However, SNN alone cannot obtain satisfactory biomedical text similarity evaluation results due to syntactic complexity and long sentences. In this paper, a cross self-attention (CSA) is proposed to design a new attention mechanism, namely self2self-attention(S2SA). Then the S2SA is introduced into SNN to construct a novel self2self-attentive siamese neural network, namely S2SA-SNN. In the S2SA-SNN, self-attention is used to learn the different weights of words and complex syntactic features in a single sentence. The means of the CSA are used to learn inherent interactive semantic information between sentences, and it employs self-attention instead of global attention to perform cross attention between sentences. Finally, three biomedical benchmark datasets of Pearson Correlation of 0.66 and 0.72/0.66 on DBMI and CDD-ful/-ref are used to test and prove the effectiveness of the S2SA-SNN. The experiment results show that the S2SA-SNN can achieve better performances with pre-trained word embedding and obtain better generalization ability than other compared methods."
9742150,Research on Classification of Kazakh Questions Integrate with Multi-feature Embedding,"Kazakh is an agglutinative language, and this feature results in data sparseness to some extent. In addition, the Kazakh question sentences lack strict grammatical rules, and the word forms are changeable and irregular. Due to this, this article proposes a CNN+BiGRU question classification model and attention mechanism that integrate multi-features based on the Kazakh language characteristics. Kazakh words and language features are used as input for the neural network. The CNN network generates high-dimensional semantic features and transmits the output to the BiGRU network. The BiGRU layer models the context information, then the Attention layer concentrates on the input features, filters out the unnecessary information, and completes the classification with SoftMax. The research in this paper shows that our model effectively integrates language features, avoids data sparsity, improves the model's performance during training, and has higher performance on the classification of Kazakh questions."
9145589,Use Chou's 5-Steps Rule With Different Word Embedding Types to Boost Performance of Electron Transport Protein Prediction Model,"Living organisms receive necessary energy substances directly from cellular respiration. The completion of electron storage and transportation requires the process of cellular respiration with the aid of electron transport chains. Therefore, the work of deciphering electron transport proteins is inevitably needed. The identification of these proteins with high performance has a prompt dependence on the choice of methods for feature extraction and machine learning algorithm. In this study, protein sequences served as natural language sentences comprising words. The nominated word embedding-based feature sets, hinged on the word embedding modulation and protein motif frequencies, were useful for feature choosing. Five word embedding types and a variety of conjoint features were examined for such feature selection. The support vector machine algorithm consequentially was employed to perform classification. The performance statistics within the 5-fold cross-validation including average accuracy, specificity, sensitivity, as well as MCC rates surpass 0.95. Such metrics in the independent test are 96.82, 97.16, 95.76 percent, and 0.9, respectively. Compared to state-of-the-art predictors, the proposed method can generate more preferable performance above all metrics indicating the effectiveness of the proposed method in determining electron transport proteins. Furthermore, this study reveals insights about the applicability of various word embeddings for understanding surveyed sequences."
8711636,Accuracy of Convolution Neural Networks for Classifying Sentiments on Movie Reviews,"Convolution Neural Networks (CNN) models have shown remarkable results for classifying text and sentiments. In this paper, we present our approach on the task of classifying movie reviews using the word embedding on a large-scale dataset provided by Stanford. We compare the accuracy of word-based CNN models for different kernel sizes with structured and unstructured sentences. We use the standard GLoVe for word embedding and use MXNet framework to perform our experiments. Our results suggest that word orderings in a sentence do not affect the classification accuracy of CNN models significantly. However, we also note that accuracy may depend on frequency of occurrences of words."
8898836,Building Type Classification from Social Media Texts via Geo-Spatial Textmining,"In this work, we present a model for building type classification from Twitter text messages (tweets) by employing geo-spatial textmining methods. First, we apply standard text pre-processing methods and convert the tweets into sentence vectors using fastText. For classification, we apply a feedforward network with two fully connected hidden layers and feed the generated sentence vectors as linguistic features. Classification results suggest that the classes are distinguishable to a certain extent with pure text even with unbalanced class distributions and a very small sample size. However, these findings also undermine, that building type classification with pure text data is a challenging task."
9008550,Language Features Matter: Effective Language Representations for Vision-Language Tasks,"Shouldn't language and vision features be treated equally in vision-language (VL) tasks? Many VL approaches treat the language component as an afterthought, using simple language models that are either built upon fixed word embeddings trained on text-only data or are learned from scratch. We conclude that language features deserve more attention, which has been informed by experiments which compare different word embeddings, language models, and embedding augmentation steps on five common VL tasks: image-sentence retrieval, image captioning, visual question answering, phrase grounding, and text-to-clip retrieval. Our experiments provide some striking results; an average embedding language model outperforms a LSTM on retrieval-style tasks; state-of-the-art representations such as BERT perform relatively poorly on vision-language tasks. From this comprehensive set of experiments we can propose a set of best practices for incorporating the language component of vision-language tasks. To further elevate language features, we also show that knowledge in vision-language problems can be transferred across tasks to gain performance with multi-task training. This multi-task training is applied to a new Graph Oriented Vision-Language Embedding (GrOVLE), which we adapt from Word2Vec using WordNet and an original visual-language graph built from Visual Genome, providing a ready-to-use vision-language embedding: http://ai.bu.edu/grovle."
8757937,Skip-Pose Vectors: Pose-based motion embedding using Encoder-Decoder models,"This paper proposes a pose-based unsupervised embedding learning method for action recognition. To classify human action based on the similarity of motions, it is important to establish a good feature space such that similar motions are mapped to similar vector representations. On the other hand, learning a feature space with this property with a supervised approach requires huge training samples, tailored supervised key-points, and action categories. Although the labeling cost of keypoints is decreasing day by day with improvement of 2D pose estimation methods, labeling video category is still problematic work due to the variety of categories, ambiguity and variations of videos. To avoid the need for such expensive category labeling, following the success of “Skip-Thought Vectors”, an unsupervised approach to model the similarity of sentences, we apply its idea to contiguous pose sequences to learn feature representations for measuring motion similarities. Thanks to handling human action as 2D poses instead of images, the model size can be small and easy to handle, and we can augment the training data by projecting 3D motion capture data to 2D. Through evaluation on the JHMDB dataset, we explore various design choices, such as whether to handle the actions as a sequence of poses or as a sequence of images. Our approach leverages pose sequences from 3D motion capture and improves its performance as much as 61.6% on JHMDB."
9451629,MABAN: Multi-Agent Boundary-Aware Network for Natural Language Moment Retrieval,"The amount of videos over the Internet and electronic surveillant cameras is growing dramatically, meanwhile paired sentence descriptions are significant clues to select attentional contents from videos. The task of natural language moment retrieval (NLMR) has drawn great interests from both academia and industry, which aims to associate specific video moments with the text descriptions figuring complex scenarios and multiple activities. In general, NLMR requires temporal context to be properly comprehended, and the existing studies suffer from two problems: (1) limited moment selection and (2) insufficient comprehension of structural context. To address these issues, a multi-agent boundary-aware network (MABAN) is proposed in this work. To guarantee flexible and goal-oriented moment selection, MABAN utilizes multi-agent reinforcement learning to decompose NLMR into localizing the two temporal boundary points for each moment. Specially, MABAN employs a two-phase cross-modal interaction to exploit the rich contextual semantic information. Moreover, temporal distance regression is considered to deduce the temporal boundaries, with which the agents can enhance the comprehension of structural context. Extensive experiments are carried out on two challenging benchmark datasets of ActivityNet Captions and Charades-STA, which demonstrate the effectiveness of the proposed approach as compared to state-of-the-art methods. The project page can be found in https://mic.tongji.edu.cn/e5/23/c9778a189731/page.htm."
9786234,Sem-TED: Semantic Twitter Event Detection and Adapting with News Stories,"Public acceptance of social networks has made the analysis of these networks essential. Event detection in these networks including Twitter is one of the most momentous subjects in the field of natural language processing and text mining. In this paper, we investigated how to link popular social media topics and news stories using transformer models and neural networks. Accordingly, this study consists of two parts: First, detecting popular topics and, second, linking them to the news. Event detection techniques have been applied to detect popular topics, while an event detection method comprises text preprocessing, text embedding using Sentence Transformer, dimension reduction using the UMAP algorithm, and grouping them using the HDBSCAN algorithm. To examine relevance or non-relevance between the news and topics, a single-layer perceptron neural network is applied, in which the output of the model indicates relevance or nonrelevance. We have implemented the mentioned parts and have investigated them on a small sampling of two known datasets. The evaluation outcomes reveal that the first part leads to an average improvement of 8% compared to the entity-based methods. Moreover, the results of the second part demonstrate that the used neural network in this study has a better performance comparing several other methods."
9706971,Video and Text Matching with Conditioned Embeddings,"We present a method for matching a text sentence from a given corpus to a given video clip and vice versa. Traditionally video and text matching is done by learning a shared embedding space and the encoding of one modality is independent of the other. In this work, we encode the dataset data in a way that takes into account the query’s relevant information. The power of the method is demonstrated to arise from pooling the interaction data between words and frames. Since the encoding of the video clip depends on the sentence compared to it, the representation needs to be recomputed for each potential match. To this end, we propose an efficient shallow neural network. Its training employs a hierarchical triplet loss that is extendable to paragraph/video matching. The method is simple, provides explainability, and achieves state-of-the-art results for both sentence-clip and video-text by a sizable margin across five different datasets: ActivityNet, DiDeMo, YouCook2, MSR-VTT, and LSMDC. We also show that our conditioned representation can be transferred to video-guided machine translation, where we improved the current results on VATEX. Source code is available at https://github.com/AmeenAli/VideoMatch."
9338931,Research on entity relationship extraction for diabetes medical literature,"At present, the research on entity relationship extraction in Chinese medical literature has made great progress. Scholars have proposed a method of remotely supervised labeling to solve the problem of time-consuming manual labeling, but this method will bring a lot of noise. In order to solve the problem of introducing a lot of noise, we propose a neural network structure based on convolutional neural networks. This model can use multi-window convolutional neural network to automatically extract sentence features, and after obtaining the sentence vector, select the sentence that is effective for the true relationship through the attention mechanism. In particular, an entity type embedding method is proposed, which is used for relationship classification by adding entity category features. We conducted experiments using 968 diabetic medical literatures. Compared with the baseline model, our model achieved very good results in the medical literature."
9734522,Semantic Augmentation Transformer Model for Ad-hoc Retrieval,"We propose SATM, a Semantic Augmentation Transformer Model for ad-hoc retrieval. SATM adapts contrastive learning, which improves the performance of semantic similarity and the ranking results of ad-hoc retrieval. It can also augment the semantic representation of sentence embeddings. Specifically, we first use an unsup-ervised contrastive learning augmentation module to learn query similarity so that the projection head can accurately capture query semantics. Then we use the trained encoder network to map queries and perform semantic similarity calculations and rankings with document embeddings. We use BERT to extract the contextual representation of the sentence, and use the augmentation module to enhance the semantics and eliminate the anisotropy of sentence embedding. Experimental results show that in the TrecQA data set, SATM has 4% and 1.4% improvements in MRR and MAP over Bert-base."
9742118,Bi-directional Joint Embedding of Encyclopedic Knowledge and Original Text for Chinese Medical Named Entity Recognition,"Named entity recognition is a basic task in natural language processing and can be used for building knowledge graph. Named entities generally refer to the entities in the text that have a specific meaning or a strong reference. In medical area, a great deal of medical information exists in the form of electronic text and we can acquire valuable part by this method. In this article, we take extra knowledge information of medical encyclopedia into account and we associate the original text in the named entity recognition task with its encyclopedic knowledge to enhance the ability of entity recognition through the establishment of the connection and interaction of the joint-network. Based on this, (1) the attention distribution based on medical encyclopedia knowledge was generated with entity embedding and context information in the sentence, which can be directly used to generate encyclopedia knowledge subnet; (2) the bi-directional joint embedding model integrates knowledge subnet and text subnet into one network so that the connection between them are considered. We conduct experiments on tow electronic medical record datasets proposed by CCKS. The experimental result shows that our proposed method has achieved a better result."
8852080,Social Network Polluting Contents Detection through Deep Learning Techniques,"Nowadays social networks are widespread used not only to enable users to share comments with other users but also as tool from which is possible to extract knowledge. As a matter of fact, social networks are increasingly considered to understand the opinion trend about a politician or related to a certain event that occurred: in general social networks have been proved useful to understand the public opinion from both governments and companies. In addition, also from the end users point of view it is difficult to identify real contents. This is the reason why in last years we are witnessing a growing interest in tools for analyzing big data gathered from social networks in order to find common opinions. In this context, content polluters on social networks make the opinion mining process difficult to browse valuable contents. In this paper we propose a method aimed to discriminate between pollute and real information from a semantic point of view. We exploit a combination of word embedding and deep learning techniques to categorize semantic similarities between (pollute and real) linguistic sentences. We experiment the proposed method on a dataset composed of real-world sentences gathered from the Twitter social network obtaining interesting results in terms of precision and recall."
9421225,Research on Text Classification Method Based on LSTM Neural Network Model,"Text classification is a process of automatically classifying test data according to given rules. Word embedding technology is based on neural probabilistic language model, which can get word vectors with rich semantic information. In the task of natural language processing, a set of excellent word vectors is the basis of all researches. In order to search and extract information from massive electronic texts, this paper constructs a LSTM neural network classification model to classify text information. LSTM can extract words and sentences with different contributions, and combine LSTM's region embedding technology to classify text. Experimental results show that, compared with traditional methods, this method has obvious improvement in performance and classification accuracy."
9146163,Hierarchical Gated Deep Memory Network With Position-Aware for Aspect-Based Sentiment Analysis,"Aspect-based sentiment analysis aims at identifying the sentiment polarity of specific aspect in the sentence. Previous work has realized the importance of the information interaction between aspect term and context. However, most existing information interaction methods are coarse-grained, which results in a certain loss of information. In addition, most methods ignore the role of position information in identifying the sentiment polarity of the aspect. To better address the two problems, we propose a novel approach, called hierarchical gated deep memory network with position-aware. Our approach has two characteristics: 1) it has fine-grained information interaction attention mechanism which models the word-level interaction between aspect and context. The sentence-to-aspect attention is used to capture the most indicative sentiment words in context. And the aspect-to-sentence attention is used to capture the most important word in the aspect term. 2) The position information is embedded as a feature in the sentence representation. Finally, we conduct sentiment classification comparative experiment on laptop and restaurant datasets. The experimental results show that our model achieves state-of-the-art performance on aspect-based sentiment analysis."
8864964,Target-Dependent Sentiment Classification With BERT,"Research on machine assisted text analysis follows the rapid development of digital media, and sentiment analysis is among the prevalent applications. Traditional sentiment analysis methods require complex feature engineering, and embedding representations have dominated leaderboards for a long time. However, the context-independent nature limits their representative power in rich context, hurting performance in Natural Language Processing (NLP) tasks. Bidirectional Encoder Representations from Transformers (BERT), among other pre-trained language models, beats existing best results in eleven NLP tasks (including sentence-level sentiment classification) by a large margin, which makes it the new baseline of text representation. As a more challenging task, fewer applications of BERT have been observed for sentiment classification at the aspect level. We implement three target-dependent variations of the BERT base model, with positioned output at the target terms and an optional sentence with the target built in. Experiments on three data collections show that our TD-BERT model achieves new state-of-the-art performance, in comparison to traditional feature engineering methods, embedding-based models and earlier applications of BERT. With the successful application of BERT in many NLP tasks, our experiments try to verify if its context-aware representation can achieve similar performance improvement in aspect-based sentiment analysis. Surprisingly, coupling it with complex neural networks that used to work well with embedding representations does not show much value, sometimes with performance below the vanilla BERT-FC implementation. On the other hand, incorporation of target information shows stable accuracy improvement, and the most effective way of utilizing that information is displayed through the experiment."
9270495,A Speech Content Retrieval Model Based on Integrated Neural Network for Natural Language Description,"The current speech content retrieval model is confronted with the problems of slow retrieval speed and low efficiency. In order to achieve accurate retrieval of specified keywords, a speech content retrieval model based on integrated neural network for natural language description is proposed. First, we construct a sentence semantic knowledge base as a comparison criterion for retrieval. The speech and audio signals to be retrieved are input into the model in the same way as the keyword signals, and the signals are pre-processed through prefiltering noise reduction, signal enhancement and other procedures. Following the principles of deep learning algorithms, an integrated convolutional neural network is constructed, and feature vectors in the acoustic and language models are extracted and the similarity between the vectors is calculated. The retrieval result of the speech content is obtained by referring to the calculation result of the similarity. It is concluded through the test experiments of the model effect that compared with the traditional retrieval model, the retrieval accuracy of the designed speech content retrieval model is improved by 16.5%, and the retrieval speed has been improved to a certain extent."
9016574,Comparative Analysis of Semantic Similarity Techniques for Medical Text,"In medical domain, physicians frequently consult information and knowledge sources for learning relevant piece of information for their clinical decisions. The sources are either internal such as electronic medical records databases or external sources such as biomedical literature. The key challenge for medical information extraction systems is finding required information, considering the explicit and implicit clinical context, with high precision and recall. Previously, many approaches were presented to exploit different aspects of information in the user query and medical text for obtaining semantic similarity at a concept, sentence, and document level, however, there doesn't exist a comparative analysis of different semantic similarity approaches for biomedical domain. This paper aims to compare different techniques used for finding semantic similarity by performing empirical analysis. We have implemented various modern semantic similarity techniques including ontological based matching and contextual word embedding."
9010396,Towards Unsupervised Image Captioning With Shared Multimodal Embeddings,"Understanding images without explicit supervision has become an important problem in computer vision. In this paper, we address image captioning by generating language descriptions of scenes without learning from annotated pairs of images and their captions. The core component of our approach is a shared latent space that is structured by visual concepts. In this space, the two modalities should be indistinguishable. A language model is first trained to encode sentences into semantically structured embeddings. Image features that are translated into this embedding space can be decoded into descriptions through the same language model, similarly to sentence embeddings. This translation is learned from weakly paired images and text using a loss robust to noisy assignments and a conditional adversarial component. Our approach allows to exploit large text corpora outside the annotated distributions of image/caption data. Our experiments show that the proposed domain alignment learns a semantically meaningful representation which outperforms previous work."
9383508,A New Dataset for Natural Language Understanding of Exercise Logs in a Food and Fitness Spoken Dialogue System,"Health and fitness are becoming increasingly important in the United States, as illustrated by the 70% of adults in the U.S. that are classified as overweight or obese, as well as globally, where obesity nearly tripled since 1975. Prior work used convolutional neural networks (CNNs) to understand a spoken sentence describing one's meal, in order to expedite the meal-logging process. However, the system lacked a complementary exercise-logging component. We have created a new dataset of 3,000 natural language exercise-logging sentences. Each token was tagged as an Exercise, Feeling, or Other, and mapped to the most relevant exercise, as well as a score of how they felt on a scale from 1 to 10. We demonstrate the following: for intent detection (i.e., logging a meal or exercise), logistic regression achieves over 99% accuracy on a held-out test set; for semantic tagging, contextual embedding models achieve 93% F1 score, outperforming conditional random field models (CRFs); and recurrent neural networks (RNNs) trained on a multiclass classification task successfully map tagged exercise and feeling segments to database matches. By connecting how the user felt while exercising to the food they ate, in the future we may provide personalized and dynamic diet recommendations."
8970831,Interpretable Feature Learning of Graphs using Tensor Decomposition,"In recent years, node embedding algorithms, which learn low dimensional vector representations for nodes in a graph, have been one of the key research interests of the graph mining community. The existing algorithms either rely on computationally expensive eigendecomposition of the large matrices, or require tuning of the word embedding-based hyperparameters as a result of representing the graph as a node sequence similar to the sentences in a document. Moreover, the latent features produced by these algorithms are hard to interpret. In this paper, we present two novel tensor decomposition-based node embedding algorithms, that can learn node features from arbitrary types of graphs: undirected, directed, and/or weighted, without relying on eigendecomposition or word embedding-based hyperparameters. Both algorithms preserve the local and global structural properties of the graph by using k-step transition probability matrices to construct third-order multidimensional arrays or tensors and perform CANDECOMP/PARAFAC (CP) decomposition in order to produce an interpretable and low dimensional vector space for the nodes. Our experiments encompass different types of graphs (undirected/directed, unweighted/weighted, sparse/dense) of different domains such as social networking and neuroscience. Our experimental evaluation proves our models to be interpretable with respect to the understandability of the feature space, precise with respect to the network reconstruction and link prediction, and accurate with respect to node classification and graph classification."
9204800,Two-Level LSTM for Sentiment Analysis With Lexicon Embedding and Polar Flipping,"Sentiment analysis is a key component in various text mining applications. Numerous sentiment classification techniques, including conventional and deep-learning-based methods, have been proposed in the literature. In most existing methods, a high-quality training set is assumed to be given. Nevertheless, constructing a high-quality training set that consists of highly accurate labels is challenging in real applications. This difficulty stems from the fact that text samples usually contain complex sentiment representations, and their annotation is subjective. We address this challenge in this study by leveraging a new labeling strategy and utilizing a two-level long short-term memory network to construct a sentiment classifier. Lexical cues are useful for sentiment analysis, and they have been utilized in conventional studies. For example, polar and negation words play important roles in sentiment analysis. A new encoding strategy, that is,
ρ
-hot encoding, is proposed to alleviate the drawbacks of one-hot encoding and, thus, effectively incorporate useful lexical cues. Moreover, the sentimental polarity of a word may change in different sentences due to label noise or context. A flipping model is proposed to model the polar flipping of words in a sentence. We compile three Chinese datasets on the basis of our label strategy and proposed methodology. Experiments demonstrate that the proposed method outperforms state-of-the-art algorithms on both benchmark English data and our compiled Chinese data."
9533695,Embedding Extra Knowledge and A Dependency Tree Based on A Graph Attention Network for Aspect-based Sentiment Analysis,"Aspect-based sentiment analysis analyses the fine-grained sentiment polarity of a particular attribute in a sentence. In addition to the fact that most papers have applied attention mechanisms and neural networks to fine-grained sentiment analysis, some studies have reported outstanding performance from graph-structured neural networks in aspect-based sentiment analysis. In this paper, we propose the use of a graph attention network (GAT) to embed external knowledge and grammatical relations (KD-GAT). First, we employ a GAT to extract the nodes and edges related to sentences in the knowledge graph. Second, we consider the influence of conjunctions and parts of speech based on the aspect-oriented syntactic dependency tree. Finally, we emphasize the effect of aspect position in the mechanism of multiple attention. Experiments on the SemEval 2014 and Twitter datasets show that our model can improve the ability to analyze sentiment and that the graph structure method can better integrate grammatical relations to understand a sentence."
8983712,Duplicate Question Management and Answer Verification System,"Management of large data sets of question papers can be cumbersome, especially when dealing with potential duplicate or erroneous questions. The addition of a natural language system that automatically handles these issues would greatly speed up the verification of such data sets. This is a tool for identifying semantic similarity between sentences in plain-text English. Handpicked features were selected which included simple structural features and word embedding features using word2vec with multiple distance metrics between the resulting sentence vectors. The model is trained on weak hardware allowing for sufficiently high accuracy on low end machines. Results demonstrate the effectiveness of boosting for improving the performance of simple learning models, allowing for complex learning in the absence of high end hardware."
9706340,Image-Text Embedding Learning via Visual and Textual Semantic Reasoning,"As a bridge between language and vision domains, cross-modal retrieval between images and texts is a hot research topic in recent years. It remains challenging because the current image representations usually lack semantic concepts in the corresponding sentence captions. To address this issue, we introduce an intuitive and interpretable reasoning model to learn a common embedding space for alignments between images and text descriptions. Specifically, our model first incorporates the semantic relationship information into visual and textual features by performing region or word relationship reasoning. Then it utilizes the gate and memory mechanism to perform global semantic reasoning on these relationship-enhanced features, select the discriminative information and gradually grow representations for the whole scene. Through the alignment learning, the learned visual representations capture key objects and semantic concepts of a scene as in the corresponding text caption. Experiments on MS-COCO and Flickr30K datasets validate that our method surpasses many recent state-of-the-arts with a clear margin. In addition to the effectiveness, our methods are also very efficient at the inference stage. Benefited from the effective overall representation learning, our methods are more than 30-75 times faster than many recent methods that rely on local matching algorithms."
9301570,Long-Text Sentiment Analysis Based on Semantic Graph,"The neural network of bilateral attention, especially Bidirectional Encoder Representations from Transformers (BERT), has achieved good results since its appearance, and has obtained high scores in various tasks of Natural Language Processing (NLP). However, Transformer completely adopts Attention mechanism and discards CNN and RNN, which also brings some problems. When analyzing long-text corpus, Transformer is usually limited by text length and cannot achieve good results. Due to the shortcoming, this paper proposed a long-text analyzing method called Semantic Graph Bidirectional Encoder Representations from Transformers (SG-BERT). SG-BERT obtains word features firstly, and then obtains sentence features, in corpus. After that, a semantic graph containing whole effective emotional will be generated, which can also be regarded as emotional labeling of long-text. Finally, semantic graph will be put into BERT to analysis sentiment. We experiment SG-BERT on Bag of Words Meets Bags of Popcorn (BoWMBoP) and online shopping 10 cats for sentiment analysis. In BoWMBoP and online shopping 10 cats 0~1000 length corpus, its ACC reached 96.3 and 95.9, respectively."
8998508,Where Did the Political News Event Happen? Primary Focus Location Extraction in Different Languages,"Political news reports are populated all over the world in various languages. It has a great value to automatically detect the geolocation from these reports for a better understanding of the associated events. Although various open-source and commercial tools exist to identify geolocation, they fail to identify at a granular level such as locality or city and they do not support most languages. Most of the techniques view the problem in terms of Named Entity Recognition (NER) and identify geolocation information at the country level for a given text. In this paper, we consider English, Spanish and Arabic news articles from different publishers. We define primary focus location as the actual location where the event occurred amongst other focus locations mentioned in the report. Our aim is to extract the primary focus location regardless of the language from articles belonging to different news agencies. We propose a mechanism to identify potential sentences containing focus locations using NER. After that, we perform sentence embedding over words from different languages and then employ a supervised classification mechanism to predict the primary focus location. We also perform bias correction over the training data using a suitable adaptation mechanism to reduce the sampling bias in training data. Our method trains a classifier using bias-corrected training data from news articles published by an agency in one language, while testing the model on news articles published by another agency in a different language. Our empirical results when compared to baseline approaches show superior performance on real-world English, Spanish and Arabic news articles."
9707265,Effect of different splitting criteria on the performance of speech emotion recognition,"Traditional speech emotion recognition (SER) eval-uations have been performed merely on a speaker-independent condition; some of them even did not evaluate their result on this condition. This paper highlights the importance of splitting training and test data for SER by script, known as sentence-open or text-independent criteria. The results show that em-ploying sentence-open criteria degraded the performance of SER. This finding implies the difficulties of recognizing emotion from speech in different linguistic information embedded in acoustic information. Surprisingly, text-independent criteria consistently performed worse than speaker+text-independent criteria. The full order of difficulties for splitting criteria on SER performances from the most difficult to the easiest is text-independent, speaker+text-independent, speaker-independent, and speaker+text-dependent, The gap between speaker+text-independent and text-independent was smaller than other criteria, strengthening the difficulties of recognizing emotion from sneech in different sentences."
9371553,Read your Circuit: Leveraging Word Embedding to Guide Logic Optimization,"To tackle the involved complexity, Electronic Design Automation (EDA) tools are broken in well-defined steps, each operating at different abstraction levels. Higher levels of abstraction shorten the flow run-time while sacrificing correlation with the physical circuit implementation. Bridging this gap between Logic Synthesis tool and Physical Design (PnR) tools is key to improve Quality of Results (QoR), while possibly shorting the time-to-market. To address this problem, in this work, we formalize logic paths as sentences, with the gates being a bag of words. Thus, we show how word embedding can be leveraged to represent generic paths and predict if a given path is likely to be critical post-PnR. We present the effectiveness of our approach, with accuracy over than 90% for our test-cases. Finally, we give a step further and introduce an intelligent and non-intrusive flow that uses this information to guide optimization. Our flow presents up to 15.53% area delay product (ADP) and 18.56% power delay product (PDP), compared to a standard flow."
9691551,Benchmarking Shallow and Deep Neural Networks for Contextual Representation of Social Data,"Representing the underlying context in text data is a much-explored research domain, where language model construction for the sizeable unstructured corpus is the central premise. To date, several deep language embedding representation techniques have been put forth for context-aware modelling of text data, focusing on word, sentence and document-level representations for specific tasks. In this paper, we experiment with shallow and deep embedding representation techniques for social media text data to predict Atherosclerotic Heart Disease (AHD) mortality rate. We employed Word2Vec, Doc2Vec, and LSTM based embedding techniques for this experimentation and analyzed the performance on standard datasets. Experimental evaluation evidence suggests that Doc2Vec, a shallow network, outperforms deep neural networks by attaining a Pearson correlation value of 0.8199 for tuned hyper-parameters, exceeding Word2Vec and Bi-LSTM models by a margin of 60 per cent."
8919467,Vietnamese Span-based Constituency Parsing with BERT Embedding,"Syntactic structure of sentences obtained from Constituency Parsing is fundamental information in many Natural Language Processing tasks. However, due to the lack of available resources and the complex linguistic features of Vietnamese, the research into Constituency Parsing has not received enough attention in this language. To the best of our knowledge, the study presented in this paper is one of the first investigations to explore this task in Vietnamese. In this work, we present a Spanbased approach which focuses on representing spans through the use of contextualized pre-trained embeddings to obtain optimal parse trees for Vietnamese sentences. The conducted experiments indicate that our system achieved promising results on the VLSP Vietnamese Treebank dataset by significantly outperforming existing methods. The results of this study support the view that encoding context information into the representation of words is effective in improving the parsing performance of Vietnamese. Consequently, this idea can be generalized to apply to other tasks such as Dependency Parsing or other low-resource languages."
9577698,Exploiting Semantic Embedding and Visual Feature for Facial Action Unit Detection,"Recent study on detecting facial action units (AU) has utilized auxiliary information (i.e., facial landmarks, relationship among AUs and expressions, web facial images, etc.), in order to improve the AU detection performance. As of now, no semantic information of AUs has yet been explored for such a task. As a matter of fact, AU semantic descriptions provide much more information than the binary AU labels alone, thus we propose to exploit the Semantic Embedding and Visual feature (SEV-Net) for AU detection. More specifically, AU semantic embeddings are obtained through both Intra-AU and Inter-AU attention modules, where the Intra-AU attention module captures the relation among words within each sentence that describes individual AU, and the Inter-AU attention module focuses on the relation among those sentences. The learned AU semantic embeddings are then used as guidance for the generation of attention maps through a cross-modality attention network. The generated cross-modality attention maps are further used as weights for the aggregated feature. Our proposed method is unique in that the semantic features are exploited as the first of this kind. The approach has been evaluated on three public AU-coded facial expression databases, and has achieved a superior performance than the state-of-the-art peer methods."
9516865,Multi-Hop Reasoning for Question Answering with Knowledge Graph,"Multi-hop Question Answering over Knowledge Graphs (KGQA) in previous studies has achieved remarkable results by exploiting the prediction property of Knowledge Graphs (KG) embedding. However, when facing Chinese sentences, its answer selection performs poorly. We improve the method for KGQA by combining the traditional method for KGQA with a lattice based CNN (LCN) model. We refine the granularity of questions and answers to make its coverage more extensive and generalizable, and expand the answer set to improve the performance in single results."
9283773,Poster: A Novel Approach for POS Tagging of Pashto Language,"Pashto is a language that belongs to the Indo-European family, mostly spoken in South Asian countries, especially in Pakistan and Afghanistan. To build software that enables us to translate Pashto sentences into various languages and building Natural Language Understand (NLU) applications to make interactive Pashto software requires a well-defined corpus and Parts of Speech (POS) tagging approach. Therefore, a well-defined corpus is developed by scraping data from different websites. We have prepared dataset according to the guidelines written for Persian and Arabic languages, as these languages are somehow similar to these languages. Training of POS tagging using BiLSTM with GloVe embedding shows the effectiveness of our proposed approach and achieved 97% accuracy."
9141874,Extracting Online Recruitment Information Based on BiLSTM-Dropout-CRF Model,"To improve the feature learning based on different job requirements, BiLSTM-Dropout-CRF (BLDC) model was proposed. Firstly, the original sentence sequences are imported into the embedding layer to obtain the word vectors. Then, BiLSTM and the dropout layer are used to learn the contextual information and key features. Finally, the CRF layer is used to accomplish the optimal sequence labeling and complete the training. For evaluating the model performance, precision, recall rate and F1 score are used to assess the extraction accuracy. The result shows that compared with traditional models, the F1 score of BLDC model severally increases 4.4% and 1.5% averagely based on two datasets about different industries and positions. It adequately illustrates that the effectiveness of online recruitment information extraction has been promoted."
9786925,B-LIAR: A novel model for handling Multiclass Fake News data utilizing a Transformer Encoder Stack-based architecture,"In today's digital era, social media is involved in purposely pushing fake news over the web to increase the readership of the people. Various techniques that are adopted for propagation are clickbait that works by posting content that attracts the users with flashy headlines to increase their advertisement reviews. Other methodologies include satire content which publishes fake news mainly for entertainment, sloppy journalism where sometimes reporters or journalists publish stories without complete reliable information which leads to misinformation to the audience. Therefore, a detection system is necessary as it can help national security agencies, investigation departments, businesses, the industry in detecting fake news in social media. In this paper, for the detection of fake news, we have used the LIAR dataset. This dataset consists of 12.8k manually labelled sentences from PolitiFact.com in various contexts. We have deployed a novel attention approach for detection using BERT embedding in our proposed model. Using our proposed model, we have reached the state-of-the-art result compared to the existing methods. The results obtained from our experiments showcase that our model can be used by researchers for further exploration."
9667761,Answer-Centric Local and Global Information Fusion for Conversational Question Generation,"Conversational Question Generation (CQG) is a new concern in Question Generation (QG) study. Recently Seq2Seq neural network model has been widely used in the QG area. CQG model is also based on the Seq2Seq neural network model. We note a problem: the CQG model's input is not a single sentence, but a long text and conversation history. Seq2Seq model can't effectively process long input, the model will generate questions not related to the answer. To solve this problem, we propose an answer-centric local and global information fusion model. We extract the evidence sentence containing the answer in the passage and encode the evidence sentence and the passage information separately. On the one hand, we add answer-centered position tags in the passage to reinforce the attention of information related to the answer. On the other hand, we put the key sentence into the question type prediction model. By combining the answer position embedding to predict the question type, and then put the predicted question types in the key sentence to guide the generation of the question. Finally, we use a gate mechanism to merge key sentence information and passage information. The experimental results show that we have achieved better results."
9477982,Turkish Dataset for Semantic Textual Similarity,"Semantic textual similarity is the task of determining how similar two texts are. In this study, we present the first Turkish evaluation benchmark dataset for semantic textual similarity. We created the dataset by translating the English STS benchmark (STSb) dataset via Google Cloud Translation API and provided various benchmark results. We used Language- Agnostic SEntence Representations (LASER), Language-agnostic BERT Sentence Embedding (LaBSE), Multilingual Universal Sentence Encoder (MUSE) and pre-trained BERT/RoBERTa models to compute sentence embeddings. We also fine-tuned pretrained BERT/RoBERTa models to compute similarity scores. We further fine tuned pre-trained BERT/RoBERTa models with SBERT architecture. In our experimental designs, we used the Turkish natural language inference (NLI-TR) dataset as well. The model performances were computed by Pearson and Spearman's correlation coefficient between the predicted similarity scores and the gold labels. The best results were obtained by fine-tuning the BERTurk model first on NLI-TR dataset, then on the STSb-TR dataset."
9010827,CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval,"Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach."
8957556,A Multi-Channel Deep Neural Network for Relation Extraction,"The task of relation recognition identifies semantic relationships between two named entities in a sentence. In neural network based models, a convolutional layer is often conducted to extract representative local features of a sentence. The convolution operation is implemented through a whole sentence, without considering the structure of a sentence. Because the task to recognize entity relation is processed in sentence level, many ambiguous phenomena (e.g., polysemy) are influential rather than in a document. Capturing structural information of a sentence is helpful to solve this problem. In this paper, a multi-channel framework is presented, which uses two named entities to divide a sentence into several channels. Each channel is stacked with layered neural networks. These channels do not interact during recurrent propagation, which enables a neural network to learn different representations. In our experiments, it outperforms the widely used position embedding approach. Comparing with the state-of-the-art approaches, its performance shows a meaningful improvement."
9383495,Protoda: Efficient Transfer Learning for Few-Shot Intent Classification,"Practical sequence classification tasks in natural language processing often suffer from low training data availability for target classes. Recent works towards mitigating this problem have focused on transfer learning using embeddings pre-trained on often unrelated tasks, for instance, language modeling. We adopt an alternative approach by transfer learning on an ensemble of related tasks using prototypical networks under the meta-learning paradigm. Using intent classification as a case study, we demonstrate that increasing variability in training tasks can significantly improve classification performance. Further, we apply data augmentation in conjunction with meta-learning to reduce sampling bias. We make use of a conditional generator for data augmentation that is trained directly using the meta-learning objective and simultaneously with prototypical networks, hence ensuring that data augmentation is customized to the task. We explore augmentation in the sentence embedding space as well as prototypical embedding space. Combining meta-learning with augmentation provides upto 6.49% and 8.53% relative F1-score improvements over the best performing systems in the 5-shot and 10-shot learning, respectively."
9355036,DeepPredict: A Zone Preference Prediction System for Online Lodging Platforms,"Online lodging platforms have become more and more popular around the world. To make a booking in these platforms, a user usually needs to select a city first, then browses among all the prospective options. To improve the user experience, understanding the zone preferences of a user's booking behavior will be helpful. In this work, we aim to predict the zone preferences of users when booking accommodations for the next travel. We have two main challenges: (1) The previous works about next information of Points Of Interest (POIs) recommendation are mainly focused on users' historical records in the same city, while in practice, the historical records of a user in the same city would be very sparse. (2) Since each city has its own specific geographical entities, it is hard to extract the structured geographical features of accommodation in different cities. Towards the difficulties, we propose DeepPredict, a zone preference prediction system. To tackle the first challenge, DeepPredict involves users' historical records in all the cities and uses a deep learning based method to process them. For the second challenge, DeepPredict uses HERE places API to get the information of POIs nearby, and processes the information with a unified way to get it. Also, the description of each accommodation might include some useful information, thus we use Sent2Vec, a sentence embedding algorithm, to get the embedding of accommodation description. Using a real-world dataset collected from Airbnb, DeepPredict can predict the zone preferences of users' next bookings with a remarkable performance. DeepPredict outperforms the state-of-the-art algorithms by 60% in macro F1-score."
9378672,Improved Word Representations Via Summed Target and Context Embeddings,"Neural embedding models are often described as having an `embedding layer', or a set of network activations that can be extracted from the model in order to obtain word or sentence representations. In this paper, we show via a modification of the well-known word2vec algorithm that relevant semantic information is contained throughout the entirety of the network, not just in the commonly-extracted hidden layer. This extra information can be extracted by summing embeddings from both the input and output weight matrices of a skip-gram model. Word embeddings generated via this method exhibit strong semantic structure, and are able to outperform traditionally extracted word2vec embeddings in a number of analogy tasks."
8819819,An Analysis on Different Document Keyword Extraction Methods,"The research document corpses grow with more and more publication in the various field of research. Keywords play an important role in grouping these documents and making it available to users. This paper gives an insight into the new emerging and ongoing models of keyword extraction. Here we have explained four new different ways for keyword extraction, that are TF-IDF, sentence embedding and graph-based models. In which graph-based models for extraction of keywords contain two different ways that are, by collective node weight and by building the graph. And in TF-IDF it shows the comparison of five different combinations of frequency measurement for extraction of keywords. These keyword extraction techniques are unsupervised methods. An unsupervised method does not need any input data other than the document itself for extracting keywords."
8868172,TEND: A Target-Dependent Representation Learning Framework for News Document,"Real-time news documents published on the Internet have global financial and political impacts. Pioneering statistical approaches investigate manually defined features to capture lexical, sentiment, and event information, which suffer from feature sparsity. As a remedy, recent work has considered learning dense vector representations for documents. Such representations are general, which can not model target-dependent scenarios, such as stance detection towards a specific claim. There has been work on target-specific word and sentence representations, but little was done on target-dependent document representation. Moreover, documents contain more potentially helpful information, but also noise compared to events and sentences. To address the above issues, we focus on models that are: 1. task-driven, which optimize the neural network representations for the end task; 2. target-specific, learning news representations by considering the influence of specific targets. In particular, we propose a novel document-level target-dependent learning framework TEND. The framework employs the information of the target and the news abstract as clues, obtaining relatively informative sentences from the entire document for our objectives. The framework assembles a document representation by integrating the news abstract representation and a weighted sum of sentence representations in the document. To the best of our knowledge, we are among the first to investigate target-dependent document representation. Existing text representation models can be easily integrated into our TEND framework, and it is general enough to be applied to different target-dependent document representation tasks. We empirically evaluate our framework on two target-dependent document-level tasks, including a cumulative abnormal return prediction task and a news stance detection task. Results show that our models give the best performances compared to state-of-the-art document embedding methods, yielding robu...
(Show More)"
9580660,Text-Based Localization of Moments in a Video Corpus,"Prior works on text-based video moment localization focus on temporally grounding the textual query in an untrimmed video. These works assume that the relevant video is already known and attempt to localize the moment on that relevant video only. Different from such works, we relax this assumption and address the task of localizing moments in a corpus of videos for a given sentence query. This task poses a unique challenge as the system is required to perform: 2) retrieval of the relevant video where only a segment of the video corresponds with the queried sentence, 2) temporal localization of moment in the relevant video based on sentence query. Towards overcoming this challenge, we propose Hierarchical Moment Alignment Network (HMAN) which learns an effective joint embedding space for moments and sentences. In addition to learning subtle differences between intra-video moments, HMAN focuses on distinguishing inter-video global semantic concepts based on sentence queries. Qualitative and quantitative results on three benchmark text-based video moment retrieval datasets - Charades-STA, DiDeMo, and ActivityNet Captions - demonstrate that our method achieves promising performance on the proposed task of temporal localization of moments in a corpus of videos."
9095628,A Robust Graph Convolutional Network for Relation Extraction by Combining Edge Information,"Graph convolutional network was widely used in natural language processing this year and achieved great success. However, existed Graph-based methods do not account for the edge information for node representation. In this paper, we propose a new graph convolutional network for relation extraction, which can embed the edge information for node representation. Specifically, we apply an edge embedding model to embed the edge label into vectors. Then we construct a new adjacent matrix which has multi-channel and can represent the edge information. During the computation of the hidden representation of the sentence, we split the adjacent matrix into several different 2D matrices where each one represents one channel matrix. Then the 2D matrices and tokens' embedding of the sentence are fed into the Attention Guided Graph Convolutional Networks (AGGCN) model to get the hidden representation of each one channel. Finally, the representations of different channels are concatenated together and then make a relation prediction through a feed-forward neural network (FFNN) and logistic regression classifier. The experiment results show that edge information contributes to the model's performance for relation extraction."
9640260,Humor Detection Using a Bidirectional Encoder Representations from Transformers (BERT) based Neural Ensemble Model,"A lot of research has been done to aim to find out what makes someone laugh in a text. In recent years, detecting humor in written sentences has shown to be a fascinating and challenging endeavor. We describe a mechanism for identifying humor in brief texts in this paper. We employ a Bidirectional Encoder Representations from Transformers (BERT) architecture because of its benefits in learning from sentence context. Our proposed methodology also uses some other embedding models e.g., Word2Vec or FastText to generate Embeddings for sentences of a given text and uses these Embeddings as inputs in a neural ensemble network. We illustrate the efficacy of this methodology. We significantly reduced our root mean squared error by using this technique."
8665517,Controlled Natural Language Framework for Generating Assertions from Hardware Specifications,"In this paper, we present a controlled natural language (CNL) framework for automatic processing and generation of assertions from hardware design specification. Current CNL systems have limitations in mapping differently worded sentences with the same meaning to the same logic structures. We aim to mitigate this limitation by developing a dependency grammar based CNL where the constructed parse tree does not follow strict surface-structure dependencies and instead extract additional relationship based on semantic information that is embedded in the grammar. In addition, current translation schemes for creating executable assertions from hardware design specifications do not provide feedback on wrongly or ambiguously written input sentences. Our natural language understanding algorithm is guided by the dependencies in the parse tree and has the capability to offer useful feedback for sentences that are not fully understood. We reported results on natural language assertions extracted from UART, Memory and AMBA AXI protocol specification documents. We successfully tested syntactic variations of these specifications as well."
9310512,Detect Turn-takings in Subtitle Streams with Semantic Recall Transformer Encoder,"Subtitles are precious dialogue text data because of similarity to human conversation, but the lack of turn structures limits their applications in many NLP tasks. The previous work takes turn-taking detection (TTD) in subtitles as a sentence-pair classification problem to predict if there is a turn-taking happened between the two adjacent utterances. The results are not good enough. For considering the dialogue context information, we innovatively take TTD as a sentence-level sequence labelling problem, predicting the turn-takings through the whole subtitle stream. First, we present a novel fine-tuning method that enables BERT to encode utterances into embedding set with effective turntaking features. Then, we propose the Semantic Recall Transformer (SRT) model to detect the turn-takings among the utterance embedding set, by taking the turn-taking features and context information into account at the same time. Compared with the baselines, it achieves state-of-the-art on both English and Chinese subtitle corpus. Moreover, we explored the impacts of the length of the subtitle stream and the number of conversation participants on our model performance, which show the performance can be further improved in the future."
8665582,Deep Neural Architecture with Character Embedding for Semantic Frame Detection,"Semantic frame detection has been extensively used for language understanding tasks, such as in dialogue systems or more recently, in Chat-bots. Traditionally, this involves two separate tasks: the detection of the semantic frame (i.e. intent detection), and the detection of frame elements (i.e. slot-filling). Recent efforts have attempted to combine the two tasks using recurrent neural networks. However, there is still room for improvement as these efforts do not efficiently model temporal dependencies. In this paper we propose a deep neural network architecture that uses long-short term memory (RNN-LSTM) with character embedding for joint modeling of intent detection and slot filling. Our results show significant improvement in slot-filling and intent detection at the sentence level over state-of-the-art semantic frame detection methods."
8835338,Text Segmentation Based on Word Embedding on Indonesian Quran Translation by Greedy with Window Approaching,"This paper proposes an approach to improve segmentation based on word embedding. The Greedy splitting is a major of the element in text segmentation. The previous weakness of this method, we need to define how many segments should be provided(K). The nature of segmentation does not provide the number of segments (K). In this research, greedy splitting was improved by applying window approaching, to minimize the greedy process by defining the number of sentences or words that would be examined. We assumed there only two segments on the local block or one splitting by getting the lowest score. We conducted the experiments by Quran as dataset and compared with other segmentation algorithms. The results showed that the proposed method was average at 0.68 (WindofDiff). Whereas TextTiling was reached 0.74, C99 at 0.743 and Original Greedy at 0.83 also the number of K segments could be searched by itself. Moreover, we found phenomena in the uselessness of stopword removal had an effect was indicated by decreased WindowDiff."
8982144,Prediction of Semantically Correct Bangla Words Using Stupid Backoff and Word-Embedding Model,"Word prediction is an essential technique used in different text entry environment to facilitate error-free writing. It also used as a helping hand for people with different types of disabilities. Word prediction technique is available in different languages. But developing an optimized Bangla word predictor is still a great research challenge. To overcome the challenge we propose a hybrid method to predict Bangla words. The stupid backoff language model is used to detect the most-probable words that may fit into the previously typed sentence by calculating the word sequence frequency. The novelty of this work is that it can provide the semantically correct words as a suggestion. The Word-Embedding model is used to maintain the semantic context of the word. To test this approach, a large corpus is built consisting of almost 0.5 million data. We compared our approach with other well-established methods. The proposed methodology surpasses them by obtaining 83% accuracy. The approach is also computationally efficient as the running time is linear with the prediction length."
9678900,A Prior Probability of Speaker Information and Emojis Embedding Approach to Sarcasm Detection,"Sarcasm language is widely presented in social media and student chats. This is not only a difficult sentiment analysis task, but also the basis for completing other follow-up tasks. However, most of the previous methods are based on complex network model design, which leads to poor traceability of inference results. Or based on the non-corresponding pre-training model, which is not taking advantage of the area where each model can do better. We propose a DialoGPT model based on pre-training on the sarcasm task, combined with the information of the sarcasm probability about the speaker, and embedded emojis in sentences to enhance recognition. The algorithm we designed is tested on the dataset named sarcasm on reddit and compared with other related methods and systems. The results show that the proposed method achieves the most advanced performance in the four indicators in basically the same time. F1-score has achieved 77, which is better than other methods, and it uses the advantages of new information content. In the field of education, the method we proposed can be used to help understand and judge students' intention and personality."
9627527,Two End-to-End Quantum-inspired Deep Neural Networks for Text Classification,"Motivated by quantum-inspired complex word embedding, interpretable complex-valued word embedding (ICWE) is proposed to design two end-to-end quantum-inspired deep neural networks (ICWE-QNN and CICWE-QNN representing convolutional complex-valued neural network based on ICWE) for binary text classification. They have the proven feasibility and effectiveness in the application of NLP and can solve the problem of text information loss in CE-Mix [1] model caused by neglecting the important linguistic features of text, since linguistic feature extraction is presented in our model with deep learning algorithms, in which gated recurrent unit (GRU) extracts the sequence information of sentences, attention mechanism makes the model focus on important words in sentences and convolutional layer captures the local features of projected matrix. The model ICWE-QNN can avoid random combination of word tokens and CICWE-QNN fully considers textual features of the projected matrix. Experiments conducted on five benchmarking classification datasets demonstrate our proposed models have higher accuracy than the compared traditional models including CaptionRep BOW, DictRep BOW and Paragram-Phrase, and they also have great performance on F1-score. Eespecially, CICWE-QNN model has higher accuracy than the quantum-inspired model CE-Mix as well for four datasets including SST, SUBJ, CR and MPQA."
9563327,Impact of Covid-19 Pandemic on Recruitment Process and its Sentiment Analysis,"Covid-19 Pandemic has affected the entire human kind in a devastating way and the effects it has caused to different sectors of life are not yet gauged. In this research paper we are investigating the effects the pandemic has had on the recruitment process in the campuses and on students and employers. The various perspectives of the recruitment process and the changes the students and the trainers had to make in the process are considered here. The various stakeholders of the process like the human resource (HR) managers, placement officers, students undergoing placements and students who are recently placed are considered here to get a 360-degree perspective of the same. A sentiment analysis of the data procured is performed to categorize the opinions into “Positive”, “Negative” and “Neutral” from the different stakeholders. We are using the rule based approach here to find the polarity of the sentences based on their scores. The study has disclosed the apprehensions of all the stakeholders regarding the shift from offline to online mode of recruitment and also training."
8679242,Pre-Training a Neural Model to Overcome Data Scarcity in Relation Extraction from Text,"Data scarcity is a major stumbling block in relation extraction. We propose an unsupervised pre-training method for extracting relational information from a huge amount of unlabeled data prior to supervised learning in the situation where hard to make golden labeled data. An objective function not requiring any labeled data is adopted during the pre-training phase, with an attempt to predict clue words crucial for inferring semantic relation types between two entities in a given sentence. The experimental result on public datasets shows that our approach achieves similar performance by using only 70% of data in a data-scarce setting."
8995326,Sentiment-Aware Short Text Classification Based on Convolutional Neural Network and Attention,"Danmaku is an emerging socio-digital media paradigm that puts anonymous, asynchronous user-generated comments on videos with rich sentiment information. This study focuses on a new type of challenging short text classification task - sentiment-aware Danmaku classification to understand the user's opinion through collective intelligence. Currently, Convolutional neural network(CNN) and Long Short Term Memory(LSTM) network-based models have poor performance in the classification of Danmaku due to the ignoration of the word's position information in a sentence and data sparsity problem. To address these limitations, this paper proposes an Attention and CNN based sentiment-aware short text classifier to advance the state-of-the-art of short-text classification. Firstly, we define a classification criterion of Danmaku intent with six categories, considering three polarities of sentiment and two language types. Then, by introducing the attention mechanism to the Bi-LSTM based model, in which the position information is preserved during training, we realize accurate Danmaku sentiment classification and generate the sentiment embedding. Lastly, to achieve accurate sentiment-aware Danmaku intent classification, a sentiment Embedding with Channel-attention Layer is introduced to a CNN based sentence classifier using the generated Danmaku sentiment embedding. Experiment results show that the Attention and Bi-LSTM based Danmaku sentiment classifier achieves a sentiment classification accuracy of 76%. Furthermore, compared with baseline models, our proposed intent classifier delivers superior performance in classifying the six intents of Danmaku. To the best of our knowledge, this is the first study that leverages deep learning and attention mechanism for the sentiment-aware Danmaku intent classification."
9469551,Sentiment Analysis of Online Product Reviews Based On SenBERT-CNN,"Sentiment analysis, also known as opinion mining, is an important area of research to analyze people's opinions. In online e-commerce marketplace like Taobao, customers are allowed to comment on different products, brands and services using text and numerical ratings. Such reviews towards a product are valuable for the improvement of the product quality as they influence consumers' purchase decisions. In this paper, we introduce a novel model, SenBERT-CNN, to analyze customer's review. In order to capture more sentiment information in sentences, SenBERT-CNN model combines a pre-trained Bidirectional Encoder Representations from Transformers (BERT) network with Convolutional Neural Network (CNN). Specifically, we use BERT structure to better express sentence semantics as a text vector, and then further extract the deep features of the sentence through a Convolutional Neural Network. The effectiveness of the proposed method is validated through a collected product reviews of mobile phone from the e-commerce website, JD.com."
9569264,Visual Relationship Learning for Cross-Modal Retrieval,"With the rapid development of deep neural networks, multimedia data has been greatly enriched during the past few years. Searching among multimedia data with a variety of modalities (i.e., cross-modal retrieval) has become a central research topic due to its ability to better express semantics compared to simple uni-modal search. These methods give amazing results in many areas, but it is difficult to deal with many objects and relationships faithfully reproducing complex sentences. In this paper, we propose a novel approach for image-sentence retrieval to overcome this limitation. Our approach leverages the semantic expressiveness of the scene graph and incorporates it into a deep neural network model for more efficient retrieval. We demonstrate the effectiveness of this approach in real-world and simulated computer vision tasks. Extensive experiments show that our method can significantly improve the accuracy of the image-sentence retrieval task."
9660505,A Multi-feature Emotion Classification Model Based on LDA Subject Target Words,"In recent years, a variety of sentiment classification models based on deep neural networks have emerged. Most of the existing models are trained based on word embedding, or rely on expensive word-level annotation, or use sentence-level annotation only. However, some important linguistic phenomena and resources have not been fully studied. Aiming at the linguistic phenomenon that a sentence may have multiple sentiments and different target words may have different sentiments, this thesis proposes a multi-feature sentiments classification model based on LDA. The model automatically extracts the subject target words through LDA, screens the global sentiment features of sentences, extracts the local sentiment features of sentences with the external sentiment vocabulary, and integrates various features with the sentiments classification model. A series of experiments on three datasets show that the multi-feature model is effective. The introduction of LDA can not only reduce the demand for labeled target words, improve the accuracy of sentiments classification, but also more accurately analyze the internal emotional trend of public opinion events."
9564203,The use of machine learning to identify the correctness of HS Code for the customs import declarations,"As an increasing volume of international trade activities around the world, the amount of cross-boarder import declarations grows rapidly, resulting in an unprecedented scale of potentially fraudulent transactions, in particular false commodity code (e.g., HS Code). The incorrect HS Code will cause duty risk and adversely impact the revenue collection. Physical investigation by the customs administrations is impractical due to the substantial quantity of declarations. This paper provides an automatic approach by harnessing the power of machine learning techniques to relief the burden of customs targeting officers. We introduced a novel model based on the off-the-shelf embedding encoder to identify the correctness of HS Code without any human effort. Determining whether the HS Code is correctly matched with commodity description is a classification task, so the labelled data is typically required. However, the lack of gold standard labelled data sets in customs domain limits the development of supervised-based approach. Our model is developed by the unsupervised mechanism and trained on the unlabelled historical declaration records, which is robust and able to be smoothly adapted by the different customs administrations. Rather than typically classifying whether the HS Code is correct or not, our model predicts the score to indicate the degree of the HS Code being correct. We have evaluated our proposed model on the ground-truth data set provided by Dutch customs officers. Results show promising performance of 71% overall accuracy."
9642079,Deep neural network based learning to rank for address standardization,"Address standardization is the process of converting and mapping free-form addresses into a standard structured format. For many business cases, the addresses are entered into the information systems by end-users. They are often noisy, uncompleted, and in different formatted styles. In this paper, we propose a deep learning-based approach to the address standardization challenge. Our key idea is to leverage a Siamese neural network model to embed raw inputs and standardized addresses into a single latent multi-dimensional space. Thus, the corresponding of the raw input address is the one with the highest-ranking score. Our experiments demonstrate that our best model achieved 95.41% accuracy, which is 6.6% improvement from the current state of the art."
9714367,Using Neural Encoder-Decoder Models With Continuous Outputs for Remote Sensing Image Captioning,"Remote sensing image captioning involves generating a concise textual description for an input aerial image. The task has received significant attention, and several recent proposals are based on neural encoder-decoder models. Most previous methods are trained to generate discrete outputs corresponding to word tokens that match the reference sentences word-by-word, thereby optimizing the generation locally at token-level instead of globally at sentence-level. This paper explores an alternative generation method based on continuous outputs, which generates sequences of embedding vectors instead of directly predicting word tokens. We argue that continuous output models have the potential to better capture the global semantic similarity between captions and images, e.g. by facilitating the use of loss functions matching different views of the data. This includes comparing representations for individual tokens and for the entire captions, and also comparing captions against intermediate image representations. We experimentally compare discrete versus continuous output captioning methods over the UCM and RSICD datasets, which are extensively used in the area despite some issues which we also discuss. Results show that the alternative encoder-decoder framework with continuous outputs can indeed lead to better results on the two datasets, compared to the standard approach based on discrete outputs. The proposed approach is also competitive against the state-of-the-art model in the area."
9684619,The Prototype Development of Thai Language Understanding based on Mental Image Directed Semantic Theory,"Nowadays, Artificial Intelligence (AI) has been embedded in various tools such as smart phones, even in those we might use unconsciously every day. However, AI technologies for automatic understanding human languages, namely, Natural Language Understanding (NLU) has been still far from its goal because of very difficult problems of semantics. This paper describes a prototype NLU system for Thai language based on Mental Image Directed Semantic Theory in order to help Thai language beginners more comprehensively. For the first step, Thai sentences of basic structures intended for amateur users were chosen for the system to process. In the system, input sentences are fragmented to detect constituent words and their grammatical relationships, and then transduced to semantic representations in a formal language named Language for Mental-image Description. The semantic interpretations are evaluated through reasoning processes such as disambiguation and entailment. Finally, the system returns animations as its understanding results of the inputs, in order to help language learners understand the text contents in a more intuitive way."
9142877,Paraphrase Detection Using Deep Neural Network Based Word Embedding Techniques,"This paper focuses on detecting paraphrase in sentences using different word vectorization techniques and finding which vectorization method is more efficient. Word vectorization is a technique which is used to retrieve information from large collection of textual data like corpus or documents by associating each word as a vector. As the textual data are massive, the problem with the text data is that it need to defined in a form of numbers for solving mathematical problems. There are elementary to composite methods to solve this problem. In this paper we are comparing different word vectorization techniques and they are, Count Vectorizer,Hashing Vectorizer, TF-IDF Vectorizer, fastText, ELMo, GloVe, BERT."
9206645,Effects of Architecture and Training on Embedding Geometry and Feature Discriminability in BERT,"Natural language processing has improved substantially in the last few years due to the increased computational power and availability of text data. Bidirectional Encoder Representations from Transformers (BERT) have further improved the performance by using an auto-encoding model that incorporates larger bidirectional contexts. However, the underlying mechanisms of BERT for its effectiveness are not well understood. In the paper we investigate how the BERT architecture and its pretraining protocol affect the geometry of its embeddings and the effectiveness of its features for classification tasks. As an autoencoding model, during pre-training, it produces representations that are context dependent and at the same time must be able to ""reconstruct"" the original input sentences. The complex interactions of the two via transformers lead to interesting geometric properties of the embeddings and subsequently affect the inherent discriminability of the resulting representations. Our experimental results illustrate that the BERT models do not produce ""effective"" contextualized representations for words and their improved performance may mainly be due to fine-tuning or classifiers that model the dependencies explicitly by encoding syntactic patterns in the training data."
9731832,Values Embedded in Legal Artificial Intelligence,"Increasingly, governments are using technological systems in the application and administration of law [6], [22]. For example, officials use computer systems to sentence criminal defendants, approve or deny government benefits, predict the location of future crimes, and disallow border entry [6], [22]. In each instance, technology is used to make substantive decisions about law, individual legal rights, or allocation of government resources. Let us refer to any automation system used in the administration or application of law as a “legal technological system.” Notably, some of these legal technological systems use machine learning and other artificial intelligence techniques to achieve their goals [13]. Let us refer to those as “legal AI systems.”"
9747801,Using Multiple Reference Audios and Style Embedding Constraints for Speech Synthesis,"The end-to-end speech synthesis model can directly take an utterance as reference audio, and generate speech from the text with prosody and speaker characteristics similar to the reference audio. However, an appropriate acoustic embedding must be manually selected during inference. Due to the fact that only the matched text and speech are used in the training process, using unmatched text and speech for inference would cause the model to synthesize speech with low content quality. In this study, we propose to mitigate these two problems by using multiple reference audios and style embedding constraints rather than using only the target audio. Multiple reference audios are automatically selected using the sentence similarity determined by Bidirectional Encoder Representations from Transformers (BERT). In addition, we use ""target"" style embedding from a pre-trained encoder as a constraint by considering the mutual information between the predicted and ""target"" style embedding. The experimental results show that the proposed model can improve the speech naturalness and content quality with multiple reference audios and can also outperform the baseline model in ABX preference tests of style similarity."
9582373,Evaluating Unsupervised Text Embeddings on Software User Feedback,"User feedback on software products has been shown to be useful for development and can be exceedingly abundant online. Many approaches have been developed to elicit requirements in different ways from this large volume of feedback, including the use of unsupervised clustering, underpinned by text embeddings. Methods for embedding text can vary significantly within the literature, highlighting the lack of a consensus as to which approaches are best able to cluster user feedback into requirements relevant groups. This work proposes a methodology for comparing text embeddings of user feedback using existing labelled datasets. Using 7 diverse datasets from the literature, we apply this methodology to evaluate both established text embedding techniques from the user feedback analysis literature (including topic modelling and word embeddings) as well as text embeddings from state of the art deep text embedding models. Results demonstrate that text embeddings produced by state of the art models, most notably the Universal Sentence Encoder (USE), group feedback with similar requirements relevant characteristics together better than other evaluated techniques across all seven datasets. These results can help researchers select appropriate embedding techniques when developing future unsupervised clustering approaches within user feedback analysis."
9063299,Deep Learning Approach to Detect Plagiarism in Sinhala Text,"In simple terms, plagiarism is taking someone else's work or ideas and passing them without giving credit to the original author. Due to the availability of a vast amount of documents on the Internet, plagiarism has become a severe problem in this digital world. Hence the need for an automatic plagiarism detection tool is inevitable. There is a right amount of existing research on automated plagiarism detection, and there are several language-dependent and language-independent plagiarism detection tools available. The lack of adequate research for Sinhala plagiarism detection and the inefficiency of plagiarism detection tools for Sinhala text motivated this research which focuses on developing a Deep learning approach for plagiarism detection in Sinhala documents. In natural language processing (NLP), Word embedding has become a popular language modelling and feature learning technique. A word embedding model is capable of representing a word or a phrase as a vector of real numbers known as a word vector. A word vector represents the semantic and syntactic similarity of a word or phrase with other words in a corpus. Further, a sentence can be represented as a vector of values using the word vectors for words in that sentence. This research work proposes a method for Sinhala plagiarism detection by representing sentences as a vector of values and comparing the sentences based on these vector of values. For plagiarised sentences, the similarity of the vector of values would be high. A word embedding model is built using a Deep learning neural network and a Sinhala text corpus as part of this study. The word2vec algorithm and the publicly available UCSC_Sinhala_News corpus are used for this purpose. In the proposed model, a simple aggregation method is used to represent a sentence as a vector of values from the word vectors for words in that sentence. The cosine similarity and soft-cosine similarity metrics are used to quantify the similarity of sentences represented a...
(Show More)"
9339154,Enhanced LSTM: a Text Matching Aggregation Model,"Text matching is widely used in various natural language tasks, such as Knowledge Inference, Search Engine and Question Answering System. In the matching aggregation algorithm, previous researches focused on the design of various complex attentions to improve the performance ability of the model on the task, but there were few researches on the sequence representation of text and the fusion of interactive information. Based on this, we propose an enhanced LSTM text matching aggregation model. First, the word vector is pre-trained by Fasttext method to solve the OOV(out of vocabulary) problem. Then an enhanced LSTM is used to encode the sentences to better represent the sequence information of the sentence pairs. Next, we extract their local interaction information by soft attention. Besides, we design a new network model combining bidirectional LSTM(BiLSTM) with Multi-window CNN(MCNN) to integrate interactive information. Finally, based on the integrated vector, a decision is made through a fully connected layer. We evaluate our model on Quora Question Pairs set and our accuracy is 88.09%, which is improved by 0.6% compared with the previous ESIM(enhanced sequential inference model) model. Besides, unlike the previous top models that use very complicated network architectures, our model is more efficient."
8633358,Semantic Descriptions of High-Resolution Remote Sensing Images,"Image captioning has attracted more and more attention in remote sensing filed since it provides more specific information than traditional tasks, such as classification. Though image captioning has gained some developments in recent years, it is difficult to describe the image in one simple sentence. To relieve the limitation, a novel captioning task is proposed and a novel framework is proposed to solve the novel task. The proposed framework uses semantic embedding to measure the image representation and the sentence representation. The captioning performance is improved by a proposed sentence representation (collective representation). Experimental results and human evaluations on three captioning data sets in remote sensing field demonstrate that the proposed framework can lead to advancement in image captioning results."
8648373,Transformer-Based Neural Network for Answer Selection in Question Answering,"Answer selection is a crucial subtask in the question answering (QA) system. Conventional avenues for this task mainly concentrate on developing linguistic tools that are limited in both performance and practicability. Answer selection approaches based on deep learning have been well investigated with the tremendous success of deep learning in natural language processing. However, the traditional neural networks employed in existing answer selection models, i.e., recursive neural network or convolutional neural network, typically suffer from obtaining the global text information due to their operating mechanisms. The recent Transformer neural network is considered to be good at extracting the global information by employing only self-attention mechanism. Thus, in this paper, we design a Transformer-based neural network for answer selection, where we deploy a bidirectional long short-term memory (BiLSTM) behind the Transformer to acquire both global information and sequential features in the question or answer sentence. Different from the original Transformer, our Transformer-based network focuses on sentence embedding rather than the seq2seq task. In addition, we employ a BiLSTM rather than utilizing the position encoding to incorporate sequential features as the universal Transformer does. Furthermore, we apply three aggregated strategies to generate sentence embeddings for question and answer, i.e., the weighted mean pooling, the max pooling, and the attentive pooling, leading to three corresponding Transformer-based models, i.e., QA-TF WP , QA-TF MP , and QA-TF AP , respectively. Finally, we evaluate our proposals on a popular QA dataset WikiQA. The experimental results demonstrate that our proposed Transformer-based answer selection models can produce a better performance compared with several competitive baselines. In detail, our best model outperforms the state-of-the-art baseline by up to 2.37%, 2.83%, and 3.79% in terms of MAP, MRR, and accuracy, respectivel...
(Show More)"
9084900,Mandarin Prosody Boundary Prediction based on Sequence-to-sequence Model,"The prediction of prosodic structure of sentences is the key for improving the naturalness of Mandarin speech synthesis. In this paper, we proposed a sequence-to-sequence (seq2seq) model-based method to improve the predictive accuracy of the prosodic boundaries from Chinese sentence. A large-scale text corpus including 100,000 Chinese sentences is collected that is manually labelled the part-of-speech and the boundaries of the prosodic words and prosodic phrases under the guidance of a linguistic expert. By analyzing the text corpus, the shallow features such as part-of-speech, word length and word embedding are selected as the input features of the seq2seq model. At the same time, a new deep feature named syntactic hierarchical number (SHN) is proposed to predict the boundary of prosodic phrases, which describes the relationship between syntactic structure and prosodic structure. Finally, we get the seq2seq model by training the labelled text corpus to predict the boundaries of prosodic words and prosodic phrases. The experimental results show that the seq2seq model achieves F1-score of 97.15% in prosodic word and 82.98% in prosodic phrase boundary prediction. Compared to the other models, our proposed method are more effective on the prediction of prosodic structure, which can be applied to the front-end of speech synthesis."
9457805,Multi-Aspect Sentiment Analysis with Latent Sentiment-Aspect Attribution,"In this paper, we introduce a new framework called the sentiment-aspect attribution module (SAAM). SAAM works on top of traditional neural networks and is designed to address the problem of multi-aspect sentiment classification and sentiment regression. The framework works by exploiting the correlations between sentence-level embedding features and variations of document-level aspect rating scores. We demonstrate several variations of our framework on top of CNN and RNN based models. Experiments on a hotel review dataset and a beer review dataset have shown SAAM can improve sentiment analysis performance over corresponding base models. Moreover, because of the way our framework intuitively combines sentence-level scores into document-level scores, it is able to provide a deeper insight into data (e.g., semi-supervised sentence aspect labeling). Hence, we end the paper with a detailed analysis that shows the potential of our models for other applications such as sentiment snippet extraction."
9270565,Global-Local Feature Fusion Mechanism for Sentiment Classification,"Short text is a common form of information and opinions in social life. The sentiment analysis of short-text is helpful to obtain the user's attitude towards items, events and services. However, because of sparse data problem of the shorttext, sentiment analysis for short-text is still in face of great challenges. To address this problem, this paper proposes a novel sentiment classification model, Cap_ONCNNA, which includes word embedding module, sentence representation module and capsule classification. Specially, in the sentence representation module, we proposed a new text representation mechanism ONCNNA. This method can generate global structure and local feature semantic representation by attention-based ON-LSTM and CNN respectively and then fuse them to enhance the sentence representation. In classification module, we are able to get more hidden information by introducing capsule network. The experiments on MR, SST1, and SST2 datasets prove our model is superior to other models. Compared with the best model of the three datasets, the accuracy of our model is improved by 0.2% (MR), 1.84% (SST1), 0.03% (SST2) respectively."
9373269,Cluster-aware Semantic Vector Learning using BERT in Natural Language Understanding,"Natural language understanding (NLU) is a core technology for implementing natural interfaces. Recently, embedding sentences and correspondence between texts as extracted semantic knowledge, called semantic frame, has shown that semantic vector representation is key for implementing or supporting robust NLU systems. However, existing studies pertain to only the relations between sentences or only the correspondence between sentences and semantic frames, and do not consider the many-to-l relationship of text-to-semantic frames and semantic clusters. Herein, we propose a novel framework that learns semantic cluster-aware vector representations using bidirectional encoder representations from transformers(BERT). A key technique is cohesion modeling for pulling paraphrase texts to semantic centroids. Another technique is separation modeling for pushing different clusters away by employing a triplet margin loss. Additionally, we propose a novel semantic frame-encoding method using bidirectional encoder representations from trans-formers(BERT). Using the proposed framework, we demonstrate that the proposed model can learn meaningful semantic vector representations."
9725106,Funny words detection via Contrastive Representations and Pre-trained Language Model,"Funniness detection of news headlines is a challenging task in computational linguistics. However, most existing works on funniness detection mainly tackle the scenario by simply judging whether a sentence is humorous, whose result is unstable due to factors such as sentence length. To solve this issue, in this paper, our idea is to fine-grained mine the detailed information of the words and the contextual relationship between different words in the sentence, which help to evaluate the correlation between keywords and the funniness of news headlines quantitatively. Specifically, we propose a funny words detection algorithm based on the contrastive representations learning and BERT model. To quantify the impact of different words on the degree of humor, we first subtract the funniness grades of the original news headlines and the funniness grades of the original news headlines with a single word replaced. Both funniness grades are predicted with a pre-trained model, which is supervised by a a threshold to limit the amount of data and ensure the validity of data. To ensure the accuracy of our prediction, we further introduce the contrastive learning to constrain the differences of news headlines before and after word replacement. Finally, according to the Root Mean Square Error (RMSE) matrix in our experiment, we develop a BERT model with mixed sequence embedding to generate a table about words and their corresponding funniness improvement about the news headlines."
9574752,Neural Topic Models for Short Text Using Pretrained Word Embeddings and Its Application To Real Data,"Latent Dirichlet Allocation (LDA) is a typical example of a topic model that estimates the latent topics of sentences. It is widely used in topic discovery, information retrieval, and document modeling. In recent years, with the advancement of research about neural networks, topic models using neural networks, such as NVLDA and ProdLDA, have been presented. Since it is easy to use accelerators such as GPUs for training these, topic modeling for large corpora can be done efficiently. Topic models are also used for short texts such as social networking sites and product reviews, where the number of words in a document is often short. This means that the co-occurrence information of words is also extremely less, and it is difficult to infer latent topics for easy understanding by humans when training with only the target corpus. In this paper, we investigated whether trained word embedding vectors on other large corpora such as Wikipedia compensate for the lack of word information in short texts. While previous studies have been conducted on long texts such as newspaper articles, we specifically checked the effect on short texts. As a result, we confirmed that under many conditions, Topic Coherence evaluation using Wikipedia was improved by using word embedding. However, we were not able to achieve stable high performance in terms of topic diversity. We also used this approach for topic modeling of tweets related to COVID-19."
9263149,Visual Question Answering for Monas Tourism Object using Deep Learning,"Visual Question Answering (VQA) is a machine learning task, given a pair of image and natural language visual question about the image, the task is to answer the question. It is known that there is no public VQA dataset currently available in Bahasa Indonesia. This research compiles a Monas VQA dataset that uses Bahasa Indonesia in the question and Monas, a memorial monument for Indonesian, as the image specific context to resolve the problem. This research also proposes methods to solve VQA using CNN for image embedding, techniques from the NLP field for sentence embedding e.g. Bag-of-Words, fastText, BERT, and BiLSTM, lastly multimodal machine learning to let both embedded information to interact with each other. The best performing model achieves 68.39% accuracy with architecture impact analysis presented."
9027096,SibNet: Sibling Convolutional Encoder for Video Captioning,"Visual captioning, the task of describing an image or a video using one or few sentences, is a challenging task owing to the complexity of understanding the copious visual information and describing it using natural language. Motivated by the success of applying neural networks for machine translation, previous work applies sequence to sequence learning to translate videos into sentences. In this work, different from previous work that encodes visual information using a single flow, we introduce a novel Sibling Convolutional Encoder (SibNet) for visual captioning, which employs a dual-branch architecture to collaboratively encode videos. The first content branch encodes visual content information of the video with an autoencoder, capturing the visual appearance information of the video as other networks often do. While the second semantic branch encodes semantic information of the video via visual-semantic joint embedding, which brings complementary representation by considering the semantics when extracting features from videos. Then both branches are effectively combined with soft-attention mechanism and finally fed into a RNN decoder to generate captions. With our SibNet explicitly capturing both content and semantic information, the proposed model can better represent the rich information in videos. To validate the advantages of the proposed model, we conduct experiments on two benchmarks for video captioning, YouTube2Text and MSR-VTT. Our results demonstrate that the proposed SibNet consistently outperforms existing methods across different evaluation metrics."
9038242,A Machine Learning Approach to Extract Keyphrases from Bengali Document using CNN-Bidirectional LSTM,"Keyphrases are single or multiple word phrases of a document which describe the principal topics of that document. These keyphrases help readers to get an overview of the document. In this paper, we proposed a system that uses the combination of Convolutional Neural Network and Bidirectional Long Short-Term Memory (BiLSTM) Recurrent Neural Network (RNN) to automatically detect keyphrases from a document. We also used some preprocessing steps to clean and generate candidates keyphrases to train the model. Convolutional Neural Network can analyze semantic meanings of sentences. Bidirectional LSTM can learn the relations among words in the sentences. A Bengali pre-trained word embedding is used in this work."
9311954,Word2Vec Duplicate Bug Records Identification Prediction Using Tensorflow,"Bug duplication reporting is one of the most widespread software problems that cause inconvenience for the internal software stakeholders. It is useful for developers to eliminate redundant bug records where the fewer bugs duplicated records in bug reports documentation the more efficiently allocated resources are set to fix and enhance the software features. In this paper, the word embedding (Word2Vec) approach is used on four different software components from the Mozilla Core dataset with different sentence types through the duplicated bug category records to compare whether two given bug record descriptions are categorized as related bugs records. Besides, this paper proposes three different similarity measures and explores the accuracy of each measure. The study results show that the approach's accuracy is proportional to the existence of similar words within any of the two given two bug records descriptions. Additionally, we found that percentage of similarity accuracy is improved by finding the closest word using the Euclidean distance method than traversing for more index adjacent values within the trained word vector array."
9689312,Improvements to Non-Intrusive Intelligibility Prediction for Reverberant Speech,"Speech intelligibility prediction (SIP) allows the prediction of intelligibility without time-consuming subjective evaluation and is being actively pursued since it has become essential to constantly monitor the intelligibility of ubiquitous speech communication. We propose a non-reference SIP method by predicting clean speech from reverberation degraded speech. Speech intelligibility was predicted from the difference between degraded and estimated clean speech. We were able to predict intelligibility with Root Mean Square Error (RMSE) between true and predicted intelligibility of 0.09, and Pearson correlation coefficient of 0.75 with the proposed method. This prediction was done using the whole sentence speech, where test words were embedded in key phrases since we are dealing with reverberations. However, intelligibility is decided by how well the keywords themselves can be differentiated. The rest of the phrase does not contribute but rather averages out the acoustic difference between the sentences. Thus, we also attempted to predict intelligibility from keyword speech only, excised from the sentence speech. The RMSE decreased to 0.07, and the correlation increased to 0.82. This is more accurate than other SIP models, such as SRMR. We further plan to expand our model to speech degraded with additive noise and reverberation."
8952216,"Discovering, Explaining and Summarizing Controversial Discussions in Community Q&A Sites","Developers often look for solutions to programming problems in community Q&A sites like Stack Overflow. Due to the crowdsourcing nature of these Q&A sites, many user-provided answers are wrong, less optimal or out-of-date. Relying on community-curated quality indicators (e.g., accepted answer, answer vote) cannot reliably identify these answer problems. Such problematic answers are often criticized by other users. However, these critiques are not readily discoverable when reading the posts. In this paper, we consider the answers being criticized and their critique posts as controversial discussions in community Q&A sites. To help developers notice such controversial discussions and make more informed choices of appropriate solutions, we design an automatic open information extraction approach for systematically discovering and summarizing the controversies in Stack Overflow and exploiting official API documentation to assist the understanding of the discovered controversies. We apply our approach to millions of java/android-tagged Stack overflow questions and answers and discover a large scale of controversial discussions in Stack Overflow. Our manual evaluation confirms that the extracted controversy information is of high accuracy. A user study with 18 developers demonstrates the usefulness of our generated controversy summaries in helping developers avoid the controversial answers and choose more appropriate solutions to programming questions."
9007875,Text-based price recommendation system for online rental houses,"Online short-term rental platforms, such as Airbnb, have been becoming popular, and a better pricing strategy is imperative for hosts of new listings. In this paper, we analyzed the relationship between the description of each listing and its price, and proposed a text-based price recommendation system called TAPE to recommend a reasonable price for newly added listings. We used deep learning techniques (e.g., feedforward network, long short-term memory, and mean shift) to design and implement TAPE. Using two chronologically extracted datasets of the same four cities, we revealed important factors (e.g., indoor equipment and high-density area) that positively or negatively affect each property's price, and evaluated our preliminary and enhanced models. Our models achieved a Root-Mean-Square Error (RMSE) of 33.73 in Boston, 20.50 in London, 34.68 in Los Angeles, and 26.31 in New York City, which are comparable to an existing model that uses more features."
8791921,Intention Understanding Model Inspired by CBC Loops,"Accurate intention understanding of the user inputs is the key to human-computer interaction (HCI). At present, more and more studies just focus on the improvement of algorithm efficiency and ignore the nature exploration of intention understanding. In humans, working memory is regarded as a cognitive system for handling a range of neuro-cognitive tasks. Because the intention understanding is a kind of human cognitive ability, in this paper we will explore the human cognitive execution mechanism and try to apply it to improve the machines' intention understanding level. First, we demonstrated a cognitive learning model called Cortico-Basal ganglia-Cerebella (CBC) loops plays an important role in the process of working memory. Then, based on the full understanding of the loops operation mechanism, we put forward a new model of intension understanding. Finally, we applied this model on speech data and compared it with other two methods. The results showed that the new model could help to get task-specific vectors and offer further gains in performance on intention understanding."
9672070,"Multilingual Financial Word Embeddings for Arabic, English and French","Natural Language Processing is increasingly being applied to analyse the text of many different types of financial documents. For many tasks, it has been shown that standard language models and tools need to be adapted to the financial domain in order to properly represent domain specific vocabulary, styles and meanings. Previous work has almost exclusively focused on English financial text, so in this paper we describe the creation of novel financial word embeddings for three languages: English, French and Arabic. In order to evaluate the effectiveness of the embeddings, we started by evaluating the English embeddings on a sentiment analysis classification task using the existing FinancialPhrase dataset and show improved performance over a standard GloVe based model using convolutional neural networks."
9834099,SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation,"We propose the (\tt SAMU{-}XLSR
): Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation learning framework. Unlike previous works on speech representation learning, which learns multilingual contextual speech embedding at the resolution of an acoustic frame (10-20ms), this work focuses on learning multimodal (speech-text) multilingual speech embedding at the resolution of a sentence (5-10s) such that the embedding vector space is semantically aligned across different languages. We combine state-of-the-art multilingual acoustic frame-level speech representation learning model \tt XLSR
with the Language Agnostic BERT Sentence Embedding (\tt LaBSE
) model to create an utterance-level multimodal multilingual speech encoder \tt SAMU{-}XLSR
. Although we train \tt SAMU{-}XLSR
with only multilingual transcribed speech data, cross-lingual speech-text and speech-speech associations emerge in its learned representation space. To substantiate our claims, we use \tt SAMU{-}XLSR
speech encoder in combination with a pre-trained \tt LaBSE
text sentence encoder for cross-lingual speech-to-text translation retrieval, and \tt SAMU{-}XLSR
alone for cross-lingual speech-to-speech translation retrieval. We highlight these applications by performing several cross-lingual text and speech translation retrieval tasks across several datasets."
8867873,Multi-Attention and Incorporating Background Information Model for Chest X-Ray Image Report Generation,"Chest X-ray images are widely used in clinical practice such as diagnosis and treatment. The automatic radiology report generation system can effectively reduce the rate of misdiagnosis and missed diagnosis. Previous studies were focused on the long text generation problem of image paragraph, ignoring the characteristics of the image and the auxiliary role of patient background information for diagnosis. In this paper, we propose a new hierarchical model with multi-attention considering the background information. The multi-attention mechanism can focus on the image's channel and spatial information simultaneously, and map it to the sentence topic. The patient's background information will be encoded by the neural network first, then it will be aggregated into a vector representation by a multi-layer perception and added to the pre-trained vanilla word embedding, which finally forms a new word embedding after fusion. Our experimental results demonstrated that the model outperforms all baselines, achieving the state-of-the-art performance in terms of accuracy."
9724298,BERTDeep-Ware: A Cross-architecture Malware Detection Solution for IoT Systems,"Malware is widely regarded as one of the most severe security threats to modern technologies. Detecting malware in the Internet of Things (IoT) infrastructures is a critical and complicated task. The complexity of this task increases with the recent growth of malware variants targeting different IoT CPU architectures since the new malware variants often use anti-forensic techniques to avoid detection and investigation. There-fore, we cannot utilize the traditional machine learning (ML) techniques that require domain knowledge and sophisticated feature engineering in detecting the unseen mal ware variants. Re-cent deep learning approaches have performed well on mal ware analysis and detection while using minimum feature engineering requirements. In this paper, we propose BERTDeep- Ware, a real-time cross-architecture malware detection solution tailored for IoT systems. BERTDeep- Ware analyzes the executable file's operation codes (OpCodes) sequence representations using Bidi-rectional Encoder Representations from Transformers (BERT) Embedding, the state-of-the-art natural language processing (NLP) approach. The extracted sentence embedding from BERT is fed into a customized hybrid multi-head CNN-BiLSTM-LocAtt model. This deep learning (DL) model combines the convolutional neural network (CNN), bidirectional long short-term memory (BiLSTM), and the local attention mechanisms (locAtt) to capture contextual features and long-term dependencies between OpCode sequences. We train and evaluate BERTDeep- Ware using the datasets created for three different CPU architectures. The performance evaluation results confirm that the proposed multi-head CNN-BiLSTM-LocAtt model produces more accurate classification results with higher detection rates and lower false positives than a number of baseline ML and DL models."
9099374,Improving Adversarial Neural Machine Translation for Morphologically Rich Language,"Generative adversarial networks (GAN) have great successes on natural language processing (NLP) and neural machine translation (NMT). However, the existing discriminator in GAN for NMT only combines two words as one query to train the translation models, which restrict the discriminator to be more meaningful and fail to apply rich monolingual information. Recent studies only consider one single reference translation during model training, this limit the GAN model to learn sufficient information about the representation of source sentence. These situations are even worse when languages are morphologically rich. In this article, an extended version of GAN model for neural machine translation is proposed to optimize the performance of morphologically rich language translation. In particular, we use the morphological word embedding instead of word embedding as input in GAN model to enrich the representation of words and overcome the data sparsity problem during model training. Moreover, multiple references are integrated into discriminator to make the model consider more context information and adapt to the diversity of different languages. Experimental results on German↔English, French↔English, Czech↔English, Finnish↔English, Turkish↔English, Chinese↔English, Finnish↔Turkish and Turkish↔Czech translation tasks demonstrate that our method achieves significant improvements over baseline systems."
9585441,Named Entity Recognition in Electric Power Metering Domain Based on Attention Mechanism,Named Entity Recognition (NER) is one key step for constructing power domain knowledge graph which is increasingly urgent in building smart grid. This paper proposes a new NER model called Att-CNN-BiGRU-CRF which consists of the following five layers. The prefix Att means the model is based on attention mechanism. A joint feature embedding layer combines the character embedding and word embedding based on BERT to obtain more semantic information. A convolutional attention layer combines the local attention mechanism and CNN to capture the relationship of local context. A BiGRU layer extracts higher-level features of power metering text. A global multi-head attention layer optimizes the processing of sentence level information. A CRF layer obtains the output tag sequences. This paper also constructs a corresponding power metering corpus data set with a new entity classification method. The novelties of our work are the five layer model structure and the attention mechanism. Experimental results show that the proposed model has high recall rate 88.16% and precision rate 89.33% which is better than the state-of-the-art models.
8953553,MirrorGAN: Learning Text-To-Image Generation by Redescription,"Generating an image from a given text description has two goals: visual realism and semantic consistency. Although significant progress has been made in generating high-quality and visually realistic images using generative adversarial networks, guaranteeing semantic consistency between the text description and visual content remains very challenging. In this paper, we address this problem by proposing a novel global-local attentive and semantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN exploits the idea of learning text-to-image generation by redescription and consists of three modules: a semantic text embedding module (STEM), a global-local collaborative attentive module for cascaded image generation (GLAM), and a semantic text regeneration and alignment module (STREAM). STEM generates word- and sentence-level embeddings. GLAM has a cascaded architecture for generating target images from coarse to fine scales, leveraging both local word attention and global sentence attention to progressively enhance the diversity and semantic consistency of the generated images. STREAM seeks to regenerate the text description from the generated image, which semantically aligns with the given text description. Thorough experiments on two public benchmark datasets demonstrate the superiority of MirrorGAN over other representative state-of-the-art methods."
8851786,A Hybrid Character Representation for Chinese Event Detection,"For the Chinese language, event triggers in a sentence may appear inside or across words after word segmentation. Thus recent works on Chinese event detection often formulate the task as a character-wise sequence labeling problem instead of a word-wise one. Due to a limited amount of corpus, however, it is more difficult in practice to train character-wise models to capture the inner structure of event triggers and the semantics of sentence-level context compared with word-wise ones. In this paper, we propose to improve character-wise models by incorporating word information and language model representation into Chinese character representation. More specifically, the former consists of the position of the character inside a word and the word's embedding, which can aid structural pattern learning; the latter is obtained by BERT, which contains long-distance semantic information. We construct a sequence tagging model equipped with the hybrid representation and evaluate our model on ACE 2005 Chinese corpus. Experiment results show that both word information and language model representation are effective enhancements, and our model gains an increase of 4.5 (6.5%) and 6.1 (9.4%) in F1-score in event trigger identification task and classification task respectively over the state-of-the-art method."
9284441,A Syntax-Augmented and Headline-Aware Neural Text Summarization Method,"With the advent of the information age, excessive information collection leads to information overload. Automatic text summarization technology has become an effective way to solve information overload. This paper proposes an automatic text summarization model, which extends traditional sequence-to-sequence (Seq2Seq) neural text summarization model by using a syntax-augmented encoder and a headline-aware decoder. The encoder encodes both syntactic structure and word information of a sentence in the sentence embedding. A hierarchical attention mechanism is proposed to pay attentions to syntactic units. The decoder is improved by a headline attention mechanism and a Dual-memory-cell LSTM network to achieve a higher quality of generated summaries. We designed experiments to compare the proposed method with baseline models on the CNN/DM datasets. The experiment results show that the proposed method is superior to abstractive baseline models in terms of the scores on ROUGE evaluation metrics, and achieve a summary generation performance comparable to the extractive baseline method. Though qualitative analysis, the summary quality of the propose method is more readable and less redundant, which agrees well with our intuition."
9413398,Multi-Speaker Emotional Speech Synthesis with Fine-Grained Prosody Modeling,"We present an end-to-end system for multi-speaker emotional speech synthesis. In particular, our system learns emotion classes from just two speakers then generalizes these classes to other speakers from whom no emotional data was seen. We address the problem by integrating disentangled, fine-grained prosody features with global, sentence-level emotion embedding. These fine-grained features learn to represent local prosodic variations disentangled from speaker, tone and global emotion label. Compared to systems that model emotions at sentence level only, our method achieves higher ratings in naturalness and expressiveness, while retaining comparable speaker similarity ratings."
9215347,An Effective Neural Machine Translation for English to Hindi Language,"Machine translation involves the conversion of text from one language to the other language. In the world of web, a huge number of resources are made available in English. Many of the people are not familiar with this global language. Manually transmuting them into native languages such as Hindi (Indian National language) is a tedious task. In such scenarios, automatic machine translation is an efficient approach. In our work, 8 advanced architectures have been experimented and contrasted their efficiencies. Six different Indian languages such as Hindi, Bengali, Gujarati, Malayalam, Tamil and Telugu is worked on. How BLEU varies with the usage of Word embedding technique have been clearly shown.. Encoder to decoder networks are found fine for short sentences. But if the length of the sentence exceeds 20, then attention architecture is suitable. The 4 Layer Bi-directional LSTM is a great choice in these networks to achieve higher BLEU is also observed. In our work, CFILT, UFAL, ILCC datasets have been considered and achieved a BLEU score of 21.97."
8787650,Multi-Intent Text Classification Using Dual Channel Convolutional Neural Network,"During the conversations among human and machine, how to comprehend user's intention according to the context is a crucial issue, which is very difficult. The commonly used Convolutional Neural Network (CNN) does not work well in this situation: due to the short length and the local features of dialogue sentences, CNN cannot well capture the global features and the semantic information. In this paper, we propose an intent classification dual-channel Convolutional Neural Networks (ICDC NN): we first extract semantic features by using Word2vec and Embedding layer to train the word vector; then, two different channels are used for convolution, one for character level word vector, the other for word-level word vector; thirdly, the character level word vectors (fine-grained) are combined with word-level word vectors to mine more in-depth semantic information of natural language question; finally, with convolution kernels of different sizes, more in-depth abstract features inside the sentences are learned. We then apply the ICDCNN to English text but with a different network structure. Experimental results show that the algorithm achieves high accuracy on both English and Chinese datasets, which shows a significant improvement compared to other methods."
9750843,Chinese text sentiment analysis based on transformer model,"As an important research area of natural language processing, text sentiment analysis has been widely used in opinion processing and product review analysis. Recurrent neural networks have the characteristics of temporal order, cannot directly extract the contextual semantic features of sentences, and cannot be computed in parallel; convolutional neural networks can be computed in parallel, but have the disadvantages of huge computation and long training time. To address the above problems, this paper proposes a Light-Transformer model based on word vector representation and positional embedding, and modifies the structure of the Transformer to keep only the Encoder module, and finally the extracted sentence features are input to two fully connected layers for classification. Experimental results on the NLPCC2014 Task2 dataset show that the method improves the classification accuracy by 0.3%-1.0% compared to traditional methods such as LSTM, CNN, etc., and the number of model parameters is greatly reduced compared to other Transformer-based models with close accuracy."
9170272,Residual Connected Enhanced Sequential Inference Model for Natural Language Inference,"Understanding the semantic and logical relationships between sentence pair is a difficult problem to be solved in natural language understanding tasks. Although the Enhanced Sequential Inference Model is simple in structure and performs well in SNLI, the limited capacity of the this model limits its further improvement of performance. Inspired by Res-Net, we propose the res-ESIM by introducing the residual connection into the ESIM model to expand the capacity of the ESIM while maintaining properties of simple structure and easy training. We explore the performance of res-ESIM with word embedding and the ability of using the contextual embedding to enhance its performance. In the experiments on SNLI, GloVe is used as word embedding for the convenience of comparing with published models. In the experiments on MultiNLI, the output of BERT-base based on different enhancement methods is used as contextual embedding. The experiment results on SNLI showed that our model achieves competitive performance in all models that haven't employed additional contextualized word representations and the experiment results on MultiNLI showed that res-ESIM can have more performance improvement than the original ESIM when the information of embedding is enhanced."
9364446,Interactive Clustering of Cooking Recipe Instructions: Towards the Automatic Detection of Events Involving Kitchen Devices,"Cooking recipes are a rich source of semantic information. They contain instructions for food preparation tasks, specifying the actions that should be carried out which typically involve various ingredients and kitchen devices. In an IoT scenario, instructions in cooking recipes can form the basis for automatically controlling kitchen devices without any programming. However, as these instructions are written in natural language, they first need to be transformed or parsed into machine-interpretable commands. As a step towards this, we investigate methods for identifying the various types of actions (events) that kitchen devices are involved in. We cast this problem as a clustering task, whereby recipe instructions involving a given device of interest, are automatically grouped according to the type of event described. Each sentence in every instruction is represented by its embedding vector which is computed using a BERT-based model, specifically one pre-trained using a Roberta architecture. We cluster these sentence embeddings using our newly proposed interactive machine learning (IML)-based framework underpinned by the HDBScan clustering technique. We demonstrate that our IML framework can detect events in sentences with satisfactory accuracy, reaching almost the same level as human performance."
9359728,"Automatic Exam Correction Framework (AECF) for the MCQs, Essays, and Equations Matching","Automatic grading requires the adaption of the latest technologies. It has become essential especially when most of the courses became online courses (MOOCs). The objectives of the current work are (1) Reviewing the literature on the text semantic similarity and automatic exam correction systems, (2) Proposing an automatic exam correction framework (HMB-AECF) for MCQs, essays, and equations that is abstracted into five layers, (3) Suggesting equations similarity checker algorithm named “HMB-MMS-EMA”, (4) Presenting an expression matching dataset named “HMB-EMD-v1”, (5) Comparing the different approaches to convert textual data into numerical data (Word2Vec, FastText, Glove, and Universal Sentence Encoder (USE)) using three well-known Python packages (Gensim, SpaCy, and NLTK), and (6) Comparing the proposed equations similarity checker algorithm (HMB-MMS-EMA) with a Python package (SymPy) on the proposed dataset (HMB-EMD-v1). Eight experiments were performed on the Quora Questions Pairs and the UNT Computer Science Short Answer datasets. The best-achieved highest accuracy in the first four experiments was 77.95% without fine-tuning the pre-trained models by the USE. The best-achieved lowest root mean square error (RMSE) in the second four experiments was 1.09 without fine-tuning the used pre-trained models by the USE. The proposed equations similarity checker algorithm (HMB-MMS-EMA) reported 100% accuracy over the SymPy Python package which reported 71.33% only on “HMB-EMD-v1”."
9339713,SVGAN: Semi-supervised Generative Adversarial Network for Image Captioning,"Image captioning is a task that enables computer to naturally describe the contents of an image like a human, moreover it involves two different major research fields of computer vision and natural language processing. In this paper, a new image captioning system is proposed, which can address the challenges of automatically describing images in the wild. Built on the state-of-the-art caption framework, we designed a deep visual detector to catch a broad range of visual concepts, a GAN(Generative Adversarial Network) with graph embedding is developed to generate accurate sentences for wild images."
9587971,Sentiment Analysis From Machine Learning to Deep Learning,"Sentiment analysis has been considered as a vital method to analyze a huge amount of documents in digital forms that are widespread and continuously increasing. In general, sentiment analysis plays an important role in social media analysis and business analysis. This paper illustrates sentiment analysis from machine learning methods (TF-IDF, Naïve Bayes, SVM) to deep learning methods (Embedding, LSTM). We explain how the technique developed from sentence level to document level and analyze their advantages and disadvantages."
8625512,Convolutional Neural Network Based Text Steganalysis,"The prevailing text steganalysis methods detect steganographic communication by extracting hand-crafted features and classifying them using SVM. However, these features are designed based on the statistical changes caused by steganography, thus they are difficult to adapt to different kinds of embedding algorithms and the detection performance is heavily dependent on the text size. In this letter, we propose a novel text steganalysis model based on convolutional neural network, which is able to capture complex dependencies and learn feature representations automatically from the texts. First, we use a word embedding layer to extract the semantic and syntax feature of words. Second, the rectangular convolution kernels with different sizes are used to learn the sentence features. To further improve the performance, we present a decision strategy for detecting the long texts. Experimental results show that the proposed method can effectively detect different kinds of text steganographic algorithms and achieve comparable or superior performance for a wide variety of text sizes compared with the previous methods."
9414880,Emotion Recognition by Fusing Time Synchronous and Time Asynchronous Representations,"In this paper, a novel two-branch neural network model structure is proposed for multimodal emotion recognition, which consists of a time synchronous branch (TSB) and a time asynchronous branch (TAB). To capture correlations between each word and its acoustic realisation, the TSB combines speech and text modalities at each input window frame and then uses pooling across time to form a single embedding vector. The TAB, by contrast, provides cross-utterance information by integrating sentence text embeddings from a number of context utterances into another embedding vector. The final emotion classification uses both the TSB and the TAB embeddings. Experimental results on the IEMOCAP dataset demonstrate that the two-branch structure achieves state-of-the-art results in 4-way classification with all common test setups. When using automatic speech recognition (ASR) output instead of manually transcribed reference text, it is shown that the cross-utterance information considerably improves robustness against ASR errors. Furthermore, by incorporating an extra class for all the other emotions, the final 5-way classification system with ASR hypotheses can be viewed as a prototype for more realistic emotion recognition systems."
8974765,Residual Self-Attention for Visual Question Answering,"Over these years, many attention mechanism-based neural network models have been put forward in the visual question answering (VQA) by some researchers. However, the semantic gap between the multimodality cannot be bridged simply, and the combination of the questions and the images may be arbitrary, which both hinder the jointly representing learning and make the VQA model easy for overfitting. Therefore, in this paper, a multi-stage attention model has been put forward, that is, for the image, the bottom-up attention and residual self-attention for the image itself and the question-guided double-headed soft (top-down) attention method were used to extract the image features; for the question, the pre-trained GloVe word embeddings were used in this paper to represent the semantics and the GRU was used to encode the questions into fixed-length sentence embedding; finally, the image features were fused with the question embedding through the Hadamard product and then input into the sigmoid multi-classifier. The experiment showed that with the model the overall accuracy of 67.26% in the VQA 2.0 dataset was finally achieved, higher than that of other advanced models."
9073755,Content Linking Via Information Compression and Similarity,"Content Linking(CL) is a new task in natural language processing(NLP) recently. In this paper, CL is regarded as a binary classification task using neural networks. The input is the feature of the sentence pair, while the output is their association. Word embedding is the most popular text feature representations. However, its dimensionality would be large, and simply stitching the word embedding features of two words couldn't contain the relationship between them. We propose a new text feature named Word2vec_V_I by Principal Component Analysis (PCA) and similarity calculation. Furthermore, a content linking method based on CNN and Word2vec_V_I is designed and implemented. Experiments on CL-SciSumm 2018 dataset verify the effectiveness of this method."
9660575,DSAMT: Dual-Source Aligned Multimodal Transformers for TextCaps,"When generating captions for images, previous caption methods tend to consider the visual features of the image but ignore the Optical Character Recognition (OCR) in it, which makes the generated caption lack text information in the image. By integrating OCR modal as well as visual modal into caption prediction, TextCaps task is aimed at producing concise sentences recapitulating the image and the text information. We propose Dual-Source Aligned Multimodal Transformers (DSAMT), which utilize words from two sources (object tags and OCR tokens) as the supplement to vocabulary. These extra words are applied to align caption embedding and visual embedding through randomly masking some tokens in caption and calculating the masked token loss. A new object detection module is used in DSAMT to extract image visual features and object tags on TextCaps. We additionally use BERTSCORE to evaluate our predictions. We demonstrate our approach achieves superior results compared to state-of-the-art models on TextCaps dataset."
9557393,Multidocument Summarization using GloVe Word Embedding and Agglomerative Cluster Methods,"This paper explores the method of extracting multi-document summary terminology that enhances single document summary methods using the information related to the document and perhaps even the relation between the documents. In the problems of fragmentation, density, duplication and selection of passages, the summarization of several documents ranges from the summarization of single documents to the creation of efficient summaries. Our approach addresses these issues by using Agglomerative cluster sentence, GloVe, TextRank, Cosine Similarity. In addition, this research also use NLTK as library filter word such as stopwords, numeric, punctuation, multiple_whitespaces, short_words in order Vectorizing the sentence when using GloVe. The result of this paper was evaluated using ROUGE; 41% for unigram, 17% for bigram, 57% for trigram."
9406329,Incremental Text-to-Speech Synthesis Using Pseudo Lookahead With Large Pretrained Language Model,"This letter presents an incremental text-to-speech (TTS) method that performs synthesis in small linguistic units while maintaining the naturalness of output speech. Incremental TTS is generally subject to a trade-off between latency and synthetic speech quality. It is challenging to produce high-quality speech with a low-latency setup that does not make much use of an unobserved future sentence (hereafter, “lookahead”). To resolve this issue, we propose an incremental TTS method that uses a pseudo lookahead generated with a language model to take the future contextual information into account without increasing latency. Our method can be regarded as imitating a human's incremental reading and uses pretrained GPT2, which accounts for the large-scale linguistic knowledge, for the lookahead generation. Evaluation results show that our method 1) achieves higher speech quality than the method taking only observed information into account and 2) achieves a speech quality equivalent to waiting for the future context observation."
8966173,Video Captioning Based on Joint Image–Audio Deep Learning Techniques,"With the advancement in technology, deep learning has been widely used for various multimedia applications. Herein, we utilized this technology to video captioning. The proposed system uses different neural networks to extract features from image, audio, and semantic signals. Image and audio features are concatenated before being fed into a long short-term memory (LSTM) for initialization. The joint audio-image features help the entire semantics to form a network with better performance. A bilingual evaluation understudy algorithm (BLEU) - an automatic speech scoring mechanism - was used to score sentences. We considered the length of the word group (one word to four words); with the increase of all BLEU scores by more than 1%, the CIDEr-D score increased by 2.27%, and the METEOR and ROUGE-L scores increased by 0.2% and 0.7%, respectively. The improvement is highly significant."
8995341,Multi-classfication Sentiment Analysis Based on the Fused Model,"The traditional methods of sentiment classification usually see the data has positive and negative two kinds of attitudes only. But actually, the real data has multi-category sentiment, is positive, negative, neutral and not mentioned four classes. Therefore, when using common classifying methods to analyzing the data sentiment, if the number of few class data is too scarce, it is difficult to learn useful information from them and the final classifying result will tend to most classes. In order to obtain accurate classification results, this paper proposes a multi-classification method based on the combination of Bert (Bidirectional Encoder Representation from Transformers) model and Liblinear (A Library for Large Linear Classification) model (It is abbreviated as B-Liblinear). Due to the Bert model's breakthrough in data preprocessing, this paper prepressed training data set, and obtained the word vector and sentence vectors from data. Next, combined with attribute label and sentiment tendency data, the unstructured data was converted into a structured training data set. It was as the standard input data of Liblinear model to construct a classification model. This model's classification mechanism is ""one vs. rest"", it can effectively solve the heavy class imbalance problem of massive data in multiple classification tasks. In this paper, the classification result of B-Liblinear model was compared with several classical multi-classification methods. And the experimental results show that the combination of Bert model and Liblinear of dealing with the text multi-classification problem is more accurate."
9287397,A Targeted Topic Model based Multi-Label Deep Learning Classification Framework for Aspect-based Opinion Mining,"Recently, deep Convolutional Neural Network (CNN) model has achieved remarkable results in Natural Language Processing (NLP) tasks, such as information retrieval, relation classification, semantic parsing, sentence modeling and other traditional NLP tasks, etc. On the other hand, topic modeling method has been proved to be effective by exploiting hidden knowledge in a corpus of documents. Motivated from these successes, we propose a framework that takes the advantages of closure domain measure to get enriched knowledge from close domains to the training dataset to improve the CNN model, and apply a Targeted Topic Model to take more detailed exploration on each labeled aspect of an opinion. Experimental results on different scenarios show the effectiveness of the proposed framework for multi-label classification task in comparison to other related models on the same Hotel review dataset."
9475960,Deep Learning Approach for Negation Handling in Sentiment Analysis,"Negation handling is an important sub-task in Sentiment Analysis. Negation plays a significant role in written text. Negation terms in sentence often changes the polarity of entire sentence from positive to negative or vice versa, resulting in the opposite meaning of the sentence than what is observed by the machine learning based linguistic model. As automatic opinion mining has become very important in this digital era, proper handling of negation term is the need of the hour. In any natural language negations can be formulated both explicitly or implicitly while their use is very much domain-specific. Existing negation handling techniques follow rule-based approach and mainly used in medical domain. Due to the complex syntactic structure of negation, it is hard to build general purpose machine learning based negation handling model on user review or conversational text data. In this paper, we investigate negation components i.e., cue and scope in a sentence which determine the polarity shift in sentence. We propose LSTM based deep neural network model for negation handling task where the model automatically learns the negation features from labeled input training dataset. We used ConanDoyle story corpus for model training and testing, which is pre-annotated with negation information. The proposed model first identify negation cues in each sentence and then using bidirectional LSTM extracts the relationship between cue and other words to identify scope of the cue in sentences. We derived word level features for model training to determine correct polarity of the sentence. Result shows that the LSTM based nonlinear language models perform comparatively better than the traditional state of the art SVM, HMM or CRF based models. BiLSTM achieved best result, F1 measures 93.34%, outperform traditional rule based model in negation handling task."
9200698,PFAN++: Bi-Directional Image-Text Retrieval With Position Focused Attention Network,"Bi-directional image-text retrieval and matching attract much attention recently. This cross-domain task demands a fine understanding of both modalities for learning a measure of different modality data. In this paper, we propose a novel position focused attention network to investigate the relation between the visual and the textual views. This work integrates the prior object position to enhance the visual-text joint-embedding learning. The image is first split into blocks, which are treated as the basic position cells, and the position of an image region is inferred. Then, we propose a position attention to model the relations between the image region and position cells. Finally, we generate a valuable position feature to further enhance the region expression and model a more reliable relationship between the visual image and the textual sentence. Experiments on the popular datasets Flickr30K and MS-COCO show the effectiveness of the proposed method. Besides the public datasets, we also conduct experiments on our collected practical large-scale news dataset (Tencent-News) to validate the practical application value of the proposed method. As far as we know, this is the first attempt to test the performance on the practical application. Our method achieves the competitive performance on all of these three datasets."
9679139,Jointly Multi-Similarity Loss for Deep Metric Learning,"Deep metric learning has been widely adopted to construct good representations for images and sentences with pair-based loss functions such as contrastive loss and triplet loss. However, these loss functions restrict the effectiveness of learned embedding and face two critical challenges: 1) high bias on account of a large set of uninformative and redundant pairs; 2) low information exploration due to the lack of processing the relation information among pairs. In this paper, we propose the jointly multi-similarity (JMS) loss to address the above challenges with a novel pair weighting strategy that assigns higher weights to the more informative pairs and discards the less informative ones. Specifically, we integrate the relationship information among pairs into a single framework and optimize the JMS loss by considering various information jointly. Furthermore, we extensively compare the JMS loss with other start-of-the-art approaches by conducting multiple experiments on image retrieval and semantic text similarity tasks. The experimental results show that the JMS loss consistently outperforms competitors."
8891702,Combining Part-of-Speech Tags and Self-Attention Mechanism for Simile Recognition,"Simile recognition is to find simile sentences and extract the tenor and vehicle from these sentences. Previous works illustrate that tenors and vehicles are typically noun phrases. A word may have different part-of-speech (POS) labels (e.g., adjectives, adverbs, nouns, and verbs) in different sentences. It is important for the simile recognition task to identify a certain POS information for each word in a sentence. However, existing models use the same word embedding to represent a word, which cannot accurately represent the POS information of this word in different sentences. In this paper, we propose a neural network framework explicitly integrating the POS information into simile recognition task, with additional self-attention mechanism to better capture long term dependencies between any two tokens in sentences. The experimental results show that our proposed models significantly outperform previous state-of-the-art methods in the simile recognition task. We also present an analysis showing that the POS information and self-attention mechanism are effective for the simile recognition task."
9534093,Automatic Topic Labeling model with Paired-Attention based on Pre-trained Deep Neural Network,"The automatic topic labeling model aims at generating a sound, interpretable, and meaningful topic label that is used to interpret an LDA-style discovered topic, intending to reduce the cognitive load of end-users while browsing or investigating the topics. In this study, we first introduced the pre-trained language model BERT to topic labeling tasks. It exploits the contextual embedding of the pre-trained language model to improve the quality of encoding sentences. To generate a topic label with higher Relevance, Coverage, and Discrimination, we propose a novel summarization neural framework. Specifically, it exploits the paired-attention to model the relationship between the candidate sentences first and then decides which sentences should be included in the final summarization topic label. Moreover, we expected that high-quality sentence encoding representation could improve our model's performance. So, for each discovered topic, we trained a specific layer to extract the important topic-related features from the sentence embeddings as well as filter the noise information. The experimental results showed that our model significantly outperforms the state-of-the-art and classic topic labeling models."
9499922,Chatbot : A Question Answering System for Student,"In the last few years there has been a fast growing up of the use of Chatbots in various fields, such as Customer Service, Marketing, Educational, Health Care and many others. This paper develops a Question Answering System in educational domain. Using Word Embedding, Latent Dirichlet Allocation and Text summarization method build 3 knowledge bases. According question sentence analysis then LineBot retrieve data from these knowledge bases and provides proper answers to the student. Comparison experimental group-used LineBot hits and reading time in e-learning system, are more than control group."
9439908,Chinese Lexical Simplification,"Lexical simplification has attracted much attention in many languages, which is the process of replacing complex words in a given sentence with simpler alternatives of equivalent meaning. Although the richness of vocabulary in Chinese makes the text very difficult to read for children and non-native speakers, there is no research work for the Chinese lexical simplification (CLS) task. To circumvent difficulties in acquiring annotations, we manually create the first benchmark dataset for CLS, which can be used for evaluating the lexical simplification systems automatically. To acquire a more thorough comparison, we present five different types of methods as baselines to generate substitute candidates for the complex word that includes synonym-based approach, word embedding-based approach, BERT-based approach, sememe-based approach, and a hybrid approach. Finally, we design the experimental evaluation of these baselines and discuss their advantages and disadvantages. To our best knowledge, this is the first study for CLS task."
9635100,BERT-based Aggregative Group Representation for Group Recommendation,"Group recommendation, as a type of recommender systems, mainly recommends items to a group of users. In this paper, inspired by Bidirectional Encoder Representation from Transformers (BERT), our goal is to explore a novel BERT-based group modeling. Therefore, we devise a novel BERT-based Group Recommendation approach, namely BGR. Specifically, BERT is adopted to capture group preferences by using sentence-level embedding from a global perspective. Based on this, we then integrate aggregation representation of group users to obtain final group representation. Finally, group recommendation is achieved in the framework of neural collaborative filtering. Experiments are conducted on two real-world datasets demonstrate that our BGR outperforms the baselines."
9283183,Quantifying the Impact of Complementary Visual and Textual Cues Under Image Captioning,"Describing an image with natural sentence without human involvement requires knowledge of both image processing and Natural Language Processing (NLP). Most of the existing works are based on unimodal representations of the visual and textual contents using an Encoder-Decoder (EnDec) Deep Neural Network (DNN), where the input images are encoded using Convolutional Neural Network (CNN) and the caption is generated by a Recurrent Neural Network (RNN). This paper dives into a basic image captioning model to quantify the impact of multimodal representation of the visual and textual cues. The multimodal representation is carried out via an early fusion of encoded visual cues from different CNNs, along with combined textual features from different word embedding techniques. The resultant of the multimodal representation of the visual and textual cues are employed to train a Long Short-Term Memory (LSTM)-based baseline caption generator to quantify the impact of various levels of complementary feature mutations. The ablation study involves two different CNN feature extractors and two types of textual feature extractors, shows that exploitation of the complementary information outperforms the unimodal representations significantly with endurable timing overhead."
8983207,Data Reconstruction Based on Temporal Expressions in Clinical Notes,"Learning representations of clinical notes poses challenges in handling complex content that necessitates preprocessing steps to make the data more suitable for data mining. An important issue, addressed here, is that of temporal expressions, where cues indicate the time when clinical events occur. We present a three-step data reconstruction algorithm for transforming similar clinical entities (e.g., symptoms, complications) into sequential data through unsupervised annotation of temporal expressions. First, the data reconstruction algorithm detects if an expression has temporal intent. Second, it decomposes and rewrites the expression into non-temporal sub-expression and temporal constraints. Finally, it clusters similar non-temporal sub-expressions by using unsupervised sentence embedding under the modified K-medoids paradigm. We experimented with our proposed algorithm on clinical notes associated with chronic obstructive pulmonary disease (COPD). Visualizing reconstruction results of cardiology reports for a longitudinal cohort of patients with COPD demonstrated that this algorithm is feasible."
9054678,Keyword Search for Sign Language,"Keyword search is the search for a written query in an archive, which is often assumed to be a collection from a spoken language. Yet, the main languages of the Deaf, i.e. sign languages, are mostly neglected in this definition due to being visual languages. In this paper, we propose a keyword search (KWS) system for a sign language. In this technique, we first extract body and hand joints from the frames of a sign language sentence and represent it with a unified spatio-temporal graph of skeleton joints. We then train our graph-convolutional-network encoder, query embedding, and selection mechanism together in a weakly supervised, end-to-end fashion. Experimental results are reported on RWTH-PHOENIX-Weather 2014T dataset."
9609109,Improving Traceability Link Recovery Using Fine-grained Requirements-to-Code Relations,"Traceability information is a fundamental prerequisite for many essential software maintenance and evolution tasks, such as change impact and software reusability analyses. However, manually generating traceability information is costly and error-prone. Therefore, researchers have developed automated approaches that utilize textual similarities between artifacts to establish trace links. These approaches tend to achieve low precision at reasonable recall levels, as they are not able to bridge the semantic gap between high-level natural language requirements and code. We propose to overcome this limitation by leveraging fine-grained, method and sentence level, similarities between the artifacts for traceability link recovery. Our approach uses word embeddings and a Word Mover's Distance-based similarity to bridge the semantic gap. The fine-grained similarities are aggregated according to the artifacts structure and participate in a majority vote to retrieve coarse-grained, requirement-to-class, trace links. In a comprehensive empirical evaluation, we show that our approach is able to outperform state-of-the-art unsupervised traceability link recovery approaches. Additionally, we illustrate the benefits of fine-grained structural analyses to word embedding-based trace link generation."
9776064,CSMRS: An Efficient and Effective Semantic-aware Ranked Search Scheme over Encrypted Cloud Data,"The document vectors constructed by the traditional searchable encryption scheme based on the term frequency-inverse document frequency model not only have high dimensionality and sparsity, but also ignore the semantic information of documents and keywords. In this paper, we introduce the sentence bidirectional encoder representations from transformers model (SBERT) to obtain semantic information-embedded vectors for documents and keywords. By adopting the SBERT model, we pro-pose a CBG-index based semantic-aware multi-keyword ranked search scheme (CSMRS). In the scheme, a topic-term frequency-inverse topic frequency (TTF-ITF) model and a clustering-based group index (CBG-index) are proposed. The TTF-ITF model is used to generate semantic vectors for keywords, and the CBG-index is used to improve the search efficiency. The experimental results demonstrate the better performance than the existing works in terms of search efficiency and search result semantic precision."
9230679,A Neural Network Model to Identify the Crisis-related Actionable Informative Tweets for Disaster Management,"During emergencies and disaster situations, we can consider twitter as a source of situation related information. Identifying informative tweets from twitter streams provide enormous opportunities for public safety personnel in coordinating aid operations or post-incident assessment. However, the brevity of tweets and its' noisy contents make it challenging to extract the necessary information effectively and identify the tweets based on different information types. In this paper, we present a unified neural architecture for actionable informative tweet classification. In our proposed model, we exploit the transfer learning features from a pre-trained sentence embedding model along with a rich set of hand-crafted features to train a multilayer perceptron (MLP) network. Besides, we employ the state-of-the-art LSTM variants, nested LSTMs (NLSTMs) to capture the long-term dependency effectively. On top of nested LSTMs, we perform the convolution using multiple kernels (CMK) to obtain the higher-level representation of tweets. Experiments on 2018 TREC incident streams (TREC-IS) dataset show that our proposed neural model gains a significant improvement against the current state-of-the-art."
8682727,A Neural Network Based Ranking Framework to Improve ASR with NLU Related Knowledge Deployed,"This work proposes a new neural network framework to simultaneously rank multiple hypotheses generated by one or more automatic speech recognition (ASR) engines for a speech utterance. Features fed in the framework not only include those calculated from the ASR information, but also involve natural language understanding (NLU) related features, such as trigger features capturing long-distance constraints between word/slot pairs and BLSTM features representing intent-sensitive sentence embedding. The framework predicts the ranking result of the input hypotheses, outputting the top-ranked hypothesis as the new ASR result together with its slot filling and intention detection results. We conduct the experiments on an in-car infotainment corpus and the ATIS (Airline Travel Information Systems) corpus, for which hypotheses are generated by different types of engines and a single engine, respectively. The experimental results achieved are encouraging on both data corpora (e.g., 21.9% relative reduction in word error rate over state-of-the-art Google cloud ASR performance on the ATIS testing data), proving the effectiveness of the proposed ranking framework."
9679443,PMatch: Semantic-based Patch Detection for Binary Programs,"Binary function matching has been proposed to detect the known vulnerabilities. However, the high similarity between the vulnerable and patched versions leads to a large of false positives. Patch detection is proposed to improve the accuracy of function matching by identifying the patched functions from matching results. However, the accuracy of existing methods decreases significantly due to the function changes introduced by high compiler optimization levels.In this paper, we propose PMatch, a method based on code semantic similarity to detect the patched binary functions. Firstly, PMatch extracts patch-affected code snippets from the patched binary function. Secondly, PMatch leverages a novel unsupervised sentence embedding technique in Natural Language Processing (NLP) to generate the semantic representations of binary code. Finally, PMatch matches the patch-affected code snippets with target blocks obtained by function diffing. To evaluate PMatch, we collect 101 CVEs and compile 304 binary programs with 4 different optimization levels. PMatch achieves an 86.43% average accuracy in detecting the patched functions, which outperforms the state-of-the-art work, and costs only 65.14ms per function. Besides, at the O3 high optimization level, PMatch achieves an accuracy improvement of over 20%."
9527648,Evolutionary Relationship Extraction of Emergencies Based on Two-way GRU and Multi-channel Self-attention Mechanism,"After emergencies occur, evolutionary phenomena often occur. What brings great challenges to the emergency decision-making process is the complexity and uncertainty of the evolution process. In order to improve the completeness of information extraction in emergency decision-making, the ""BiGRU-MultiAtt"" model, which integrates two-way GRU and multi-channel self-attention mechanism, is used to extract the evolutionary relationship between emergencies. Compared with the mainstream ""LSTM-ATT"", this model will capture sentence-level semantic feature information more comprehensively, enhance the ability to learn urgent texts, and significantly reduce the overall complexity of the model. In addition, under the network model, a weight-based position embedding method is proposed to assist training, which further enhances the importance of text vocabulary position information and achieves the effect of enhancing training. In the evolutionary relationship extraction experiment, the accuracy and recall rate of the introduction of the multi-channel self-attention model method have been improved, and the F1 value has reached 90.41%."
9201846,Automated Summarization of Bug Reports to speed-up software development/maintenance process by using Natural Language Processing (NLP),"Bug reports can provide a great deal of assistance for developers during the process of development. But due to the large size of bug repositories, it is sometimes difficult to take advantage of these artifacts in the available time. One way of helping developers to provide summaries of these reports and provide relevant details only. Once it's decided that this is the required report then one can study the details. As text mining technology advances, many substantial approaches have been proposed to generate optimized summaries for bug reports. In this paper, we have proposed an extractive based methodology for the generation of summaries of bug reports by using the sentence embedding. We achieved improved rouge-1 and rouge- 2 results than the previous state of the art systems for the bug report summary generation."
9325781,A Hierarchical Bidirectional LSTM Sequence Model for Extractive Text Summarization in Electric Power Systems,"With the increasing volume of documents in electric power systems, it is urgent and necessary for electric power systems managers to efficiently analyze the massive documents and make reasonable decisions by capturing the main points of the document as quickly as possible. The text summarization technique provides a feasible way to efficiently analyze and obtain the main contents residing in the document. In this paper, we present a Hierarchical Bidirectional Long Term Short Memory Sequence model for extractive text summarization in electric power systems in order to efficiently and accurately summarize electric power documents and obtain a summary of the document. Our model is divided into four layers including the embedding layer, the word layer, the sentence layer, and the classification layer in a hierarchical manner. The related experiments were made based on the electric power data set that contains more than 2000 electrical papers, in comparison with the existing approaches based on the CRF, CNN, and RNN models. The experimental results show that the performance based on our approach is superior to the three approaches against the three performance indexes ROUGE-1, ROUGE-2, and ROUGE-L."
9780135,SimCSE for Encrypted Traffic Detection and Zero-Day Attack Detection,"Traffic detection has attracted much attention in recent years, playing an essential role in intrusion detection systems (IDS). This paper proposes a new approach for traffic detection at the packet level, inspired by natural language processing (NLP), using simple contrastive learning of sentence embeddings (SimCSE) as an embedding model. The new approach can learn the features of traffic from raw packet data. Experiments were conducted on two well-known datasets to evaluate our approach. For detecting malicious activity, our model achieved an accuracy of 99.99% on the USTC-TFC2016 dataset, whereas for detecting virtual private network (VPN) activity, our model achieved an accuracy of 99.98% on the ISCXVPN2016 dataset. Furthermore, the resulting model was found to be robust based on zero-day attack detection, which shows the model’s ability to detect attacks that have not been seen before. Experiments show that our approach can effectively detect network traffic and outperforms many other state-of-the-art methods."
9313128,OntoSem: an Ontology Semantic Representation Methodology for Biomedical Domain,"Ontologies are essential description tools for biomedical concepts and entities, supporting biomedical fundamental research such as semantic similarity analysis, protein-protein interaction prediction and so on. An increasing amount of ontology-like domain knowledge is published in scientific publications, meanwhile, advanced natural language processing (NLP) techniques have been widespread to extract information from text resources automatically, both of which facilitate the exploration of the semantic representation of biomedical ontologies. We propose a novel distributional semantic representation methodology based on the combination of two pre-trained and domain-specific word embedding tools, the non-contextualized Word2Vec and the context-dependent NCBI-blueBERT, to enhance the encoding ability for biomedical ontologies. Furthermore, we utilize a randomly initialized bidirectional LSTM to project the obtained word vector sequence to a fixed-length sentence vector, facilitating a flexible and uniform way for the computation of downstream tasks. We evaluate our method in two categories of tasks: the similarity access of ontology terms, and the ontology annotation-based protein-protein interaction classification. Experimental results demonstrate that our method provides encouraging results compared to the baselines in all tests. Our approach offers promising opportunities for representing ontologies semantics and in turn characterizing entities including proteins in biomedical research."
9206732,Dependency Based Bilingual word Embeddings without word alignment,"In this work, we trained different bilingual word embeddings models without word alignments (BilBOWA) using linear Bag-of-words contexts and dependency-based contexts. BilBOWA embedding models learn distributed representations of words by jointly optimizing a monolingual and a bilingual objective. Including dependency features in the monolingual objective, improves the accuracy of learning bilingual word embeddings up to 6% points in English-Spanish (En-Es) and up to 2.5% points in English-German (En-De) language pairs in word translation task compared to the baseline model. However, using these dependency features in both monolingual and bilingual objectives does not lead to any improvement in the En-Es language pair and only shows minor improvement for En-De. Moreover, our results provide evidence that using dependency features in bilingual word embeddings has a different effect based on syntactic and sentence structure similarity of the language pair."
9041749,Iterative Document Retrieval via Deep Learning Approaches for Biomedical Question Answering,"The ever expanding nature of the scientific literature makes finding answers in them increasingly more challenging for researchers. With the goal to ease this challenge, we have developed a biomedical question answering system called Bio-AnswerFinder which uses a greedy iterative document retrieval approach to find candidate documents in which the answer supporting sentences are searched. To improve the performance of the baseline retrieval approach, neural network based keyword selection and importance ranking approaches are introduced. Together with two ensemble approaches and a non-iterative word embedding based nearest neighbor approach, seven retrieval approaches are evaluated using Bio-AnswerFinder on hundred test questions with manual inspection. The test results revealed that the iterative keyword ranking approach more than doubled MRR@10 score over the baseline having the best Precision@1 score and a close second MRR@10 score to the ensemble of keyword selection and ranking iterative retrieval approaches."
9009843,See-Through-Text Grouping for Referring Image Segmentation,"Motivated by the conventional grouping techniques to image segmentation, we develop their DNN counterpart to tackle the referring variant. The proposed method is driven by a convolutional-recurrent neural network (ConvRNN) that iteratively carries out top-down processing of bottom-up segmentation cues. Given a natural language referring expression, our method learns to predict its relevance to each pixel and derives a See-through-Text Embedding Pixelwise (STEP) heatmap, which reveals segmentation cues of pixel level via the learned visual-textual co-embedding. The ConvRNN performs a top-down approximation by converting the STEP heatmap into a refined one, whereas the improvement is expected from training the network with a classification loss from the ground truth. With the refined heatmap, we update the textual representation of the referring expression by re-evaluating its attention distribution and then compute a new STEP heatmap as the next input to the ConvRNN. Boosting by such collaborative learning, the framework can progressively and simultaneously yield the desired referring segmentation and reasonable attention distribution over the referring sentence. Our method is general and does not rely on, say, the outcomes of object detection from other DNN models, while achieving state-of-the-art performance in all of the four datasets in the experiments."
9378388,Large Scale Financial Filing Analysis on HPCC Systems,"Insights from public companies' financial filings are necessary for securities analysts and investors to make the right investment decisions. Synthesizing salient facts from such filings is a complex language task, especially now as the data volume is growing at an overwhelming pace. To ease human labor in this process, our work proposed a financial filing analysis pipeline which automatically scrapes financial filings, generates the embeddings of the contextual data and performs sentiment analysis in order to predict future performance of the underlying companies. The pipeline is built on top of Big Data processing platform HPCC Systems to enable the capability of processing large amounts of financial filings in a scalable and timely manner. By applying word embedding and machine learning models to a large amount of SEC financial filings, our pipeline is able to process 20 GB of XBRL files -- 5,000 filing documents for more than 3,500 companies -- into 50,000 sentence embeddings within 5 minutes and transform the same data to TF-IDF embedding in about 8 minutes. To test sentiment analysis, we randomly sampled and manually labeled 5,000 SEC filings. As a result, the sentiment analysis suggested that the usefulness of stock price as a metric is specific to each industry and overall market, but is usable as long as the scope of inquiry is sufficiently narrow. Additionally, while our model is trained only on 5,000 manually labeled filings with unigrams and a final loss of 0.09, the results of the sentiment analysis exhibited discriminatory power exceeding naïve label selection through random or biased choice, suggesting that there is efficacy in using Natural Language Processing to analyze SEC filings."
9358583,Comparison of Deep Learning Approaches for Sentiment Classification,"Word embeddings are used to convert the unstructured text to numerical values for further analysis. Nowadays, prediction based embedding models like Continuous Bag Of Words (CBOW) and Skip grams are used in comparison to frequency based embeddings. Unlike frequency based embeddings, prediction based embeddings are able to model the semantics of the terms present in a sentence. Sentiment Analysis (SA) is a field of study that aims to automatically extract opinions from the data and to further classify them as positive and negative. The application of sentiment analysis in almost all the domains stands as a motivating factor for this work. It suffers from the problem of non-availability of sufficient labeled data to train the model. Due to the scalability and ability of deep learning models to perform automatic feature extraction from the data, they can be introduced to address this problem. They are also used for various applications due to its capability to extract hierarchical structures from complex data. Keras is a Deep Learning (DL) framework that provides an embedding layer to produce the vector representation of words present in the document. The objective of this work is to analyze the performance of three deep learning models namely Convolutional Neural Network (CNN), Simple Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM) for classifying the book reviews. From the experiments conducted, it is found that LS TM model performs better than CNN and simple RNN for sentiment classification."
8811936,Analysis and Detection of Information Types of Open Source Software Issue Discussions,"Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders."
9162270,DACNN: Dynamic Weighted Attention with Multi-channel Convolutional Neural Network for Emotion Recognition,"In recent years, due to the vigorous development of social network media, a large amount of social data can be obtained through social media. This trend has made it easier for researchers in natural language processing to obtain textual research materials, and has further stimulated the development of natural language processing. Emotion recognition is an important task in the field of natural language processing, and emotion recognition helps improve interaction in social. In this study, the emotion recognition of the content of twitter users' posts and comments is based on the sentence level. We use large datasets to collect users' posts and comments on Twitter for comparative analysis, and we attempt to understand the emotions expressed by users. Most of the recent studies focus on the analysis of vocabulary and syntactic features in sentences to learn the emotion of the sentence. However, the property of multi-feature may be conducive for promoting the performance of the model and may also be a disturbance of the training of the model. Our study is based on the latest of attention mechanism and XLNet to improve the recognition of emotions. We propose a novel emotion recognition model called Dynamic Weighted Attention with Multichannel Convolutional Neural Network (DACNN), which combines the multi-channel convolutional neural network and the attention mechanism of automatic weight adjustment to effectively improve the outcome of emotion recognition. In addition, we use the latest word embedding technology XLNet to obtain high-quality feature vectors of sentences. In the experiments, we compare the DACNN with the baselines on various datasets, and the results show that DACNN is better than the state-of-the-art methods in accuracy, precision, recall, and fl-score. The experimental results also prove that our attention mechanism and dynamic weighting can well promote the accuracy of emotion recognition."
9667690,Global Semantics with Boundary Constraint Knowledge Graph for Chinese Financial Event Detection,"Chinese financial event detection has a great significance in the application of financial risk analysis, en-terprise management and decision-making. The existing tasks of Chinese event detection are mainly regarded as character-based or word-based classification, which suffers from the ambiguity of trigger words. These tasks only concentrate on local information (e.g character and word), which loses sight of global information like sentence semantics. Furthermore, in the finance field, there exists the problem of fuzzy boundary between different event types. In this paper, we propose a global semantics with boundary constraint knowledge graph (BCKG) for Chinese financial event detection, which considers both sentence semantics and boundary knowledge. At first, Chinese financial dataset (CFD) is constructed by considering the complexity in financial area. And then, the sentence seman-tics embedding is obtained by pre-training BERT fine-tuning mechanism to address the problem of ambiguity of trigger words, which considers both syntactic information and context sentence semantics comprehensively. Finally, we construct the BCKG for financial event, which can add additional prior knowledge to solve fuzzy boundary problem. The proposed method for event detection achieves outstanding performance on standard ACE 2005 Chinese dataset and constructed CFD. The experimental results demonstrate the effectiveness of the proposed method."
9197932,H-LSTM Framework for Temporal Information Retrieval in Code-Mixed Social Media Text,"The contents of social media consists of mixed script information and from these type of sources one can extract temporal information. The temporal contents are difficult to be processed by any machine. The challenge here is to retrieve temporal domain information used by the users for expressing their interests on any issue. The work outlines the comparative study of different approaches of temporal data retrieval in transliterated domain. A novel handcrafted rule based technique is applied which accepts input in mixed script format and on the basis of illustrated rules, the system is suppose to extract temporal expressions from the sentence. The evaluation measures use rule-based approach validations along with the evaluations based on statistical measures. To further validate the results a voting technique is applied that selects the most valid and suitable temporal tag for that word. The experimental results suggests that the voting method performs applied here gives the best result in context to accurate temporal tagging."
9808008,Document Classification And Automatic Grading,"Computing similarity between documents plays a vital role in grade assessment. Methods like word-by-word comparison are available to compute similarity between documents. Automatic assessments have to deal with lots of unstructured data which may be the same in meaning but different in sentences. A new methodology has to be adapted to compute similarity between unstructured documents. This paper provides a faster and efficient way to make embeddings for documents and computes the similarity for grade calculation. The most widely used similarity measure cosine similarity is used for computing the similarity score. To make embeddings for documents, this paper uses two algorithms Word2Vec and Doc2Vec. An aggregate result of these two methods is used for calculating grade scores."
9087114,Computational Intelligence for Temporal Expression Retrieval in Code-Mixed Text,"Text on social media contains code-mixed contents which can be used to extract temporal information. These temporal information is hard to be retrieved in transliterated domain. The extraction of temporal expression used by the people to express their opinions on web isa challenging task in code mixed environment.The work presented in this paper tries to identify temporal information in code mixed social media text in transliterated domain. Thehand crafted rulesfor identifying temporal content in text is proposed, that takes codemixed text as input and on the basis of rules proposed, the system produces a tagged temporal output by identifying the temporal expression in the sentence.The hypothesis is evaluated on the basis of experiments undertaken in terms of the hand-crafted rule approach and to the statistical approaches of Conditional Random Field and Support Vector Machines. On the obtained results from mentioned approaches, a voting technique is applied which selects the best temporal tag for a word based on majority. This voting tag is also useful when the applied approaches give dissimilar tag for a word. The voting approach helps in considering the best temporal tag. The used votingtechniqueprovides the best results for tagging as compared to other different approaches used in the experiment with high accuracy."
9382457,Analysis of Polysemy using Variance Values for Word Embeddings by BERT,"Word embeddings output by BERT depends on the context of the sentence in which the word appears. Therefore, we collect examples of word w and calculate their variance values from word w's embeddings obtained by BERT, and it is considered that the variance values correspond to the polysemy of word w. We collected examples of polysemous words (head)"", ""(meaning)"", ""(nuclear)"", ""(record)"", ""(language)"", ""(chest)"") and monosemous words (production)"", ""(politics)"", ""(consciousness)"", ""(protest)"", ""(scores)"") and compared the variance values. We expected that variance values for polysemous and monosemous words would be large and small, respectively. We also investigated which BERT position has the most influence on polysemy. However, we found that it is difficult to measure polysemy using the previous variance values, and we considered the cause of this difficulty."
9392700,Measuring Semantic Similarity of Bengali Texts with Parts-of-Speech Tags and Word-Level Semantics,"The semantic textual similarity is essential for many applications related to natural language processing. But measuring the semantic similarity is not an easy task. Because there are different types of sentences and the diversities of sentences’ structure make assessing the semantic similarity a formidable task. When two texts are lexicographically dissimilar but semantically similar, the traditional lexical matching cannot return the actual degree of semantic similarity. Besides these, the lack of well-recognized language processing resources for Bengali texts makes the semantic textual similarity calculation a difficult task. In this paper, we tried to measure the semantic similarity of Bengali texts using word-level similarity and parts-of-speech tags. To assess the semantic similarity, we exploit the Bengali parts-of-speech tagger and pre-trained word-embedding model. Then, the maximum word-to-word similarity of the words is employed if the words belong to identical parts-of-speech tag. We also introduced a grammatical role level similarity in our proposed method to measure sentences’ similarity. To validate the performance of our method, we conducted experiments on a publicly available benchmark Bengali dataset. The results of the experiments demonstrated that our proposed method is effective to measure the degree of similarity of Bengali texts and achieved state-of-the-art performance."
9792411,Video Pivoting Unsupervised Multi-Modal Machine Translation,"The main challenge in the field of unsupervised machine translation (UMT) is to associate source-target sentences in the latent space. As people who speak different languages share biologically similar visual systems, various unsupervised multi-modal machine translation (UMMT) models have been proposed to improve the performances of UMT by employing visual contents in natural images to facilitate alignment. Commonly, relation information is the important semantic in a sentence. Compared with images, videos can better present the interactions between objects and the ways in which an object transforms over time. However, current state-of-the-art methods only explore scene-level or object-level information from images without explicitly modeling objects relation; thus, they are sensitive to spurious correlations, which poses a new challenge for UMMT models. In this paper, we employ a spatial-temporal graph obtained from videos to exploit object interactions in space and time for disambiguation purposes and to promote latent space alignment in UMMT. Our model employs multi-modal back-translation and features pseudo-visual pivoting, in which we learn a shared multilingual visual-semantic embedding space and incorporate visually pivoted captioning as additional weak supervision. Experimental results on the VATEX Translation 2020 and HowToWorld datasets validate the translation capabilities of our model on both sentence-level and word-level and generalizes well when videos are not available during the testing phase."
9031531,ICAN: Introspective Convolutional Attention Network for Semantic Text Classification,"Semantic text classification involves a deep understanding of natural language by going beyond mere syntactic information and capturing complex semantic properties like synonymy, polysemy and negation. We propose a novel attention mechanism called Introspective Semantic Attention embedded within a cascaded CNN architecture. We call our network Introspective Convolutional Attention Network (ICAN). In addition to extracting semantic information using convolution operations, ICAN derives semantic attention from its primary convolutional features instead of using a separate attention module. We also introduce a novel hybrid pooling strategy for our architecture which aids in preserving pertinent information encapsulated within a sentence, while discarding meaningless noise. Our architecture, while light-weight and efficient, promises high accuracy with respect to state of the art architectures - making it ideal for embedded systems and commercial servers alike."
9303639,Persian Emoji Prediction Using Deep Learning and Emoji Embedding,"The appearance of social networks and the increasing expansion of these networks has created many challenges, especially in the field of natural language processing. One of these social networks that has been welcomed by many researchers is Twitter. Twitter's users have the opportunity to consider one or more emojis for a tweet depending on the feeling and meaning of the tweet. Emojis contain information and concepts that the author of each tweet has in mind, the semantic and emotional range of each emoji is very wide and each emoji can be used in many different types of sentences. Therefore, by analyzing the content and emotion of each tweet, we can achieve the appropriate emoji of that tweet. For such reasons, predicting an emoji for a textual data is one of the challenges that has attracted the attention of researchers. In this article, using deep neural networks an attempt for the first time has been made to predict the emoji for Persian text data extracted from Twitter. And we were able to achieve F-score of 33% in 10 most frequent emojis which is 5% higher than the result of the SVM model and also 11% better than the result of the Naïve Bayes model, and F- score of 46% in 5 most frequent emojis which is 5% higher than the result of the SVM model and also 5% better than the result of the Naïve Bayes model."
9533371,A Multimodal Classification of Noisy Hate Speech using Character Level Embedding and Attention,"Hate speech has become a critical problem in all social media, leading to many hate crimes alongside affecting the mental and emotional well-being of affected individuals. This calls out for methods to detect online hate speech more than ever. While numerous architectures exist for hate speech detection in unimodal setup (i.e., either textual or visual) we have targeted the problem in the context of both text and images inspired by the real-world raw data which involves several modalities. We propose a multimodal hate speech classifier, called as Character Text Image Classifier (CTIC), which builds upon Bidirectional Encoder Representations from Transformers (BERT), Capsule Network, and EfficientNet involving four modalities, namely word embeddings, character embeddings, sentence embeddings, and images. We report the experiments performed upon our proposed model and several other models which have been tested in the process. We train our model with different sampling techniques, and selective training, upon a Twitter dataset, called MMHS150K, consisting of both texts and associated images. Our proposed multimodal approach attains better performance than the previous models constructed upon the dataset."
9095302,Capsule Networks With Word-Attention Dynamic Routing for Cultural Relics Relation Extraction,"Online museums and online cultural relic information provide abundant data for relation extraction research. However, in the relation extraction task of modelling space information, spatially insensitive methods of convolutional neural networks and long short term memory network in most current works still remain challenging in rich text structures, which makes models difficult to encode effectively and lacks the ability of text expression. To address this issue, we propose a framework named WAtt-Capsnet (the capsule network with word-attention dynamic routing), which is based on capsule networks with word-attention dynamic routing for the relation extraction task of online cultural relic data for capturing richer instantiation features. We further present combination embedding for capturing the characteristic information of Chinese sentences by considering the contribution of word embedding, parts of speech, character embedding and the position of words to capture rich internal structure information of sentences. More importantly, to reduce the decay of useful information in long sentences, we propose a routing algorithm based on a word-attention mechanism to focus on informative words. The experimental results demonstrate that the proposed method achieves significant performance for the relation extraction task of online cultural relic data."
8954611,SIFRank: A New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model,"In the age of social media, faced with a huge amount of knowledge and information, accurate and effective keyphrase extraction methods are needed to be applied in information retrieval and natural language processing. It is difficult for traditional keyphrase extraction models to contain a large amount of external knowledge information, but with the rise of pre-trained language models, there is a new way to solve this problem. Based on the above background, we propose a new baseline for unsupervised keyphrase extraction based on pre-trained language model called SIFRank. SIFRank combines sentence embedding model SIF and autoregressive pre-trained language model ELMo, and it has the best performance in keyphrase extraction for short documents. We speed up SIFRank while maintaining its accuracy by document segmentation and contextual word embeddings alignment. For long documents, we upgrade SIFRank to SIFRank+ by position-biased weight, greatly improve its performance on long documents. Compared to other baseline models, our model achieves state-of-the-art level on three widely used datasets."
9474589,Corporate Culture Explained by Mission and Vision Statements Using Natural Language Processing,"The study of corporate culture has a long-standing tradition in social sciences. With the advent of global digitalization, corporations are striving for winning culture strategies with the ambition to integrate seamlessly into the digital revolution and thus to succeed in the competitive, global market. During the last two years, research into the evaluation and transformation of corporate culture leveraging Natural Language Processing techniques has become increasingly notable. One interesting question that exists in this context is: can we reasonably explain corporate culture via analyzing corporations' mission, vision, and strategy statements? To tackle this problem, first we have collected some representative mission, vision, and strategy statements from 257 corporations operating in Switzerland, Germany, and Austria. Subsequently, leveraging word and sentence embedding models in Natural Language Processing, we have measured the so-called Big-9 cultural values: agility, collaboration, customer, diversity, execution, innovation, integrity, performance, and respect. Furthermore, the corporate culture reflected in texts of mission and vision has been validated using the external datasets from MyCareerGate, x28, and Glassdoor Culture 500, showing the preliminary effectiveness of our proposed methods and the potential challenges in evaluating corporate culture. Finally, more strategic solutions have been suggested regarding which firsthand data must be additionally collected and how to leverage these data for a comprehensive assessment of corporate culture."
9619833,Complexity of International Law for Cyber Operations,"Policy documents are usually written in text form— word after word, sentence after sentence etc.—which often obscures some of their most critical features. Text cannot easily situate interconnections among elements, or identify feedback, nor reveal other embedded features. This paper presents a computational approach to International Law Applicable to Cyber Operations 2.0, Tallinn Manual, a seminal work of 600 pages at the intersection of law and cyberspace. The results identify the dominance of specific Rules, the centrality of select Rules, and Rules with autonomous standing, as well as the feedback structure that holds the system together. None of these features are evident from the text alone."
8502148,Functional and Contextual Attention-Based LSTM for Service Recommendation in Mashup Creation,"Service recommendation is a fundamental task in many application environments (e.g., Mashup creation and cloud computing). In the past, various methods have been proposed to facilitate the service selection process based on the original functional descriptions. However, the mined features from the descriptions are usually too sparse for training a well-performed model. In addition, most methods neglect to differentiate the weights of various features, while words included in descriptions usually exhibit different intentions (e.g., functional or non-functional). To address these challenges, in this paper we propose a text expansion and deep model-based approach for service recommendation. Specifically, we first expand the description of services at sentence level based on a novel probabilistic topic model that learns topics of words, sentences and descriptions in a stratified fashion. The expansion process can bridge the vocabulary gap between services and user queries with the collective semantic similarity of sentences and descriptions. Then, we propose a Long Short-Term Memory-based model to recommend services with two attention mechanisms - a functional attention mechanism that takes tags as functional prior to mine the function-related features of services and Mashups, and a contextual attention mechanism that considers Mashup requirements as application scenario to help select the most appropriate services. We evaluate the proposed approach on a real-world dataset and the results show it has an improvement of 34 percent in F-measure over the basic LSTM model."
8876707,A Deep Learning Approach With Deep Contextualized Word Representations for Chemical–Protein Interaction Extraction From Biomedical Literature,"Mining interactions between chemicals and proteins/genes is of crucial relevance for clinical medicine, adverse drug effects, and pharmacological research. Although chemical-protein interactions (CPIs) can be manually extracted, this process is expensive and time-consuming. Therefore, it is of considerable significance to automatically extract CPIs from biomedical literature. Currently, the popular methods for CPI extraction are based on deep learning to avoid sophisticated handcrafted features derived from linguistic analyses. However, the performance of existing methods is usually unsatisfactory. The reasons may be that (1) traditional word-embedding methods cannot adequately model context information, and (2) it is difficult to effectively distinguish which words play critical roles in long biomedical sentences. In this study, we propose a novel Deep-contextualized Stacked Bi-LSTM model (DS-LSTM) to tackle the drawbacks of existing methods. Specifically, our model mainly consists of three components: deep contextualized word representations, the entity attention mechanism, and stacked bidirectional long short-term memory networks (Bi-LSTMs). The deep contextualized word representations are introduced to effectively model complex characteristics of word use (e.g., syntax and semantics) and the variations of these words in the context (i.e., to model polysemy), thereby generating context information. The entity attention mechanism is applied to prioritize the weights of words associated with target entities to distinguish which words play critical roles in long biomedical sentences. We evaluate our model on the CHEMPROT corpus. Our approach achieves a micro-averaged F-score of 69.44%, which is significantly higher than existing state-of-the-art methods. Experimental results show that our approach can adequately model context information, effectively distinguish which words play critical roles in long biomedical sentences and, therefore, improve the overall performa...
(Show More)"
9671925,Soft Sensing Transformer: Hundreds of Sensors are Worth a Single Word,"With the rapid development of AI technology in recent years, there have been many studies with deep learning models in soft sensing area. However, the models have become more and more complex, yet, the data sets remain limited: researchers are fitting million-parameter models with hundreds of data samples, which is insufficient to exercise the effectiveness of their models. To solve this long-lasting problem, we are providing large-scale high-dimensional time series manufacturing sensor data from Seagate Technology to the public. We demonstrate the challenges and effectiveness of modeling industrial big data by a Soft Sensing Transformer model on these data sets. Transformer is used because, it has outperformed state-of-the-art techniques in Natural Language Processing, and since then has also performed well in the direct application to computer vision without introduction of image- specific inductive biases. We observe the similarity of a sentence structure to the sensor readings and process the multi-variable sensor readings in a similar manner of sentences in natural language. The high-dimensional time series data is formatted into the same shape of embedded sentences and fed into the transformer model. The results show that transformer model outperforms the benchmark models in soft sensing field based on auto-encoder and long short-term memory (LSTM). To the best of our knowledge, we are the first team in academia or industry to benchmark the performance of original transformer model with large-scale numerical soft sensing data. Additionally, In contrast to the natural language processing or computer vision tasks where human-level performances are regarded as golden standards, our large-scale soft sensing study is an example that transformer is able to interpret high-dimensional numerical data which is not interpretable by human."
8995199,Distant-Supervised Relation Extraction with Hierarchical Attention Based on Knowledge Graph,"Relation Extraction is concentrated on finding the unknown relational facts automatically from the unstructured texts. Most current methods, especially the distant supervision relation extraction (DSRE), have been successfully applied to achieve this goal. DSRE combines knowledge graph and text corpus to corporately generate plenty of labeled data without human efforts. However, the existing methods of DSRE ignore the noisy words within sentences and suffer from the noisy labelling problem; the additional knowledge is represented in a common semantic space and ignores the semantic-space difference between relations and entities. To address these problems, this study proposes a novel hierarchical attention model, named the Bi-GRU-based Knowledge Graph Attention Model (BG2KGA) for DSRE using the Bidirectional Gated Recurrent Unit (Bi-GRU) network. BG2KGA contains the word-level and sentence-level attentions with the guidance of additional knowledge graph, to highlight the key words and sentences respectively which can contribute more to the final relation representations. Further-more, the additional knowledge graph are embedded in the multi-semantic vector space to capture the relations in 1-N, N-1 and N-N entity pairs. Experiments are conducted on a widely used dataset for distant supervision. The experimental results have shown that the proposed model outperforms the current methods and can improve the Precision/Recall (PR) curve area by 8% to 16% compared to the state-of-the-art models; the AUC of BG2KGA can reach 0.468 in the best case."
9478044,Performance Evaluation of BERT Vectors on Natural Language Inference Models,"Natural language inference aims to classify the binary relation between opinionated sentences as a contradiction, entailment, or neutral. To accomplish the task, classifiers transform textual data into numerical representations called vectors or embeddings. In this study, both static (Glove, OntoNotes5) and contextual (BERT) word embedding methods are used. Classifying the logical relationships between opinionated sentences is difficult. These sentences have complex grammatical structures to convert them into logical representations, and traditional natural language processing solutions are insufficient to meet the requirement. This study uses Decomposable Attention and Advanced LSTM for Natural Language Inference (ESIM) deep learning methods to perform this classification. The best accuracy score is achieved with 88% using ESIM - BERT on the SNLI corpus."
9763615,Knowledge Extraction: A Few-shot Relation Learning Approach,"More than 80% of human civilization information exists in the text. Knowledge extraction aims to obtain structured knowledge from text, a sub-task to build a knowledge graph. Current methods mainly target closed scenarios, but with the growth of knowledge, this will face challenges. Few-shot learning can extract new relations in open scenes. We proposed a bootstrapping method, Topic Model (TS), based on Neural Snowball to improve performance further. Specifically, we designed a new framework and embedded the instance selector tBERT that selects quality sentences. The relation classifier is trained from filtered sentences and a small amount of labeled data to predict new relations. Experiments show that our approach can screen out more diverse sentences for better few-shot relation learning. Moreover, Topic Snowball achieved significant improvement compared to Neural Snowball when the number of seed instances is small. Codes and datasets will be released soon."
9437745,A Framework for Document-level Cybersecurity Event Extraction from Open Source Data,"With the rapid development of the Internet, the number of cyber threats increases exponentially. More and more cyber threats come from new and unexpected sources, leading organizations and individuals to facing more security risks and vulnerabilities. Automatically obtaining and structuring security information from cybersecurity news can help security analysts to identify useful information more quickly. Most existing studies on extracting security events merely focused on the event detection task, aiming to discover and categorize cybersecurity events from the plain text. However, such event detection methods cannot capture useful information such as who performed the cyberattack, when the data breach event happened, who was the victim, etc. These arguments of a cybersecurity event are needed for analysts to get cybersecurity event details directly. Several studies have tried to extract rich semantic information of cybersecurity events, but they merely focused on extracting event arguments within the sentence scope. These studies still have limitations when the event arguments needed to recognize spread across multiple sentences. In this paper, we proposed a framework that effectively extracts cybersecurity events at the document-level from cybersecurity news, blogs and announcements. We model the document level event extraction task as a sequence tagging problem. The goal is to identify the related arguments of cybersecurity events from documents. Firstly, we get the characters embedding and incorporate the word information into the character representations. Then we design a sliding window mechanism to get the cross-sentence context information. Finally, we predict the label of each character. We build a Chinese cybersecurity dataset and use three methods to evaluate our method, and the experimental results demonstrate the effectiveness of the proposed model."
9678633,DeepCURATER: Deep Learning for CoURse And Teaching Evaluation and Review,"Course and Teaching Evaluation (CTE) provides rel-evant and important feedback to a course that would help teach-ers identify potential issues in the curriculum, teaching materials, pedagogy, assessment, among others. The proper evaluation and review of the feedback surveys would lead to meaningful and ben-eficial outcomes for the stakeholders such as teachers, curriculum designers, school administrators, and ultimately learners as well. However, manually analyzing these surveys has many challenging issues, such as, objectivity, consistency, and scalability. To tackle these challenges, we present DeepCURATER, a CTE analysis system and its design, development, and implementation. The system collects unstructured survey texts and summarizes the output in the form of automatically-generated labels based on clustering algorithms by using sentence embeddings. The system also performs the aspect-based sentiment analysis of the survey with high precision. The DeepCURATER system is implemented as a user-friendly web-based system that allows users to explore the analysis results with visualization that would provide stakeholders with feedback to improve the teaching and learning environment."
8919542,Participatory AI: Reducing AI Bias and Developing Socially Responsible AI in Smart Cities,"As smart cities evolve, artificial intelligence (AI) will increasingly be used to manage decisions for how cities operate. For everything from incarceration sentencing, city pension appropriation, surveillance and infrastructure management, AI will play a role. The author argues that implementing AI for a smart city should be decided similarly to how cities decide on major infrastructural planning projects. For both, there are social and ethical implications of deployment. A protocol is proposed for smart city AI so that AI can be seen as an ethical and trustworthy city asset rather than an adversary fraught with controversy and bias. This is achieved through participatory AI - the marriage of a fully transparent data architecture, such as the blockchain, and the urban planning practice of participatory planning. The diversity of opinions that participatory AI affords enables cities to facilitate socially responsible AI outcomes."
9023683,A Deep Learning Approach for Chinese Tourism Field Attribute Extraction,"Attribute extraction is a very significant and fundamental task of Natural Language Processing (NLP). It aims to extract missing attribute knowledge for entities from unstructured contexts. Attribute knowledge is an important part of the knowledge graph. Chinese tourism field attribute extraction can help people to construct knowledge graph in Chinese tourism field. In this paper, we crawl attractions information for different travel websites as annotating raw data. We formalize the problem as a sequence labeling task and propose a novel deep sequence labeling model, called BERT-ResCNNs-BLSTM-CRF. First, we fine-tune the pre-trained BERT model to get character embedding. We employ multiple convolutional layers to capture local features from character embedding. In order to tackle the vanishing gradient problem in deep convolutional networks, we use deep residual learning. Then the local features are concatenated with the character embedding vector to feed into Bidirectional long short-term network (BLSTM) to obtain local information of each sentence. Finally, we utilize the conditional random field (CRF) to predict a label sequence for an input sentence. As far as we know, we are the first to propose the joint model for sequence labeling task. Experimental results demonstrate the effectiveness of our method compared with several state-of-art baselines."
9063318,Optical Character Translation Using Spectacles (OCTS),"Optical Character Recognition (OCR) is reproducing the text as a digital format that has been produced by the non-computerized system. The translation is an essential part because people have barriers in languages. Especially when people read articles and books in their native language, they can understand more clearly and can get more ideas about the content. The framework and the portable hardware system developed takes images of printed documents and converts images as OCR text by using tesseract and then translates the text by using Google Translator API. The framework implements image capturing techniques, optical character recognition and translation using an embedded system based on Raspberry Pi. Daily used sentences were selected and captured in different development stages, viewpoints, angles, and background. Then the images are classified through the proposed system. Several pre-processing mechanisms were introduced in the process of OCR to get quality output via improving the accuracy rate of the system. From the experiment carried out on one hundred and twenty-three daily used sentences, 97% of the sentences were correctly detected and 91% Tamil and 89% Sinhala sentences were correctly translated."
9317476,Semantic Dependency Word Representation,"We provide a novel model architecture for learning Chinese word representations with semantic dependency information. These representations can solve two problems, the first is to avoid polysemy, the second is to capture remote features. Our representations are learned from a Chinese semantic dependency model and a normal word embedding model (e.g.,CBOW or Skip-gram). Using CBOW or Skip-gram model, the word representations can get some semantic information from the position of words. Semantic dependency model can find different word pairs in the sentence and predict the semantic relationship between each word pair. The first word of each word pair represents the current word, and the second word represents the target word. By fusing the semantic information after semantic analysis into the basic word representations, we can get more accurate and dynamic word representations. More importantly, we have achieved advanced results when we use these word representations in sentiment classification task and subject classification task."
9316186,A Novel Deep Learning-Based Multilevel Parallel Attention Neural (MPAN) Model for Multidomain Arabic Sentiment Analysis,"Over the past few years, much work has been done to develop machine learning models that perform Arabic sentiment analysis (ASA) tasks at various levels and in different domains. However, most of this work has been based on shallow machine learning, with little attention given to deep learning approaches. Furthermore, the deep learning models used for ASA have been based on noncontextualized embedding schemes that negatively impact model performances. This article proposes a novel deep learning-based multilevel parallel attention neural (MPAN) model that uses a simple positioning binary embedding scheme (PBES) to simultaneously compute contextualized embeddings at the character, word, and sentence levels. The MPAN model then computes multilevel attention vectors and concatenates them at the output level to produce competitive accuracies. Specifically, the MPAN model produces state-of-the-art results that outperform all established ASA baselines using 34 publicly available ASA datasets. The proposed model is further shown to produce new state-of-the-art accuracies for two multidomain collections: 95.61% for a binary classification collection and 94.25% for a tertiary classification collection. Finally, the performance of the MPAN model is further validated using the public IMDB movie review dataset, on which it produces an accuracy of 96.13%, placing it in second position on the global IMDB leaderboard."
9047338,Convolutional Neural Network with Character Embeddings for Malicious Web Request Detection,"In this paper, a deep learning based approach to malicious Web request detection is proposed, where a Web request is a message sent from a client to a server for communication. The characters in a request message can be considered as words in a sentence from the perspective of natural language processing. Consequently, character embeddings can be learned from a corpus of Web requests in an unsupervised manner via language models. In terms of Web requests, the URL path and request query string convey the most information, which can be extracted as the corpus to learn character-level embeddings. Furthermore, a specially designed convolutional neural network (CNN) is applied to the learned character-level embeddings for malicious and legitimate request identification. Our approach is totally data-driven, being free from hand-crafted features that require domain expertise and high computational cost. Extensive experiments conducted on HTTP DATASET CSIC 2010 dataset show significant improvement on the performance against existing state-of-the-art methods, especially for extremely imbalanced cases. The devised CNN model exploiting the embedding representation of Web requests achieves a false positive rate (FPR) of 0.02% with 98.72% on true positive rate (TPR), and 99.46% on accuracy (ACC), respectively. Codes are publicly available at https://github.com/acew14/CL-Embedding-CNN."
8954297,Weakly Supervised Video Moment Retrieval From Text Queries,"There have been a few recent methods proposed in text to video moment retrieval using natural language queries, but requiring full supervision during training. However, acquiring a large number of training videos with temporal boundary annotations for each text description is extremely time-consuming and often not scalable. In order to cope with this issue, in this work, we introduce the problem of learning from weak labels for the task of text to video moment retrieval. The weak nature of the supervision is because, during training, we only have access to the video-text pairs rather than the temporal extent of the video to which different text descriptions relate. We propose a joint visual-semantic embedding based framework that learns the notion of relevant segments from video using only video-level sentence descriptions. Specifically, our main idea is to utilize latent alignment between video frames and sentence descriptions using Text-Guided Attention (TGA). TGA is then used during the test phase to retrieve relevant moments. Experiments on two benchmark datasets demonstrate that our method achieves comparable performance to state-of-the-art fully supervised approaches."
9056501,Aceso: PICO-Guided Evidence Summarization on Medical Literature,"Evidence-Based Medicine (EBM) aims to apply the best available evidence gained from scientific methods to clinical decision making. A generally accepted criterion to formulate evidence is to use the PICO framework, where PICO stands for Problem/Population, Intervention, Comparison, and Outcome. Automatic extraction of PICO-related sentences from medical literature is crucial to the success of many EBM applications. In this work, we present our Aceso 1 system, which automatically generates PICO-based evidence summaries from medical literature. In Aceso, we adopt an active learning paradigm, which helps to minimize the cost of manual labeling and to optimize the quality of summarization with limited labeled data. An UMLS2Vec model is proposed to learn a vector representation of medical concepts in UMLS, 2 and we fuse the embedding of medical knowledge with textual features in summarization. The evaluation shows that our approach is better on identifying PICO sentences against state-of-the-art studies and outperforms baseline methods on producing high-quality evidence summaries."
9261114,Hear Sign Language: A Real-Time End-to-End Sign Language Recognition System,"Sign language recognition (SLR) bridges the communication gap between the hearing-impaired and the ordinary people. However, existing SLR systems either cannot provide continuous recognition or suffer from low recognition accuracy due to the difficulty of sign segmentation and the insufficiency of capturing both finger and arm motions. The latest system, SignSpeaker, has a significant limit in recognizing two-handed signs with only one smartwatch. To address these problems, this paper designs a novel real-time end-to-end SLR system, called DeepSLR, to translate sign language into voices to help people “hear” sign language. Specifically, two armbands embedded with an IMU sensor and multi-channel sEMG sensors are attached on the forearms to capture both coarse-grained arm movements and fine-grained finger motions. We propose an attention-based encoder-decoder model with a multi-channel convolutional neural network (CNN) to realize accurate, scalable, and end-to-end continuous SLR without sign segmentation. We have implemented DeepSLR on a smartphone and evaluated its effectiveness through extensive evaluations. The average word error rate of continuous sentence recognition is 10.8 percent, and it takes less than 1.1s for detecting signals and recognizing a sentence with 4 sign words, validating the recognition efficiency and real-time ability of DeepSLR in real-world scenarios."
9581629,An Effective Hybrid Approach Based on Machine Learning Techniques for Auto-Translation: Japanese to English,"In recent years machine learning techniques have been able to perform tasks previously thought impossible or impractical such as image classification and natural language translation, as such this allows for the automation of tasks previously thought only possible by humans. This research work aims to test a naïve post processing grammar correction method using a Long Short Term Memory neural network to rearrange translated sentences from Subject Object Verb to Subject Verb Object. Here machine learning based techniques are used to successfully translate works in an automated fashion rather than manually and post processing translations to increase sentiment and grammar accuracy. The implementation of the proposed methodology uses a bounding box object detection model, optical character recognition model and a natural language processing model to fully translate manga without human intervention. The grammar correction experimentation tries to fix a common problem when machines translate between two natural languages that use different ordering, in this case from Japanese Subject Object Verb to English Subject Verb Object. For this experimentation 2 sequence to sequence Long Short Term Memory neural networks were developed, a character level and a word level model using word embedding to reorder English sentences from Subject Object Verb to Subject Verb Object. The results showed that the methodology works in practice and can automate the translation process successfully."
8851884,Knowledge Adaptive Neural Network for Natural Language Inference,"Natural language inference (NLI) has received widespread attention in recent years due to its contribution to various natural language processing tasks, such as question answering, abstract text summarization, and video caption. Most existing works focus on modeling the sentence interaction information, while the use of commonsense knowledge is not well studied for NLI. In this paper, we propose knowledge adaptive neural network (KANN) that adaptively incorporates commonsense knowledge at sentence encoding and inference stages. We first perform knowledge collection and representation to identify the relevant knowledge. Then we use a knowledge absorption gate to embed knowledge into neural network models. Experiments on two benchmark datasets, namely SNLI and MultiNLI for natural language inference, show the advantages of our proposed model. Furthermore, our model is comparable to if not better than the recent neural network based approaches on NLI."
9674630,Joint Extraction of Entities and Relations by Adversarial Training and Mixup Data Augmentation,"Joint extraction of entities and relations is an important task for building a knowledge graph and information extraction. However, small interference in the text can greatly change the semantics of words and sentences in natural language, thereby affecting the model’s prediction results. To solve the sensitivity of text to noise, in this paper, we present a method, called BERT of Adversarial Training and Sentence Mixup Data Augmentation (BERT-AT-SMDA), which use the BERT pre-training model to obtain the embedding vector representation of the text and then to incorporate the FGM adversarial training strategy into the fine-tuning of Bert to construct adversarial samples. Both the adversarial sample and the original sample into the encoder to training. Then, to prevent the over-fitting phenomenon and the additional interference caused by the instability of adversarial training, we introduce the mixup data augmentation method after obtaining the hidden layer of the encoder output, which performs a linear interpolation between the output hidden layer. In addition, we also study the effect of linear interpolation at different positions on the model’s joint extraction of entities and relations. We conduct experiments on a public dataset produced by the distant supervision method. The experimental results show that our method is effective for the joint extraction method based on tagging scheme. Not only improves the accuracy of entity and relation extraction, and ensures the robustness and generalization of the model."
9726829,"Comparison and Analysis of the Positive and Negative words in the websites among Australia, UK and Canada","With the rapid development of the Internet, network public opinion forms more rapidly and spreads more widely, and has a great impact on the society. In the context of social network media, network public opinion has largely influenced and eventually changed people’s attitude towards the important topics such as government and national policies. For this reason, in this study, the paper analyzes how netizens are positive and negative to their countries in Australia, UK and Canada. The paper designs an efficient crawling system by embedding the two text corpora, and extracts the positive and negative words as well as the positive sentences by text mining. The results indicate the average of positive words in Australia is the largest, and the average of negative words in Canada is the smallest. Australia has the highest ratio of the positive words to negative words among the three countries. The average of positive sentences per website in Australia is 0.357, which is largest. The results reveal Australians are more positive to their country than British and Canadians. Finally, the suggestions to improve netizens positive to their countries are discussed."
9428222,Fine-Grained Discourse for Metaphor Detection,"Most current metaphor detection methods use restricted context, such as modeling the context of a single sentence. Considering the language environment of metaphors, we argue that combining broader discourse features has a greater impact on the improvement of metaphor detection performance. We propose a metaphor detection method based on fine-grained discourse, which embeds the current sentence and surrounding context in a weighted manner. With the help of fine-grained discourse, our model learns local and remote information as a reference for decision-making, and provides an efficient and natural method for metaphor detection tasks. Experimental results on VU Amsterdam Metaphor Corpus show that our technique surpasses the state-of-the-art models."
9508422,Text Backdoor Detection Using an Interpretable RNN Abstract Model,"Deep neural networks (DNNs) are known to be inherently vulnerable to malicious attacks such as the adversarial attack and the backdoor attack. The former is crafted by adding small perturbations to benign inputs so as to fool a DNN. The latter generally embeds a hidden pattern in a DNN by poisoning the dataset during the training process, which causes the infected model to misbehave on predefined inputs with a specific trigger and normally perform for others. Much work has been conducted on defending against the adversarial samples, while the backdoor attack received much less attention, especially in recurrent neural networks (RNNs), which play an important role in the text processing field. Two main limitations make it hard to directly apply existing image backdoor detection approaches to RNN-based text classification systems. First, a layer in an RNN does not preserve the same feature latent space function for different inputs, making it impossible to map the inserted specific pattern with the neural activations. Second, the text data is inherently discrete, making it hard to optimize the text like image pixels. In this work, we propose a novel backdoor detection approach named InterRNN for RNN-based text classification systems from the interpretation perspective. Specifically, we first propose a novel RNN interpretation technique by constructing a nondeterministic finite automaton (NFA) based abstract model, which can effectively reduce the analysis complexity of an RNN while preserving its original logic rules. Then, based on the abstract model, we can obtain interpretation results that explain the fundamental reason behind the decision for each input. We then detect trigger words by leveraging the differences between the behaviors in the backdoor sentences and those in the normal sentences. The extensive experiment results on four benchmark datasets demonstrate that our approach can generate better interpretation results compared to state-of-the-art approaches...
(Show More)"
9667004,Progressive Text-to-Face Synthesis with Generative Adversarial Network,"Text-to-Face synthesis has considerable challenges and potentials in the field of public safety. Compared with the Text-to-Image synthesis models, the text descriptions of facial features are more complex and diverse. For the text embedding, most of the previous Text-to-Face synthesis models only deal with a single sentence containing several features of face images, and the generated images are vague and lack of details. In this paper, a novel Progressive Text-to-Face synthesis with Generative Adversarial Network (PFGAN) is proposed to generate natural face images from text descriptions. Firstly, a new text encoding method Convolution-Deconvolution Word Embedding LSTM (CDWE-BLSTM) is leveraged as the text encoder, which tackles more complex sentences and improves the accuracy of text encoding. Secondly, the PFGAN is composed of multiple generators and discriminators arranged in a tree-like structure. Furthermore, face images at multiple scales are progressively generated from different branches of the tree, corresponding to the same descriptions. images at multiple scales corresponding to the same scene are generated from different branches of the tree. By comparing with three existing Text-to-Face synthesis methods, extensive experiments demonstrate that the proposed PFGAN is very competitive in the IS (Inception Scores), FID (Frechet Inception Distance) and resolution of the generated face images."
9750480,Relation Extraction based on Data Partition and Representation Integration,"Relation extraction (RE) is the cornerstone of many natural language processing applications. The success of machine learning algorithms generally depends on the embedding representation of data, since it involves different explanatory factors of variation behind the data. This paper explores integrating multiple representations to improve relation extraction. Note that the shortest dependency path (SDP) can retain relevant information and eliminate irrelevant words in the sentence, we partition the input dataset into multiple subsets based on the deformed SDP. For the input dataset and each subset, we train different encoders respectively. The input dataset encoder pays more attention to words in SDP, while the subset encoders pay more attention to words beyond SDP. In this way, we can get multiple embedding representations of diversity for the same sentence and improve the performance of RE by integrating them with predefined strategies. Experimental results on a widely used dataset demonstrate the effectiveness of our approach."
9134909,A Deep Learning Model for RNA-Protein Binding Preference Prediction Based on Hierarchical LSTM and Attention Network,"Attention mechanism has the ability to find important information in the sequence. The regions of the RNA sequence that can bind to proteins are more important than those that cannot bind to proteins. Neither conventional methods nor deep learning-based methods, they are not good at learning this information. In this study, LSTM is used to extract the correlation features between different sites in RNA sequence. We also use attention mechanism to evaluate the importance of different sites in RNA sequence. We get the optimal combination of k-mer length, k-mer stride window, k-mer sentence length, k-mer sentence stride window, and optimization function through hyper-parm experiments. The results show that the performance of our method is better than other methods. We tested the effects of changes in k-mer vector length on model performance. We show model performance changes under various k-mer related parameter settings. Furthermore, we investigate the effect of attention mechanism and RNA structure data on model performance."
9312396,Multiple Embeddings Enhanced Multi-Graph Neural Networks for Chinese Healthcare Named Entity Recognition,"Named Entity Recognition (NER) is a natural language processing task for recognizing named entities in a given sentence. Chinese NER is difficult due to the lack of delimited spaces and conventional features for determining named entity boundaries and categories. This study proposes the ME-MGNN (Multiple Embeddings enhanced Multi-Graph Neural Networks) model for Chinese NER in the healthcare domain. We integrate multiple embeddings at different granularities from the radical, character to word levels for an extended character representation, and this is fed into multiple gated graph sequence neural networks to identify named entities and classify their types. The experimental datasets were collected from health-related news, digital health magazines and medical question/answer forums. Manual annotation was conducted for a total of 68,460 named entities across 10 entity types (body, symptom, instrument, examination, chemical, disease, drug, supplement, treatment and time) in 30,692 sentences. Experimental results indicated our ME-MGNN model achieved an F1-score result of 75.69, outperforming previous methods. In practice, a series of model analysis implied that our method is effective and efficient for Chinese healthcare NER."
9761302,Aspect Based Sentiment Analysis for Customer Reviews,"In todays world people buy their product based on customer This project aims at providing a service for business which would do aspect-based product rating and data visualizations using the existing reviews of the product. In online shopping sites or in movie-info sites, many user reviews are present. Many of these reviews contain useful specific information about the various features of the product. In the existing systems, we do not get accurate sentiment results if we review sentences having more than one aspect about the product or service. Various aspects may be present in the sentence with varying polarities and this may lead to the misinterpretation of the products or services to the customers online. Performing natural language processing using the Natural Language Tool Kit (NLTK) on these structured data and doing sentiment analysis on the processed text, will help us to get aspect-based sentiment analysis of a particular feature whether it is good or bad. These analyses are later processed by a mathematical model and converted into individual features rating. We get the results by providing the aspects present in the customer review and display its rating out of 5 using a mathematical model. It helps to provide an easy way for customers to know about the goodness of a feature in a product through real-world customer reviews rather than the specification of the feature."
9095571,Multi-Level Attentional Network for Aspect-based Sentiment Analysis,"Aspect-based Sentiment Analysis (ABSA) has attracted much attention due to its capacity to determine the sentiment polarity of the certain aspect in a sentence. In previous works, great significance of the interaction between aspect and sentence has been exhibited in ABSA. In consequence, a Multi-Level Attentional Networks (MLAN) is proposed. MLAN consists of four parts: Embedding Layer, Encoding Layer, Multi-Level Attentional (MLA) Layers and Final Prediction Layer. Among these parts, MLA Layers including Aspect Level Attentional (ALA) Layer and Interactive Attentional (ILA) Layer is the innovation of MLAN, and its function is to focus on the important information and obtain multiple levels' attentional weighted representation of aspect and sentence, which makes MLAN superior than the most of deep learning models for ABSA. In the experiments, MLAN is compared with classical TD-LSTM, MemNet, RAM, ATAE-LSTM, IAN, AOA, LCR-Rot and AEN-Globe on SemEval 2014 Dataset. The experimental results show that MLAN outperforms those state-of-the-art models greatly. And in case study, the works of ALA Layer and ILA Layer have been proven to be effective and interpretable."
9795573,Thai Tokenizer Invariant Classification Based on Bi-LSTM and DistilBERT Encoders,"This research aims to solve Thai word boundaries problem by grouping vectors of words obtained from various tokenizers in the same sentence using Bi-LSTM and DistilBERT encoders. Generally, one sentence can achieve different sets of words from multiple tokenizers, resulting in uncontrollable accuracies in various tasks. The proposed method based on Bi-LSTM and DistilBERT is to train and transform data with triplet hard loss to a particular domain where the embedding vectors of each same sentence are much closer to each other. In this study, sentiment classification is conducted to show that our method can be used as pre-trained process for other tasks."
9456108,Local and Global Feature Based Hybrid Deep Learning Model for Bangla Parts of Speech Tagging,"Parts of Speech (pos) tagging corresponds to techniques of tagging accurate pos tags of each word of a sentence. It is a pivotal and indispensable part of sentiment analysis, synthesizing text to speech, analyzing textual data and many more Natural Language Processing (NLP) tasks. Though there are many researches done in the field of languages like English, Chinese etc., it is still comparatively unexplored for Bangla. An excellent pos tagger in Bangla language can accelerate NLP of Bangla textual data. In this study, we have developed a novel hybrid Deep Learning (DL) model with two parallel input networks with Convolutional Neural Network (CNN), and Bidirectional Long Short Term Memory (BiLSTM), and an output network of CNN. CNN works well regarding the local dependencies of words in sentences, and BiLSTM works well with the global dependencies of words regarding the context in a sentence. No other work has been done with local features of textual data along with long term word dependencies in Bangla pos tagging studies. The model can work with any external Word Embeddings. In this study, it is implemented with both Embedding Layer and Word2Vec layer. In the evaluation process, Mathews Correlation Coefficient (MCC), Precision-Recall curve are used along with accuracy and F1-score due to the dataset's imbalanced characteristic. Our proposed model accompanied by Word2Vec layer has outperformed Vanilla Recurrent Neural Network (RNN), Gated Recurrent Units (GRU), Long Short Term Memory (LSTM), BiLSTM networks, and also one recent study using the same dataset with the highest accuracy of 0.974, F1-score of 0.883, and MCC value of 0.936."
9512062,UTSA: Urdu Text Sentiment Analysis Using Deep Learning Methods,"The Internet has seen substantial growth of regional language data in recent years. It enables people to express their opinion by incapacitating the language barriers. Urdu is a language used by 170.2 million people for communication. Sentiment analysis is used to get insight of people opinion. In recent years, researchers' interest in Urdu sentiment analysis has grown. Application of deep learning methods for Urdu sentiment analysis has been least explored. There is a lot of ground to cover in terms of text processing in Urdu since it is a morphologically rich language. In this paper, we propose a framework for Urdu Text Sentiment Analysis (UTSA) by exploring deep learning techniques in combination with various word vector representations. The performance of deep learning methods such as Long Short-Term Memory (LSTM), attention-based Bidirectional LSTM (BiLSTM-ATT), Convolutional Neural Networks (CNN) and CNN-LSTM is evaluated for sentiment analysis. Stacked layers are applied in sequential model LSTM, BiLSTM-ATT, and C-LSTM. In CNN, various filters are used with single convolution layer. Role of pre-trained and unsupervised self-trained embedding models is investigated on sentiment classification task. The results obtained show that the BiLSTM-ATT outperformed other deep learning models by accomplishing 77.9% accuracy and 72.7% F1 score."
9734125,Dedicated Farm-Haystack Question Answering System for Pregnant Women and Neonates Using Corona Virus Literature,"The global pandemic, COVID-19 has made it more important to quickly and precisely retrieve critical information for effective use by specialists in a wide range of fields. Domain question answering system will work or produce good results to certain extent but still favour more to the pretrained dataset. In this work we target developing a customized question answering framework that can assist the medical network with retrieval of answers to important logical questions like risk factors, effective modes of communication, various treatment options for target high-risk populaces like pregnant women and neonates.The proposed framework uses a customized Farm-Haystack question answering system and introduces a novel pipeline architecture using latent dirichlet allocation and bidirectional encoder representation from transformers for embedding the information. The system is modeled to produce the best and reliable answers for the delicate population, which requires more efficient answers rather than generic population, which can be answered using pretrained systems. In this context, the system has showed the accurate and compact answers for different inquiries related to the sensitive population."
9533453,Transformer-based Relation Detect Model for Aspect-based Sentiment Analysis,"Aspect-based sentiment analysis (ABSA) aims to detect the sentiment polarities of a sentence with a given aspect. Aspect-term sentiment analysis (ATSA) is a subtask of ABSA, in ATSA, the aspect is given by aspect term: a word or a phrase in sentence. In previous work, most models apply attention mechanism or gating mechanism to capture the key part of the sentence and detect the sentiment polarity by classifying the weighted sum vector, which are regardless of the aspect information during the classification. However, same contexts may show different sentiment polarity with differnet aspects. For example, in sentence “The scene hunky waiters dub diner darling and it sounds like they mean it.” the context “dub diner dearling” shows negative polarity towards the aspect term “waiters”. But in sentence “The diner's husband dub diner darling.”, the same context would show neural polarity towards “husband”. The absent of aspect information would make model get a wrong result in some sentence. To solve this problem, we propose a model called Trasfomer-based context-aspect Relation Detect Model (TRDM), and add two special tokens “[CLSA]” and “[SEPA]” to specify the aspect term in the sentence. TRDM uses the embedding of two tyes of tokens (i.e. “[CLS]” and “[CLSA]”) for classification where “[CLS]” is used to represent the entire sentence in BERT. This scheme enable TRDM to combine the sentence information and aspect information in the step of sentiment polarity classification. We evaluate the performance of our model on two datasets: Restaurant dataset from SemEval2014 and MAMS from NLPCC 2020. Experiment results show that our model obtain noticeable improvement compared with state-of-art transfromer-based models."
8854152,Aspect-Based Sentiment Analysis with New Target Representation and Dependency Attention,"Aspect-based sentiment analysis (ABSA) is crucial for exploring user feedbacks and preferences on produces or services. Although numerous classical deep learning-based methods have been proposed in previous literature, several useful cues (e.g., contextual, lexical, and syntactic) are still not fully considered and utilized. In this study, a new approach for ABSA is proposed through the guidance of contextual, lexical, and syntactic cues. First, a novel sub-network is introduced to represent a target in a sentence in ABSA by considering the whole context. Second, lexicon embedding is applied to incorporate additional lexical cues. Third, a new attention module, namely, dependency attention, is proposed to elaborate syntactic dependency cues between words in attention inference. Experimental results on four benchmark data sets demonstrate the effectiveness of our proposed approach to aspect-based sentiment analysis."
9640269,Token Level Evaluation and Feature Enhancer for Transformer-based Models,"Most recent state-of-the-art models in the natural language processing (NLP) field such as BERT, ALBERT and XLNet share a common architecture of including an embedding and Transformer encoder and/or decoder layers (aka. Transformer blocks). Here we propose Token Level Evaluation and Feature Enhancer (TLEFE) to be added on any of the Transformer-based models. The TLEFE is supposed to upscale token features for every token in the sentence passed into the system. Through experiments using datasets provided by SemEval-2020 Commonsense Validation and Explanation, we have demonstrated that TLEFE applied model performs better than plain model with negligible additional number of parameters."
9429059,Development of Domain-Specific Lexicon for Aspect-Based Sentiment Analysis,"Aspect term extraction is one of the main subtasks in aspect-based sentiment analysis. An aspect extraction method based on the Sequential Covering algorithm successfully used a list of aspects and opinion words to improve the performance of aspect extraction. The aspect and opinion list used in the method is crafted manually, costing a significant amount of time and effort. To ease the effort, we proposed a way to automatically build the aspect and opinion list. We also modified the scope of the word list to a larger domain. The resulting word list is therefore called domain-specific lexicon. In this paper, we used word embedding technique to develop domain-specific lexicons of size 1000. The data used in the development of domain-specific lexicon were collected from review websites using a focused web crawler. The resulting domain-specific lexicons were used in the modified Sequential Covering method. We used a total of 3,124 review sentences from the digital camera, handphone, and restaurant domain to evaluate the performance of the modified Sequential Covering method. Experimental results showed that this method managed to yield better F1 scores than the baseline Sequential Covering method. The best F1 scores for each dataset were as follows: 0.645 for Nikon Coolpix 4300, 0.581 for Canon G3, 0.629 for Nokia 6610, and 0.705 for ABSA16_Restaurants_Train_SB1."
9643300,Diagnosis Detection Support based on Time Series Similarity of Patients Physiological Parameters,"Facilitate clinicians in their complex decision- making processes allows to improve patient outcomes along disease-specific pathways and healthcare delivery. This could be realized by enhancing medical decisions with targeted clinical knowledge, patient information, and other health data. Nowadays, a key paradigm in healthcare, designed to be a direct aid to clinical-decision making, is the Clinical Decision Support System (CDSS). Along this line, the work presented in the paper focuses on detecting patients diagnosis, proposing a learning methodology that, on the basis of the current patient status, clinical history, diagnostic and results from their pathological reports, provides insights for clinicians in the diagnosis and therapy processes. The patients physiological signals have been modeled as time series and the similarity among them has been exploited. The main idea is that patients with similar patterns of vital signs are affected by the same or similar health problems and, therefore, may have the same or very close diagnoses. The diagnosis detection method is formulated as a classification problem, combining time series similarity and an ad-hoc multi- label k-nearest neighbor approach (ML-KNN). The proposed classifier exploits the semantic similarity of diagnoses catched through sentence embedding. Results, performed over a real- world clinical dataset, show that the proposed approach is able to successfully detect diagnoses with a precision up to about 75%."
9170726,Identifying Hidden Sentiment in Text Using Deep Neural Network,"Deep learning has emerged as de-facto standard in computer vision. Due to its wide spread success and popularity with image data almost every problem from other domain is now trying to finding its solution with deep learning and the case is no different for text analytics and language modeling in NLP. In sentiment analysis, identifying users' hidden sentiment in text review is a challenging task. Though there are many rule based and statistical machine learning based approaches for sentiment analysis. Most of these machine learning approaches are probabilistic based model and treats the problem as document classification problem based on the word count and their polarity often missing sentiment. With the introduction of the distributed word vector representation embedding models like word2vec, GloVe and FastText, capturing the meaning of the neighboring words for a give target word in the sentence, NLP expectation with deep learning in sentiment analysis has increased a lot. In this paper we used deep neural network based model to perform sentiment analysis on the IMDB movie review dataset. We compare deep learning based approach for sentiment analysis with other traditional machine learning approaches to evaluate the performance of the model. With experiment result we found that the deep learning based NLP model outperformed the traditional machine learning approaches in several ways."
8754254,Chinese Social Media Entity Linking Based on Effective Context with Topic Semantics,"On social media, entity linking is very important for natural language processing tasks, such as Sentiment Analysis, Question Answering (QA) and Machine Translation. Compared to English-oriented entity linking, Chinese entity linking has its special difficulties. Just like the entity linking for short text, Chinese microblogs have lots of noise and the mention lacks effective context information. In order to solve these problems, we present a new model for Chinese microblogs entity linking. Entity linking usually includes two steps: candidate entities generation and candidate entities ranking. First, based on the characteristics of Chinese, we put forward multi-method fusion strategies for candidate generation to improve the recall rate of candidate entities. Second, we propose a new neural network model called TAS (Topic attention Siamese) for candidate entities ranking. In TAS model, we add effective topic semantics on Siamese network to learn representations of context, mention and entity, and rank the mention-entity similarity. The representation of mention incorporates information from multiple sentences on the same topic, which can effectively solve the problem of the lack of contextual information. We also use Character-enhanced Word Embedding model (CWE) to pre-train both word embedding and characters embedding to work out noise and word segmentation impact. Experimental results demonstrate that our method significantly outperforms the state-of-the-art results for entity linking on Chinese social media."
9207194,A Novel Ensemble Representation Framework for Sentiment Classification,"Text representation has a critical impact on the accuracy of text classifiers which is imperative to be strengthened. On the other hand, the question of how the state-of-the-art embeddings outperform previous approaches cannot be well explained. To advance text representation and better understand the internal mechanism, we propose a novel end-to-end framework named Ensemble Framework for Text Embedding (EFTE), which weightedly combines diverse embeddings and simultaneously represents sentences' and tokens' features in a more reasonable way. According to the experimental results in sentiment classification, our proposed embedding apparently improves the effectiveness compared to six single embeddings. Moreover, the importance of each embedding in terms of EFTE integration and how different embeddings influence the results by classification are discussed."
9478180,Building a Question and Answer System for News Domain,"This project attempts to build a Question-Answering system in the News Domain, where Passages will be News articles, and anyone can ask a Question against it. We have built a span-based model using an Attention mechanism, where the model predicts the answer to a question as to the position of the start and end tokens in a paragraph. For training our model, we have used the Stanford Question and Answer (SQuAD 2.0) dataset[1]. To do well on SQuAD 2.0, systems must not only answer questions when possible but also determine when no answer is supported by the paragraph and abstain from answering. Our model architecture comprises three layers- Embedding Layer, RNN Layer, and the Attention Layer. For the Embedding layer, we used GloVe and the Universal Sentence Encoder. For the RNN Layer, we built variations of the RNN Layer including bi-LSTM and Stacked LSTM and we built an Attention Layer using a Context to Question Attention and also improvised on the innovative Bidirectional Attention Layer. Our best performing model which uses GloVe Embedding combined with Bi-LSTM and Context to Question Attention achieved an F1 Score and EM of 33.095 and 33.094 respectively. We also leveraged transfer learning and built a Transformer based model using BERT. The BERT-based model achieved an F1 Score and EM of 57.513 and 49.769 respectively. We concluded that the BERT model is superior in all aspects of answering various types of questions."
8851892,Generating Natural Video Descriptions using Semantic Gate,"Video captioning task aims to generate a textual description of the situation in a video. It is challenging because of the nature of modality-difference between video and language. We present a novel method to bridge the gap between them by utilizing the semantic gate in two ways. First, we develop an activation mechanism to make a video description that captures the concept of the video. Next, we design a network that evaluates the similarity between visual and sentence feature. Semantic gate is used to transform sentence into a semantic embedding. We also conduct experiments to show that image and action classification task performance is transferred to video captioning task. Experimental results show that our proposed method has gained promising improvements compared to the baseline model. Consequently, our model demonstrated the effectiveness by achieving new best record on MSRVTT and MSVD dataset."
9771498,Artificial Intelligence for Unidentified Mode S Registers Decoding,"Aeronautical surveillance is primarily based on cooperative sensors that perform an exchange of messages between aircraft and surveillance ground infrastructure. The aircraft electronic equipment in charge of the messages exchange is called transponder. In addition to basic surveillance information, the transponder also holds registers with avionic related data such as the selected altitude, the magnetic heading, the ground speed, etc. However, messages containing data from transponder registers do not always include an identification number, making the decoding of such messages impractical for an external observer.In this paper, a new method to identify a transponder register without prior knowledge of its identification number is presented, allowing an external observer to decode correctly registers passively received.The proposed method is based on an innovative approach using Artificial Intelligence and more specifically Natural Language Processing (NLP) techniques applied to surveillance messages. Each message is considered as a sentence and is evenly split into words. First, a tokenization layer identifies each word, then the embedding layer attaches to them a fixed number of properties, and finally a recurrent neural network recomposes the sentence and ensures the message identification.The implemented method achieves a very good performance, showing an accuracy in the registers identification of 0.999987, which is significantly higher than any other existing solutions."
9690073,TF-SS Matching: Retrieval of Power Industry Technical Standards via Term Frequency and Semantic Similarity Matching,"Power industry technical standard retrieval is an information retrieval problem that aims to search relevant candidate documents according to query phrases or sentences. How to correctly match queries to the corpus and utilize term frequency and semantic features is critical for our task. In this work, we propose a novel solution to these problems based on term frequency (TF) and semantic similarity (SS) matching. Specifically, it consists of three key components: (1) TF matching aims to mine the significance of a particular term within the overall document while filtering out the content without the queries. This way is suitable for accurate retrieval. (2) SS matching is formulated into a deep metric learning framework to jointly learn the sentence representations and semantical embedding metric. (3) Rerank the fusion results from the mentioned TF and SS models to refine the output ranking list. Extensive experiments on the dataset, which contains 38 frequently used technical standards in daily work, prove the effectiveness of our proposed method. Our SS matching achieves comparable results with complex polyencoders. Combined with TF matching, our TF-SS matching can achieve state-of-the-art performance with 80.84% recall, 84% MRR, and 79.82% ROUGE."
8845685,Referring Image Segmentation by Generative Adversarial Learning,"Referring expression is a kind of language expression being used for referring to particular objects. In this paper, we focus on the problem of image segmentation from natural language referring expressions. Existing works tackle this problem by augmenting the convolutional semantic segmentation networks with an LSTM sentence encoder, which is optimized by a pixel-wise classification loss. We argue that the distribution similarity between the inference and ground truth plays an important role in referring image segmentation. Therefore we introduce a complementary loss considering the consistency between the two distributions. To this end, we propose to train the referring image segmentation model in a generative adversarial fashion, which well addresses the distribution similarity problem. In particular, the proposed adversarial semantic guidance network (ASGN) includes the following advantages: a) more detailed visual information is incorporated by the detail enhancement; b) semantic information counteracts the word embedding impact; c) the proposed adversarial learning approach relieves the distribution inconsistencies. Experimental results on four standard datasets show significant improvements over all the compared baseline models, demonstrating the effectiveness of our method."
9423269,DORi: Discovering Object Relationships for Moment Localization of a Natural Language Query in a Video,"This paper studies the task of temporal moment localization in long untrimmed videos using natural language queries. Given a query sentence, the goal is to determine the start and end of the relevant segment within the video. Our key innovation is to learn a video feature embedding through a language-conditioned message-passing algorithm suitable for temporal moment localization which captures the relationships between humans, objects and activities in the video. These relationships are obtained by a spatial sub-graph that contextualizes the scene representation using detected objects and human features conditioned in the language query. Moreover, a temporal sub-graph captures the activities within the video through time. Our method is evaluated on three standard benchmark datasets, and we also introduce YouCookII as a new benchmark for this task. Experiments show our method outperforms state-of-the-art methods on these datasets, confirming the effectiveness of our approach."
9336561,Drug Reaction Discriminator within Encoder-Decoder Neural Network Model: COVID-19 Pandemic Case Study,"Social networks become widely used for understanding patients shared experiences, and reaching a vast audience in a matter of seconds. In particular, many health-related organizations used sentiment analysis to automatically reporting treatment issues, drug misuse, new infectious disease symptoms. Few approaches have proposed in this matter, especially for detecting different drug reaction descriptions from patients generated narratives on social networks. Most of them consisted of only detecting adverse drug reaction(ADR), but may fail to retrieve other aspect, e.g, the beneficial drug reaction or drug retroviral effects such as “relieve intraocular pressure associated with glaucoma”. In this study, we propose to develop an encoder-decoder for drug reaction discrimination that involves an enhanced distributed biomedical representation from controlled medical vocabulary such as PubMed and Clinical note MIMIC III. The embedding mechanism primarily leverages contextual information and learn from predefined clinical relationships in term of medical conditions in order to define possible drug reaction of individual meaning and multi-word expressions in the field of distributional semantics configuration that clarifies sentence's similarity in the same contextual target space, which are further share semantically common drug description meanings. Furthermore, the bidirectional sentiment inductive model are created to enhance drug reactions vectorization from real-world patients description whereby achieved higher performance in terms of disambiguating false positive and/or negative assessments. As a result, we achieved an 85.2% accuracy performance and the architecture shows a well-encoding of real-world drug entities descriptions."
9096328,Disease-Pertinent Knowledge Extraction in Online Health Communities Using GRU Based on a Double Attention Mechanism,"Relationship extraction among diseases, symptoms and tests has always been a concerning research issue in the biomedical field. Disease-pertinent relationship extraction for user-generated content in the online health community represents a research trend. By training the word embedding vectors for the medical-health field, conducting entity recognition and relationship annotation, and using deep learning technology, we construct a relation extraction model for extracting the relationships among diseases, symptoms and tests. Our relationship extraction model of the bidirectional gate recurrent unit (BiGRU) network based on character-level and sentence-level attention mechanisms achieved the best results on question-answer data in the online health community. Our research results can not only help physician diagnoses but also help patients perform health management, which has important industrial application value."
9035290,Outperforming State-of-the-Art Systems for Aspect-Based Sentiment Analysis,"Aspect-Based Sentiment Analysis (ABSA) is a very important problem with numerous applications. The three editions of SemEval's ABSA Shared Task have been instrumental in fostering the development in this field. One of its sub-tasks is the sentence-level ABSA. This sub-task has received a lot of attention and new techniques and better results are reported on it frequently. The purpose of this work is to achieve the highest accuracy for this problem. We follow a state-of-the-art (SOTA) approach that is based on multi-grain attention networks and infuse it with better embedding mechanisms in order to improve the results. For the famous SemEval's ABSA Shared Task, the results of the SOTA approaches reach 81.25 accuracy and 71.94 F1 score, whereas our approach surpasses them with 83.75 accuracy and 75.75 F1 score."
9688174,Non-Autoregressive Mandarin-English Code-Switching Speech Recognition,"Mandarin-English code-switching (CS) is frequently used among East and Southeast Asian people. However, the intra-sentence language switching of the two very different languages makes recognizing CS speech challenging. Meanwhile, the recent successful non-autoregressive (NAR) ASR models remove the need for left-to-right beam decoding in autoregressive (AR) models and achieved outstanding performance and fast inference speed, but it has not been applied to Mandarin-English CS speech recognition. This paper takes advantage of the Mask-CTC NAR ASR framework to tackle the CS speech recognition issue. We further propose to change the Mandarin output target of the encoder to Pinyin for faster encoder training and introduce the Pinyin-to-Mandarin decoder to learn contextualized information. Moreover, we use word embedding label smoothing to regularize the decoder with contextualized information and projection matrix regularization to bridge that gap between the encoder and decoder. We evaluate these methods on the SEAME corpus and achieved exciting results."
9447458,Anomaly Detection Mechanism Based on Hierarchical Weights through Large-Scale Log Data,"In order to realize Intelligent Disaster Recovery and break the traditional reactive backup mode, it is necessary to forecast the potential system anomalies, and proactively backup the real-time datas and configurations. System logs record the running status as well as the critical events (including errors and warnings), which can help to detect system performance, debug system faults and analyze the causes of anomalies. What's more, with the features of real-time, hierarchies and easy-access, log data can be an ideal source for monitoring system status. To reduce the complexity and improve the robustness and practicability of existing log-based anomaly detection methods, we propose a new anomaly detection mechanism based on hierarchical weights, which can deal with unstable log data. We firstly extract semantic information of log strings, and get the word-level weights by SIF algorithm to embed log strings into vectors, which are then feed into attention-based Long Short-Term Memory(LSTM) deep learning network model. In addition to get sentence-level weight which can be used to explore the interdependence between different log sequences and improve the accuracy, we utilize attention weights to help with building workflow to diagnose the abnormal points in the execution of a specific task. Our experimental results show that the hierarchical weights mechanism can effectively improve accuracy of perdition task and reduce complexity of the model, which provides the feasibility foundation support for Intelligent Disaster Recovery."
9206632,Extricating from GroundTruth: An Unpaired Learning Based Evaluation Metric for Image Captioning,"Recently, instead of pursuing high performance on classical evaluation metrics, the research focus of image captioning has shifted to generating sentences which are more vivid and stylized than human-written ones. However, there are still no applicable metrics which can judge how close the generated captions are to the human-written ones. In this paper, we propose a novel learning-based evaluation metric, namely Unpaired Image Captioning Evaluation (UICE), which can be trained to distinguish between human-written and generated captions. Unlike existing metrics, our UICE consists of two parts: the semantic alignment module measuring the semantic distance between extracted image features and caption meanings, and the syntactic discriminating module syntactically judging how human-like the candidate caption is. The semantic alignment module is implemented by mapping the image features and the word embedding into a unified tensor space. And the syntactic discriminating module is designed to be learning-based, and thereby can be trained to be stylized by users' own, fed with additional personalized corpus during the training process. Extensive experiments indicate that our metric can correctly judge the grammatical correctness of generated captions and the semantic consistency between captions and corresponding images."
9729805,A WeChat Official Account Reading Quantity Prediction Model Based on Text and Image Feature Extraction,"This paper describes a study that built a neural network prediction model based on feature extraction, focusing on text analysis and image analysis of WeChat official accounts reading quantity. Based on the embedding method of the deep learning model, we extracted the text features in the title and the image features in the cover picture, explored the relationship between these features and the reading quantity, and built a neural network model based on these features to predict the reading quantity. The results show that there is a phenomenon of sentiment fusion in the text, and a sentence vector model based on Doc2Vec and a neural network model both had a good performance. This paper proposes a tool that can predict the reading quantity in advance and help administrators adjust the titles and images according to the predicted results."
9659189,Sentiment and Context-refined Word Embeddings for Sentiment Analysis,"Word embeddings have become the de-facto tool for representing text in natural language processing (NLP) tasks, as they can capture semantic and syntactic relations, unlike their precedents such as Bag-of-Words. Although word embeddings have been employed in various studies in recent years and proven to be effective in many NLP tasks, they are still immature for sentiment analysis, as they suffer from insufficient sentiment information. General word embedding models pre-trained on large corpora with methods such as Word2Vec or GloVe achieve limited success in domain-specific NLP tasks. On the other hand, training domain-specific word embeddings from scratch requires a high amount of data and computation power. In this work, we target both shortcomings of pre-trained word embeddings to boost the performance of domain-specific sentiment analysis tasks. We propose a model that refines pre-trained word embeddings with context information and leverages the sentiment scores of sentences obtained from a lexicon-based method to further improve performance. Experiment results on two benchmark datasets show that the proposed method significantly increases the accuracy of sentiment classification."
9458619,NewsLink: Empowering Intuitive News Search with Knowledge Graphs,"News search tools help end users to identify relevant news stories. However, existing search approaches often carry out in a ""black-box"" process. There is little intuition that helps users understand how the results are related to the query. In this paper, we propose a novel news search framework, called NEWSLINK, to empower intuitive news search by using relationship paths discovered from open Knowledge Graphs (KGs). Specifically, NEWSLINK embeds both a query and news documents to subgraphs, called subgraph embeddings, in the KG. Their embeddings' overlap induces relationship paths between the involving entities. Two major advantages are obtained by incorporating subgraph embeddings into search. First, they enrich the search context, leading to robust results. Second, the relationship paths linking entities inter and intra news documents can help users better understand and digest the results for the given query. Through both human and automatic evaluations, we verify that NEWSLINK can help users understand the result-to-query relatedness, while its search quality is robust and outperforms many established search approaches, including Apache Lucene and a KG-powered query expansion approach, as well as popular deep learning models, Sentence-BERT (SBERT) and DOC2VEC."
9306402,Temporal Attention Feature Encoding for Video Captioning,"In this paper, we propose a novel video captioning algorithm including a feature encoder (FENC) and a decoder architecture to provide more accurate and richer representation. Our network model incorporates feature temporal attention (FTA) to efficiently embed important events to a feature vector. In FTA, the proposed feature is given as the weighted fusion of the video features extracted from 3D CNN, and, therefore it allows a decoder to know when the feature is activated. In a decoder, similarly, a feature word attention (FWA) is used for weighting some elements of the encoded feature vector. The FWA determines which elements in the feature should be activated to generate the appropriate word. The training is further facilitated by a new loss function, reducing the variance of the frequencies of words. It is demonstrated with experimental results that the proposed algorithms outperforms the conventional algorithms in VATEX that is a recent large-scale dataset for long-term video sentence generation."
9065209,Attention Based R&CNN Medical Question Answering System in Chinese,"This paper proposes a Chinese medical question answering system based on CNN s with self-attention embedded model. The key idea of the proposed framework is first understanding whole sentences via LSTM, obtaining the feature map by the convolution layer of CNN, then further-comprehending of question and answer by self-attention and finally pass through to polling layer of CNN to enhance accuracy. In addition, due to the word segmentation issue in the Chinese especially in medical field, character embedding is applied to enhance the accuracy. The experimental results show that the proposed framework improves the accuracy around 80%."
8950747,Parallel Data-Local Training for Optimizing Word2Vec Embeddings for Word and Graph Embeddings,"The Word2Vec model is a neural network-based unsupervised word embedding technique widely used in applications such as natural language processing, bioinformatics and graph mining. As Word2Vec repeatedly performs Stochastic Gradient Descent (SGD) to minimize the objective function, it is very compute-intensive. However, existing methods for parallelizing Word2Vec are not optimized enough for data locality to achieve high performance. In this paper, we develop a parallel data-locality-enhanced Word2Vec algorithm based on Skip-gram with a novel negative sampling method that decouples loss calculation with positive and negative samples; this allows us to efficiently reformulate matrix-matrix operations for the negative samples over the sentence. Experimental results demonstrate our parallel implementations on multi-core CPUs and GPUs achieve significant performance improvement over the existing state-of-the-art parallel Word2Vec implementations while maintaining evaluation quality. We also show the utility of our Word2Vec implementation within the Node2Vec algorithm which accelerates embedding learning for large graphs."
8905116,Vietnamese- English Cross-Lingual Paraphrase Identification Using Siamese Recurrent Architectures,"The paraphrase identification task is one of the important problems that affect the quality of many natural language processing tasks such as querying information, text summarization, plagiarism detection, etc. Especially in the present time, with the development of machine translation tools, the task of paraphrase identification also has to be considered in the case of pairs of texts in two different languages. In this paper, we propose to use the Siamese Recurrent architectures to identify the Vietnamese- English cross-lingual paraphrase cases. In addition, we also use methods such as mapping bilingual word embedding, adding POS vector to word embedding and adjusting the POS tagging label between Vietnamese text and English text. The experimental results in the English- Vietnamese paraphrase corpus with 44,652 sentence pairs show that the use of these methods help to improve the accuracy of the model from 86.86% to 89.61%."
9374921,Sequence-to-Sequence Emotional Voice Conversion With Strength Control,"This paper proposes an improved emotional voice conversion (EVC) method with emotional strength and duration controllability. EVC methods without duration mapping generate emotional speech with identical duration to that of the neutral input speech. In reality, even the same sentences would have different speeds and rhythms depending on the emotions. To solve this, the proposed method adopts a sequence-to-sequence network with an attention module that enables the network to learn attention in the neutral input sequence should be focused on which part of the emotional output sequence. Besides, to capture the multi-attribute aspects of emotional variations, an emotion encoder is designed for transforming acoustic features into emotion embedding vectors. By aggregating the emotion embedding vectors for each emotion, a representative vector for the target emotion is obtained and weighted to reflect emotion strength. By introducing a speaker encoder, the proposed method can preserve speaker identity even after the emotion conversion. Objective and subjective evaluation results confirm that the proposed method is superior to other previous works. Especially, in emotion strength control, we achieve in getting successful results."
9362731,Comparison of Pretrained Embeddings to Identify Hate Speech in Indian Code-Mixed Text,"Two or more languages used in the same sentence is known as the code-mixed text. The phenomenon is abundant in social media due to multilingualism. It poses a considerable challenge for classic NLP tools trained on monolingual corpora. Automatic hate speech detection in code-mixed text becomes even more challenging due to non-standard variations in the spelling, grammar and writing in foreign scripts. Pre-trained models provide word embedding trained on massive monolingual corpora, which are now ubiquitous forms of word representation to classifying text. In this paper, we compare pretrained models and create an ensemble model for code-mixed data of hate speech classification task on Hindi-English data. We have also experimented with using word embedding for CNN networks and showed that XLNet performs better for hate speech detection in code-mixed text."
9729392,ParsiNorm: A Persian Toolkit for Speech Processing Normalization,"In general, speech processing models consist of a language model along with an acoustic model. Regardless of the language model's complexity and variants, three critical pre-processing steps are needed in language models: cleaning, normalization, and tokenization. Among mentioned steps, the normalization step is so essential to format unification in pure textual applications. However, for embedded language models in speech processing modules, normalization is not limited to format unification. Moreover, it has to convert each readable symbol, number, etc., to how they are pronounced. To the best of our knowledge, there is no Persian normalization toolkits for embedded language models in speech processing modules, So in this paper, we propose an open-source normalization toolkit for text processing in speech applications. Briefly, we consider different readable Persian text like symbols (common currencies,#,@,URL, etc.), numbers (date, time, phone number, national code, etc.), and so on. Comparison with other available Persian textual normalization tools indicates the superiority of the proposed method in speech processing. Also, comparing the model's performance for one of the proposed functions (sentence separation) with other common natural language libraries such as HAZM and Parsivar indicates the proper performance of the proposed method. Besides, its evaluation of some Persian Wikipedia data confirms the proper performance of the proposed method."
9793955,CLEAR: Contrastive Learning for API Recommendation,"Automatic API recommendation has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information-retrieval-based (IR-based) and neural-based methods. Although these approaches were reported having remarkable performance, our observation shows that existing approaches can fail due to the following two reasons: 1) most IR-based approaches treat task queries as bag-of-words and use word embedding to represent queries, which cannot capture the sequential semantic information. 2) both the IR-based and the neural-based approaches are weak at distinguishing the semantic difference among lexically similar queries. In this paper, we propose CLEAR, which leverages BERT sen-tence embedding and contrastive learning to tackle the above two is-sues. Specifically, CLEAR embeds the whole sentence of queries and Stack Overflow (SO) posts with a BERT-based model rather than the bag-of-word-based word embedding model, which can preserve the semantic-related sequential information. In addition, CLEAR uses contrastive learning to train the BERT-based embedding model for learning precise semantic representation of programming termi-nologies regardless of their lexical information. CLEAR also builds a BERT-based re-ranking model to optimize its recommendation results. Given a query, CLEAR first selects a set of candidate SO posts via the BERT sentence embedding-based similarity to reduce search space. CLEAR further leverages a BERT-based re-ranking model to rank candidate SO posts and recommends the APIs from the ranked top SO posts for the query. Our experiment results on three different test datasets confirm the effectiveness of CLEAR for both method-level and class-level API recommendation. Compared to the state-of-the-art API recom-mendation approaches, CLEAR improves the MAP by 25%-187% at method-level and 10%-100% at class-level."
9003979,Probing the Information Encoded in X-Vectors,"Deep neural network based speaker embeddings, such as x-vectors, have been shown to perform well in text-independent speaker recognition/verification tasks. In this paper, we use simple classifiers to investigate the contents encoded by x-vector embeddings. We probe these embeddings for information related to the speaker, channel, transcription (sentence, words, phones), and meta information about the utterance (duration and augmentation type), and compare these with the information encoded by i-vectors across a varying number of dimensions. We also study the effect of data augmentation during extractor training on the information captured by x-vectors. Experiments on the RedDots data set show that x-vectors capture spoken content and channel-related information, while performing well on speaker verification tasks."
9225328,Sentiment Analysis of Restaurant Reviews using Combined CNN-LSTM,"The combination of machine learning approach and natural language processing is applied to analyze the sentiment of text for particular sentences. In this particular area lots of work done in recent times. Restaurant business was always a popular business in Bangladesh. These business is now Leaning towards online delivery services and the overall quality of restaurants are now judged by reviews of customers. One try to understand the quality of a restaurant by the reviews from other customers. These opinions of customers organizing in structured way and to understand perception of customers reviews and reactions is the main motto of our work. Collecting data was the first thing we have done for deploying this piece of work. Then making a dataset which we harvested from websites and tried to deploy with deep learning technique. In this piece of research, a combined CNN-LSTM architecture used in our dataset and got an accuracy of 94.22%. Also used some other performance metrics to evaluate our model."
8904286,Predicting the Mumble of Wireless Channel with Sequence-to-Sequence Models,"Accurate prediction of fading channel in the upcoming transmission frame is essential to realize adaptive transmission for transmitters, and receivers with the ability of channel prediction can also save some computations of channel estimation. However, due to the rapid channel variation and channel estimation error, reliable prediction is hard to realize. In this situation, an appropriate channel model should be selected, which can cover both the statistical model and small scale fading of channel, this reminds us the natural languages, which also have statistical word frequency and specific sentences. Accordingly, in this paper, we take wireless channel model as a language model, and the time-varying channel as talking in this language, while the realistic noisy estimated channel can be compared with mumbling. Furthermore, in order to utilize as much as possible the information a channel coefficient takes, we discard the conventional two features of absolute value and phase, replacing with hundreds of features which will be learned by our channel model, to do this, we use a vocabulary to map a complex channel coefficient into an ID, which is represented by a vector of real numbers. Recurrent neural networks technique is used as its good balance between memorization and generalization, moreover, we creatively introduce sequence-to-sequence (seq2seq) models in time series channel prediction, which can translate past channel into future channel. The results show that realistic channel prediction with superior performance relative to channel estimation is attainable."
9080691,On Finding Similar Verses from the Holy Quran using Word Embeddings,"Finding semantic text similarity (STS) between two pieces of text is a well-known problem in Natural Language Processing. Its applications are nearly in every field such as plagiarism detection, finding related user queries in customer services or finding similar questions in search engines or forums like Stack Overflow, Quora and Stack exchange. If applied to any religious text, it can help to relate how similar pieces of knowledge are described in different places. This paper uses Word2Vec and Sent2Vec models to facilitate the process of knowledge extraction from a given corpus. The paper makes use of several English translations of the Holy Quran which is the most sacred book for Muslims. Sent2vec models have been trained from several translations of the book and the trained models are then subsequently utilized to study the semantic relationship between different words and sentences. The performance of the custom-built word embeddings is compared against the pre-trained embeddings provided by the Spacy library."
8819803,VQAR: Review on Information Retrieval Techniques based on Computer Vision and Natural Language Processing,"Recently Computer vision and Natural language processing paradigm contains enormous research progress in their respective areas. Despite the progress in both areas, still it remains as a challenging task for machines to extract image semantics and then communicate this extracted information with the desired users. These problems will be solved by Visual Question Answering (VQA) system by connecting both computer vision and natural language processing paradigms. In VQA, system is presented with an image and textual question related to that image. The system will generate the answer by processing on both image and textual features. Answer generated by VQA is in one word, phrase or in sentence. Various datasets are available for training and evaluating VQA system which contains real or abstract images and question-answer pairs related to the semantics available in the image. VQA is being used in many areas such as for blind and visually impaired users, robotics, art gallery and many more areas. This paper discusses VQA techniques, VQA datasets and highlights the parametric evaluation of these techniques along with generic issues in VQA system."
9417702,Emerging App Issue Identification via Online Joint Sentiment-Topic Tracing,"Millions of mobile apps are available in app stores, such as Apple's App Store and Google Play. For a mobile app, it would be increasingly challenging to stand out from the enormous competitors and become prevalent among users. Good user experience and well-designed functionalities are the keys to a successful app. To achieve this, popular apps usually schedule their updates frequently. If we can capture the critical app issues faced by users in a timely and accurate manner, developers can make timely updates, and good user experience can be ensured. There exist prior studies on analyzing reviews for detecting emerging app issues. These studies are usually based on topic modeling or clustering techniques. However, the short-length characteristics and sentiment of user reviews have not been considered. In this paper, we propose a novel emerging issue detection approach named MERIT to take into consideration the two aforementioned characteristics. Specifically, we propose an Adaptive Online Biterm Sentiment-Topic (AOBST) model for jointly modeling topics and corresponding sentiments that takes into consideration app versions. Based on the AOBST model, we infer the topics negatively reflected in user reviews for one app version, and automatically interpret the meaning of the topics with most relevant phrases and sentences. Experiments on popular apps from Google Play and Apple's App Store demonstrate the effectiveness of MERIT in identifying emerging app issues, improving the state-of-the-art method by 22.3% in terms of F1-score. In terms of efficiency, MERIT can return results within acceptable time."
9580101,Word Sense Disambiguation for Large Documents Using Neural Network Model,"Lexical ambiguity in natural language processing is a live problem and needs to be resolved. The lexical ambiguity mainly occurred because of polysemous words. Because of polysemous words, Machine fails to understand the user queries and generates ambiguous results. This research aims to rectify the ambiguous queries with corrected queries by using supervised machine learning approach. The input query is preprocessed first with sentence splitting, tokenization, lemmatization, and stemming. From the filtered output the ambiguous words are detected and context information is stored into local generated data structures. The adaptive word vector is constructed with respect to the ambiguous word and stored context information. While constructing adaptive word vector the mean-similarity and max-similarity of the ambiguous words is considered. The adaptive word vector is provided as an input to the supervised neural network model for the classification. The corrected sense values are available at the out layer. These sense values are mapped with freely available lexical dataset WordNet for getting the most correct meaning of ambiguous words. The ultimate goal of this research is to provide unambiguous communication in between human and machine."
9741993,A Cascade Binary Tagging Joint Extraction Method,"Extracting entities and relations from unstructured text has become a significant task in natural processing, especially knowledge graphs. However, the traditional relationship extraction method processes this task in a pipelined manner, i.e., identifying entity first and then recognizing their relations, and rarely consider the relevance and independence of the two subtasks. To solve the problems of error accumulation, we present a jointly learning method to a tagging problem to tackle this problem. Meanwhile, to understand the semantics of sentences, we introduce the pre-training method into our models, such as BERT. To solve the problem of entity overlap, we replace softmax with a sigmoid activation function. In the language and intelligent technology competition, experimental results show that our model outperforms the baseline model."
9538456,A Study of Word Presentation in Vietnamese Sentiment Analysis,"Sentiment analysis is a field of research that analyzes people’s attitudes, psychology, emotions about a particular issue. In recent years, deep learning techniques have yielded remarkable success in sentiment analysis. One of the paramount factors that affect the effectiveness of the deep learning model in sentiment analysis is the representation of the words in the input documents. In this paper, first, we have evaluated the effectiveness of some recent word representation in sentiment analysis for Vietnamese reviews. Second, we have proposed a new feature representing a document named BERT-POS feature. This feature has both contextual semantic and syntactic information of the sentences in a comment. Thus, it helps to improve the accuracy of the sentiment analysis problem. Third, we have conducted several extensive experiments to prove the effectiveness of our proposed feature."
9808596,Identification of Intra-Domain Ambiguity using Transformer-based Machine Learning,"Recently, the application of neural word embeddings for detecting cross-domain ambiguities in software requirements has gained a significant attention from the requirements engineering (RE) community. Several approaches have been proposed in the literature for estimating the variation of meaning of commonly used terms in different domains. A major limitation of these techniques is that they are unable to identify and detect the terms that have been used in different contexts within the same application domain, i.e. intra-domain ambiguities or in a requirements document of an interdisciplinary project. We propose an approach based on the idea of bidirectional encoder representations from Transformers (BERT) and clustering for identifying such ambiguities. For every context in which a term has been used in the document, our approach returns a list of its most similar words and also provides some example sentences from the corpus highlighting its context-specific interpretation. We apply our approach to a computer science (CS) specific corpora and a multi-domain corpora which consists of textual data from eight different application domains. Our experimental results show that this approach is very effective in identifying and detecting intra-domain ambiguities."
9548576,Arabic Semantic Textual Similarity Identification based on Convolutional Gated Recurrent Units,"The augmentation of data exchanged on the internet has favored the practice of paraphrase. Its detection is one of the fundamental tasks of Natural Language Processing (NLP). It consists of identifying the degree of semantic similarity between sentences that convey the same meaning with different words. Although many researchers focused on this task on the English language, there are few works for other languages like the Arabic. It has presented important challenges because of its richness of features and processing complexities. Nowadays, deep neural networks have yielded immense success in most NLP applications. In this paper, a Siamese architecture is proposed for Arabic paraphrase detection. It has proven its relevance for semantic textual similarity. Indeed, the effectiveness of feed forward and recurrent neural networks models are studied. Experiments are conducted on the paraphrased Open-Source Arabic Corpora (OSAC). It is generated semi-automatically and validated using the benchmark SemEval. Evaluations demonstrated that Gated Recurrent Unit (GRU) outperformed Convolutional Neural Network (CNN) and other state-of-the-art methods."
9697110,A Comprehensive Study and Detailed Review On Hate Speech Classification : A Systematic Analysis,"Hate speech is about making insults, threats, or stereotypes towards people or a group of people because of its characteristics such as origin, race, gender, religion, disabilities, and more. Modern society uses social networking websites for sharing thoughts and emotions. However, sometimes it can lead to hate speech. Hate speech is a severe issue as it may lead to repulsive outcomes. Hate speech is a critical element of social media, and there are various types of hate speech due to the ambiguity of the sentences. The traditional state-of-art methods give significant accuracy for classifying hate speech. Therefore, in recent years, the state-of art-methods have been succeeded in detecting and recognizing hate speech. In this paper, we have investigated the various research work that has been accomplished so far for hate speech classification."
9477816,Word Sense Disambiguation using KeNet,"The highly studied Natural Language Processing (NLP) problem Word Sense Disambiguation (WSD) is the process of removing the ambiguities of multiple-sense words that have the same morphological structure. The first step of WSD is to list the probable meanings of the word.The next step is identify the meaning which is used in the context within the sentence. A Turkish WordNet called KeNet is used to list the word-senses. The elimination of the ambiguity was done with BERT word embeddings by comparing with the meaning via cosine similarity. The impact of the system is evaluated by both with and without adding it to a search engine of Turkish news. Each news of topmost 10 news returned for each query is manually labeled as related or not related.Results of the labeling, relatedness of the first n documents, and ordered biased precision metrics are evaluated. Positive increment on the results is shown when the WSD modul is added on the system."
9647767,Towards Change Detection in Privacy Policies with Natural Language Processing,"Privacy policies notify users about the privacy practices of websites, mobile apps, and other products and services. However, users rarely read them and struggle to understand their contents. Due to the complicated nature of these documents, it gets even harder to understand and take note of any changes of interest or concern when the policies are changed or revised. With advances in machine learning and natural language processing, tools that can automatically annotate sentences of policies have been developed. These annotations can help a user identify and understand relevant parts of a privacy policy. In this paper, we present our attempt to further such annotations by also detecting the important changes that occurred across sentences. Using supervised machine learning models, word-embedding, similarity matching, and structural analysis of sentences, we present a process that takes two different versions of a privacy policy as input, matches the sentences of one version to another based on semantic similarity, and identifies relevant changes between two matched sentences. We present the results and insights of applying our approach on 79 privacy policies manually downloaded from Facebook, WhatsApp, Twitter, Google, LinkedIn and Snapchat, ranging between the period of 1999 to 2020."
9710042,DAE-GAN: Dynamic Aspect-aware GAN for Text-to-Image Synthesis,"Text-to-image synthesis refers to generating an image from a given text description, the key goal of which lies in photo realism and semantic consistency. Previous methods usually generate an initial image with sentence embedding and then refine it with fine-grained word embedding. Despite the significant progress, the ‘aspect’ information (e.g., red eyes) contained in the text, referring to several words rather than a word that depicts ‘a particular part or feature of something’, is often ignored, which is highly helpful for synthesizing image details. How to make better utilization of aspect information in text-to-image synthesis still remains an unresolved challenge. To address this problem, in this paper, we propose a Dynamic Aspect-awarE GAN (DAE-GAN) that represents text information comprehensively from multiple granularities, including sentence-level, word-level, and aspect-level. Moreover, inspired by human learning behaviors, we develop a novel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement (ALR) module are alternately employed. AGR utilizes word-level embedding to globally enhance the previously generated image, while ALR dynamically employs aspect-level embedding to refine image details from a local perspective. Finally, a corresponding matching loss function is designed to ensure the text-image semantic consistency at different levels. Extensive experiments on two well-studied and publicly available datasets (i.e., CUB-200 and COCO) demonstrate the superiority and rationality of our method."
9313350,A Deep Learning Knowledge Graph Approach to Drug Labelling,"Ensuring the accuracy and completeness of drug labels is a labour-intensive and potentially error prone process, as labels contain unstructured text that is not suitable for automated processing. To address this, we have developed a novel deep learning system that uses a bidirectional LSTM model to extract and structure drug information in a knowledge graph-based embedding space. This allows us to evaluate drug label consistency with ground truth knowledge, along with the ability to predict additional drug interactions. Annotated sentences from 7,117 drug labels sentences were used to train the LSTM model and 1,779 were used to test it. The drug entity extraction system was able to correctly detect relevant entities and relations with a F1 score of 91% and 81% respectively. The knowledge graph embedding model was able to identify inconsistent facts with ground truth data in 76% of the cases tested. This demonstrates that there is potential in building a natural language processing system that automatically extracts drug interaction information from drug labels and embeds this structured data into a knowledge graph embedding space to help evaluate drug label accuracy. We note that the accuracy of the system needs to be improved significantly before it can fully automate drug labeling related tasks. Rather such a system could provide best utility within a human-in-the-loop approach, where operators augment model training and evaluation."
8698340,Improving Aspect Term Extraction With Bidirectional Dependency Tree Representation,"Aspect term extraction is one of the important subtasks in aspect-based sentiment analysis. Previous studies have shown that using dependency tree structure representation is promising for this task. However, most dependency tree structures involve only one directional propagation on the dependency tree. In this paper, we first propose a novel bidirectional dependency tree network to extract dependency structure features from the given sentences. The key idea is to explicitly incorporate both representations gained separately from the bottom-up and top-down propagation on the given dependency syntactic tree. An end-to-end framework is then developed to integrate the embedded representations and BiLSTM plus CRF to learn both tree-structured and sequential features to solve the aspect term extraction problem. Experimental results demonstrate that the proposed model outperforms state-of-the-art baseline models on four benchmark SemEval datasets."
9154525,Toward Remote Sensing Image Retrieval Under a Deep Image Captioning Perspective,"The performance of remote sensing image retrieval (RSIR) systems depends on the capability of the extracted features in characterizing the semantic content of images. Existing RSIR systems describe images by visual descriptors that model the primitives (such as different land-cover classes) present in the images. However, the visual descriptors may not be sufficient to describe the high-level complex content of RS images (e.g., attributes and relationships among different land-cover classes). To address this issue, in this article, we present an RSIR system that aims at generating and exploiting textual descriptions to accurately describe the relationships between the objects and their attributes present in RS images with captions (i.e., sentences). To this end, the proposed retrieval system consists of three main steps. The first step aims to encode the image visual features and then translate the encoded features into a textual description that summarizes the content of the image with captions. This is achieved based on the combination of a convolutional neural network with a recurrent neural network. The second step aims to convert the generated textual descriptions into semantically meaningful feature vectors. This is achieved by using the recent word embedding techniques. Finally, the last step estimates the similarity between the vectors of the textual descriptions of the query image and those of the archive images, and then retrieve the most similar images to the query image. Experimental results obtained on two different datasets show that the description of the image content with captions in the framework of RSIR leads to an accurate retrieval performance."
8843891,VAA: Visual Aligning Attention Model for Remote Sensing Image Captioning,"Owing to the effectiveness in selectively focusing on regions of interest of images, the attention mechanism has been widely used in image caption task, which can provide more accurate image information for training deep sequential models. Existing attention-based models typically rely on top-down attention mechanism. While somewhat effective, attention masks in these attention-based models are queried from image features by hidden states of LSTM, rather than optimized by the objective functions. This indirectly supervised training approach cannot ensure that attention layers accurately focus on regions of interest. To address the above issue, in this paper, a novel attention model, Visual Aligning Attention model (VAA), is proposed. In this model, the attention layer is optimized by a well-designed visual aligning loss during the training phase. The visual aligning loss is obtained by explicitly calculating the feature similarity of attended image features and corresponding word embedding vectors. Besides, in order to eliminate the influence of non-visual words in training the attention layer, a visual vocab used for filtering out non-visual words in sentences is proposed, which can neglect the non-visual words when calculating the visual aligning loss. Experiments on UCM-Captions and Sydney-Captions prove that the proposed method is more effective in remote sensing image caption task."
8371531,Enhanced Robot Speech Recognition Using Biomimetic Binaural Sound Source Localization,"Inspired by the behavior of humans talking in noisy environments, we propose an embodied embedded cognition approach to improve automatic speech recognition (ASR) systems for robots in challenging environments, such as with ego noise, using binaural sound source localization (SSL). The approach is verified by measuring the impact of SSL with a humanoid robot head on the performance of an ASR system. More specifically, a robot orients itself toward the angle where the signal-to-noise ratio (SNR) of speech is maximized for one microphone before doing an ASR task. First, a spiking neural network inspired by the midbrain auditory system based on our previous work is applied to calculate the sound signal angle. Then, a feedforward neural network is used to handle high levels of ego noise and reverberation in the signal. Finally, the sound signal is fed into an ASR system. For ASR, we use a system developed by our group and compare its performance with and without the support from SSL. We test our SSL and ASR systems on two humanoid platforms with different structural and material properties. With our approach we halve the sentence error rate with respect to the common downmixing of both channels. Surprisingly, the ASR performance is more than two times better when the angle between the humanoid head and the sound source allows sound waves to be reflected most intensely from the pinna to the ear microphone, rather than when sound waves arrive perpendicularly to the membrane."
8930077,Gaze-Driven Adaptive Interventions for Magazine-Style Narrative Visualizations,"In this article, we investigate the value of gaze-driven adaptive interventions to support the processing of textual documents with embedded visualizations, i.e., Magazine Style Narrative Visualizations (MSNVs). These interventions are provided dynamically by highlighting relevant data points in the visualization when the user reads related sentences in the MSNV text, as detected by an eye-tracker. We conducted a user study during which participants read a set of MSNVs with our interventions, and compared their performance and experience with participants who received no interventions. Our work extends previous findings by showing that dynamic, gaze-driven interventions can be delivered based on reading behaviors in MSNVs, a widespread form of documents that have never been considered for gaze-driven adaptation so far. Next, we found that the interventions significantly improved the performance of users with low levels of visualization literacy, i.e., those users who need help the most due to their lower ability to process and understand data visualizations. However, high literacy users were not impacted by the interventions, providing initial evidence that gaze-driven interventions can be further improved by personalizing them to the levels of visualization literacy of their users."
9133064,An Approximate Model for Event Detection From Twitter Data,"The abundance and real-time availability of Twitter data have proved beneficial in detecting events in various domains such as emergency situations, crime detection, public health, place recommendations, etc. Nevertheless, two critical challenges occur while detecting events using social media data. First, the uncertainty in capturing the contextual relationship among tweets, which is the result of the limited availability of the contextual information due to the small length of tweets. Second, the high computation cost required in event detection due to massive data processing. Earlier research works, addressing these challenges, have tried to capture the contextual information by using the dense vector representations of texts leveraging deep neural word embedding generation models such as Word2Vec and GloVe. However, these models are trained on the Euclidean vector space which fails to amalgamate the directional information of the vectors with the semantic information in text, incurring high computational costs. To target both the problems simultaneously, we propose modeling Twitter data as a graph-of-sentences which retains the contextual relationships while maintaining lower computational cost. The proposed model captures contextual information using JoSE, a spherical vector representation leveraging the word-word and word-paragraph semantic co-occurrence statistics in a spherical generative model. Furthermore, the framework uses the weighted-graph model to capture all the relationships among the Twitter data efficiently. The graph is further pruned with the help of the graph component filtering approach. The graph clustering model, employed to detect the events, leverages the edge weights and the partial-k clustering approach maintaining low computation costs. The experimentation on the annotated benchmark Twitter data set and the real-world datasets show improved run-time performance up to 30% while maintaining the qualitative performance (F1-score) comparabl...
(Show More)"
9068713,Development of a Deep Learning-Based Brain-Computer Interface for Visual Imagery Recognition,"Electroencephalogram (EEG) signals are naturally chaotic and nonlinear. This, however, poses problems in the decoding of brain states for recognition and classification applications, particularly in brain-computer interfaces (BCIs). Recent BCI technologies are focused on the extraction of linear and nonlinear signal features. However, these systems do not necessarily capture the chaoticity of EEG signals, and hence, may affect the accuracy of classification. Furthermore, most recent EEG-based BCI systems involve nonlinear feature extraction; however, these systems require explicit and hand-selection of these features, and may therefore impose further errors. In this study, we investigate the use of deep learning techniques that involve inherent and embedded feature selection and extraction in their hidden layers, and hence, they do not require explicit and human-intervened selection of features. The performance of different deep learning models were compared to those of the traditional classifiers. Results show that deep learning-based BCI systems performed significantly better in comparison to the conventional classifiers. This BCI system can be extended in different applications, such as in recognition of imagined phonemes, words, phrases and sentences."
8852171,Character-Aware Convolutional Recurrent Networks with Self-Attention for Emotion Detection on Twitter,"Despite myriad efforts in the literature designing neural representation system for emotion detection, very few works consider constructing effective model for apperceiving various emotion intensity on social media because of the informal expression and lack of context. In this paper, we proposed a character-aware convolutional recurrent networks with self-attention for emotion detection on user-generated content. The proposed model contains three parts: the character-level convolutional layer is designed to learn word representation based on character n-grams for capturing subword information and independent on pre-trained word embedding. The recurrent neural networks learn the sequential context information used both forward and backward recurrent neural network. And the self-attention module is used to extract different emotion aspects of the sentence into multiple vector representations. The attention module performs on top of recurrent networks which enables attention to be used in special domain or task when there are no extra inputs. We evaluate the proposed model on two public emotion datasets including both emotion intensity detection and emotion classification. We compare our model with the state-of-the-art methods on these datasets and the experimental results demonstrate that the proposed model outperforms several baselines on most emotion types detection and indicates the effectiveness of the designed model. In addition, the training of the proposed model for these tasks relies exclusively on initialized character vector which can be used for morphologically rich languages with long-tailed frequency distributions or domains with dynamic vocabuaries."
9037672,Syntax-aware Transformer Encoder for Neural Machine Translation,"Syntax has been shown a helpful clue in various natural language processing tasks including previous statistical machine translation and recurrent neural network based machine translation. However, since the state-of-the-art neural machine translation (NMT) has to be built on the Transformer based encoder, few attempts are found on such a syntax enhancement. Thus in this paper, we explore effective ways to introduce syntax into Transformer for better machine translation. We empirically compare two ways, positional encoding and input embedding, to exploit syntactic clues from dependency tree over source sentence. Our proposed methods have a merit keeping the architecture of Transformer unchanged, thus the efficiency of Transformer can be kept. The experimental results on IWSLT' 14 German-to-English and WMT14 English-to-German show that our method can yield advanced results over strong Transformer baselines."
9049616,Mixed Word Representation and Minimal Bi-GRU Model for Sentiment Analysis,"In the mission of natural language processing, sentiment analysis is a formidable challenge due to the complexity of deep network architecture and the lack of standard sentiment word representation. In this paper, we proposed a new learning method of the word representation for the comprehensive information of texts and a minimal Bi-GRU (bidirectional gate recurrent unit) model for the task of sentiment classification. First, for capturing sentiment information of words, the supervised three-layer network is used for construct sentiment word representation. We propose the mixed word representation to denote the classification characteristics, which combines the word embedding of neural probabilistic language model with the proposed the sentiment word representation. Next, we propose bidirectional GRU network including forward and backward propagation to consider the semantic relations before and after sentences, meanwhile, to simple the architecture, we apply minimal GRU network. Then, we combine minimal Bi-GRU model with the mixed word representation taking a full account of semantic and sentiment information to classify the sentiment data set as Movie Reviews and IMDB data set. Experimental results demonstrate that the simplicity of the model and superiority of the performance."
8942380,Hashtag Recommender System Based on LSTM Neural Reccurent Network,"The successfulness reached by numerous neural network models for encoding word embedding has conducted approaches for encoding vector representation for sentences, paragraphs or even micro-blogs.. Meanwhile, the hashtag is a keyword that denotes the topic of a tweet. Hashtags supply significant information for several text mining tasks, for instance sentiment classification, news analysis, etc. Hence, an appropriate hashtag recommender system is needed to assist users in choosing relevant hashtags for their tweets. Therefore, we present a hashtags recommender method to encode the tweet vector-based representation by using a long short-term memory recurrent neural network. Specifically, we first learn words vector-based representation by training the Skip gram model to generate embeddings of tweets. Then we apply the density-based spatial clustering to gather similar tweets into homogeneous clusters. Subsequently, we recommend the k top highest results of tweets to the user, via calculating their co-occurrence with the nearest clusters centers to a given tweet. Our experiments on a real data set show appropriate results."
9037717,combination of Semantic Relatedness with Supervised Method for Word Sense Disambiguation,"We present a semi-supervised learning method at efficiently exploits semantic relatedness in order to incorporate sense knowledge into a word sense disambiguation model and to leverage system performance. We have presented sense relativeness algorithms which combine neural model learned from a generic embedding function for variable length contexts of target words on a POS-labeled text corpus, with sense-labeled data in the form of example sentences. This paper investigates the way of incorporating semantic relatedness in a word sense disambiguation setting and evaluates the method on some SensEval/SemEval lexical sample tasks. The obtained results show that such representations consistently improve the accuracy of the selective supervised WSD system."
9363108,Multi-Stream Semantics-Guided Dynamic Aggregation Graph Convolution Networks to Extract Overlapping Relations,"The existing relation extraction approaches select the relevant partial dependency structures and exhibit the limitation associated with long-distance dependencies. Moreover, due to the repeated use of the irrelevant redundant information and the lack of consideration of the key semantic details, the extraction of relations is relatively complex when the entities overlap. To address this limitation and effectively exploit the relevant information while ignoring irrelevant information, this paper proposes a simple but effective multistream semantics-guided dynamic aggregation graph convolution network (SG-DAGCN) to realize the extraction of overlapping relations. The proposed model constructs the entity relation graphs by enumerating the possible candidates and external auxiliary information and adaptively manages the relevant substructure. Subsequently, this framework models the relational graphs between the entities through a dynamic aggregation graph convolution module and gradually produces the discriminative embedded features and a refined graph through the dynamic aggregation of nodes. The proposed approach can effectively leverage the rich multiscale structural information and capture the long-distance dependencies between overlapping entities in long sentences. The results of the experiments conducted on two typical benchmark datasets show that the proposed model can achieve a high level of performance and outperform other state-of-the-art methods in both qualitative and quantitative aspects."
8876975,Identifying sound descriptions in Written Documents,"Automatically identifying sound descriptions can be useful to extract soundscapes from written documents. In this respect, we present a preliminary work on the identification of sound descriptions which first expands a list of sound terms taken from a machine-readable lexical resource. Then, to find out the producer of each sound mentioned in a text collection, two methods have been tested out: one which relies on dependency parsing and the other which uses a pre-trained word embedding model. Both methods have been evaluated on a set of sentences extracted from three 19th century French novels; the obtained results suggest that dependency parsing performs better on this specific task. It should be noted that the first step of the proposed system is domain-independent and can generate a domain-specific lexicon for any topic."
9640790,Analysis on Sentiment Analytics Using Deep Learning Techniques,"Sentiment analytics is the process of applying natural language processing and methods for text-based information to define and extract subjective knowledge of the text. Natural language processing and text classifications can deal with limited corpus data and more attention has been gained by semantic texts and word embedding methods. Deep learning is a powerful method that learns different layers of representations or qualities of information and produces state-of-the-art prediction results. In different applications of sentiment analytics, deep learning methods are used at the sentence, document, and aspect levels. This review paper is based on the main difficulties in the sentiment assessment stage that significantly affect sentiment score, pooling, and polarity detection. The most popular deep learning methods are a Convolution Neural Network and Recurrent Neural Network. Finally, a comparative study is made with a vast literature survey using deep learning models."
9523106,Using Text to Teach Image Retrieval,"Image retrieval relies heavily on the quality of the data modeling and the distance measurement in the feature space. Building on the concept of image manifold, we first propose to represent the feature space of images, learned via neural networks, as a graph. Neighborhoods in the feature space are now defined by the geodesic distance between images, represented as graph vertices or manifold samples. When limited images are available, this manifold is sparsely sampled, making the geodesic computation and the corresponding retrieval harder. To address this, we augment the manifold samples with geometrically aligned text, thereby using a plethora of sentences to teach us about images. In addition to extensive results on standard datasets illustrating the power of text to help in image retrieval, a new public dataset based on CLEVR is introduced to quantify the semantic similarity between visual data and text data. The experimental results show that the joint embedding manifold is a robust representation, allowing it to be a better basis to perform image retrieval given only an image and a textual instruction on the desired modifications over the image."
9693527,Adversarial Machine Learning in Text Processing: A Literature Survey,"Machine learning algorithms represent the intelligence that controls many information systems and applications around us. As such, they are targeted by attackers to impact their decisions. Text created by machine learning algorithms has many types of applications, some of which can be considered malicious especially if there is an intention to present machine-generated text as human-generated. In this paper, we surveyed major subjects in adversarial machine learning for text processing applications. Unlike adversarial machine learning in images, text problems and applications are heterogeneous. Thus, each problem can have its own challenges. We focused on some of the evolving research areas such as: malicious versus genuine text generation metrics, defense against adversarial attacks, and text generation models and algorithms. Our study showed that as applications of text generation will continue to grow in the near future, the type and nature of attacks on those applications and their machine learning algorithms will continue to grow as well. Literature survey indicated an increasing trend in using pre-trained models in machine learning. Word/sentence embedding models and transformers are examples of those pre-trained models. Adversarial models may utilize same or similar pre-trained models as well. In another trend related to text generation models, literature showed effort to develop universal text perturbations to be used in both black-and white-box attack settings. Literature showed also using conditional GANs to create latent representation for writing types. This usage will allow for a seamless lexical and grammatical transition between various writing styles. In text generation metrics, research trends showed developing successful automated or semi-automated assessment metrics that may include human judgement. Literature showed also research trends of designing and developing new memory models that increase performance and memory utilization efficiency witho...
(Show More)"
9679919,Modelling Context with Graph Convolutional Networks for Aspect-based Sentiment Analysis,"Aspect-based sentiment analysis is a fine-grained natural language processing task that aims to predict a specific target's sentiment polarity in its context. Existing researches mainly focus on the exploration of the interaction between the sentiment polarity of aspects and contexts. Models based on the self-attention mechanism can fully explore the syntactic structure of sentences. In contrast, models based on a convolutional neural network have the ability to make aspects and the semantics of contextual words alignment. These methods all have some limitations; that is, they lack the ability to make full use of syntactic information and long-range word dependencies to carry out relevant syntactic constraints while associating the target’s sentiment with the local context. And they are not able to handle affective ambivalence in text. In this paper, we propose a stacked ensemble method for predicting the sentiment polarity by combining a local context embedding and a global graph convolutional network. It uses a Graph Convolutional Network (GCN) to supplement local information to improve the accuracy of the aspect sentiment classifier with revealing multi-level sentiments. Experimental results on three commonly used datasets show that our approach outperforms the state-of-the-art models in the Semeval-2014 dataset."
9831788,Language Model Guided Knowledge Graph Embeddings,"Knowledge graph embedding models have become a popular approach for knowledge graph completion through predicting the plausibility of (potential) triples. This is performed by transforming the entities and relations of the knowledge graph into an embedding space. However, knowledge graphs often include further textual information stored in literal, which is ignored by such embedding models. As a consequence, the learning process stays limited to the structure and the connections between the entities, which has the potential to negatively influence the performance. We bridge this gap by leveraging the capabilities of pre-trained language models to include textual knowledge in the learning process of embedding models. This is achieved by introducing a new loss function that guides embedding models in measuring the likelihood of triples by taking such complementary knowledge into consideration. The proposed solution is a model-independent loss function that can be plugged into any knowledge graph embedding model. In this paper, Sentence-BERT and fastText are used as pre-trained language models from which the embeddings of the textual knowledge are obtained and injected into the loss function. The loss function contains a trainable slack variable that determines the degree to which the language models influence the plausibility of triples. Our experimental evaluation on six benchmarks, namely Nations, UMLS, WordNet, and three versions of CodEx confirms the advantage of using pre-trained language models for boosting the accuracy of knowledge graph embedding models. We showcase this by performing evaluations on top of the five well-known knowledge graph embedding models such as TransE, RotatE, ComplEx, DistMult, and QuatE. The results show an improvement in accuracy up to 9% on UMLS dataset for the Distmult model and 4.2% on the Nations dataset for the ComplEx model when they are guided by pre-trained language models. We additionally studied the effect of multiple factors...
(Show More)"
8594635,Collaborative Learning for Answer Selection in Question Answering,"Answer selection is an essential step in a question answering (QA) system. Traditional methods for this task mainly focus on developing linguistic features that are limited in practice. With the great success of deep learning method in distributed text representation, deep learning-based answer selection approaches have been well investigated, which mainly employ only one neural network, i.e., convolutional neural network (CNN) or long short term memory (LSTM), leading to failures in extracting some rich sentence features. Thus, in this paper, we propose a collaborative learning-based answer selection model (QA-CL), where we deploy a parallel training architecture to collaboratively learn the initial word vector matrix of the sentence by CNN and bidirectional LSTM (BiLSTM) at the same time. In addition, we extend our model by incorporating the sentence embedding generated by the QA-CL model into a joint distributed sentence representation using a strong unsupervised baseline weight removal (WR), i.e., the QA-CLWR model. We evaluate our proposals on a popular QA dataset, InsuranceQA. The experimental results indicate that our proposed answer selection methods can produce a better performance compared with several strong baselines. Finally, we investigate the models’ performance with respect to different question types and find that question types with a medium number of questions have a better and more stable performance than those types with too large or too small number of questions."
9103040,Lexicon-Based Sentiment Convolutional Neural Networks for Online Review Analysis,"With the growing availability and popularity of sentiment-rich resources like blogs and online reviews, new opportunities and challenges have emerged regarding the identification, extraction, and organization of sentiments from user-generated documents or sentences. Recently, many studies have exploited lexicon-based methods or supervised learning algorithms to separately conduct sentiment analysis tasks; however, the former approaches ignore contextual information of sentences and the latter ones do not take sentiment information embedded in sentiment words into consideration. To tackle these limitations, we propose a new model named Sentiment Convolutional Neural Network (SentiCNN) to analyze the sentiments of sentences with both contextual and sentiment information of sentiment words, in which, contextual information is captured from word embeddings and sentiment information is identified using existing lexicons. We incorporate a Highway Network into our model to adaptively combine sentiment and contextual information from sentences by strengthening the connection between features of both sentences and their sentiment words. Furthermore, we propose three lexicon-based attention mechanisms (LBAMs) for our SentiCNN model to find the most important indicators of sentiments and make predictions more effectively. Experiments over two well-known datasets indicate that sentiment words, the Highway Network, and LBAMs contribute to sentiment analysis."
9679873,Transformer-based Hierarchical Encoder for Document Classification,"Document Classification has a wide range of applications in various domains like Ontology Mapping, Sentiment Analysis, Topic Categorization and Document Clustering, to mention a few. Unlike Text Classification, Document Classification works with longer sequences that typically contain multiple paragraphs. Previous approaches for this task have achieved promising results, but have often relied on complex recurrence mechanisms that are expensive and time-consuming in nature. Recently, self-attention based models like Transformers and BERT have achieved state-of-the-art performance on several Natural Language Understanding (NLU) tasks, but owing to the quadratic computational complexity of the self-attention mechanism with respect to the input sequence length, these approaches are generally applied to shorter text sequences. In this paper, we address this issue, by proposing a new Transformer-based Hierarchical Encoder approach for the Document Classification task. The hierarchical framework we adopt helps us extend the self-attention mechanism to long-form text modelling thereby reducing the complexity considerably. We use the Bidirectional Transformer Encoder (BTE) at the sentence-level to generate a fixed-size sentence embedding for each sentence in the document. A document-level Transformer Encoder is then used to model the global document context and learn the inter-sentence dependencies. We also carry out experiments with the BTE in a feature-extraction and a fine-tuning setup, allowing us to evaluate the trade-off between computation power and accuracy. Furthermore, we also conduct ablation experiments, and evaluate the impact of different pre-training strategies on the overall performance. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on two standard benchmark datasets."
9768661,Automated Radiographic Report Generation Purely On Transformer: A Multi-criteria Supervised Approach,"Automated radiographic report generation is challenging in at least two aspects. First, medical images are very similar to each other and the visual differences of clinic importance are often fine-grained. Second, the disease-related words may be submerged by many similar sentences describing the common content of the images, causing the abnormal to be misinterpreted as the normal in the worst case. To tackle these challenges, this paper proposes a pure transformer-based framework to jointly enforce better visual-textual alignment, multi-label diagnostic classification, and word importance weighting, to facilitate report generation. To the best of our knowledge, this is the first pure transformer-based framework for medical report generation, which enjoys the capacity of transformer in learning long range dependencies for both image regions and sentence words. Specifically, for the first challenge, we design a novel mechanism to embed an auxiliary image-text matching objective into the transformer’s encoder-decoder structure, so that better correlated image and text features could be learned to help a report to discriminate similar images. For the second challenge, we integrate an additional multi-label classification task into our framework to guide the model in making correct diagnostic predictions. Also, a term-weighting scheme is proposed to reflect the importance of words for training so that our model would not miss key discriminative information. Our work achieves promising performance over the state-of-the-arts on two benchmark datasets, including the largest dataset MIMIC-CXR."
9791116,Context-Aware Attentive Multilevel Feature Fusion for Named Entity Recognition,"In the era of information explosion, named entity recognition (NER) has attracted widespread attention in the field of natural language processing, as it is fundamental to information extraction. Recently, methods of NER based on representation learning, e.g., character embedding and word embedding, have demonstrated promising recognition results. However, existing models only consider partial features derived from words or characters while failing to integrate semantic and syntactic information, e.g., capitalization, inter-word relations, keywords, and lexical phrases, from multilevel perspectives. Intuitively, multilevel features can be helpful when recognizing named entities from complex sentences. In this study, we propose a novel attentive multilevel feature fusion (AMFF) model for NER, which captures the multilevel features in the current context from various perspectives. It consists of four components to, respectively, capture the local character-level (CL), global character-level (CG), local word-level (WL), and global word-level (WG) features in the current context. In addition, we further define document-level features crafted from other sentences to enhance the representation learning of the current context. To this end, we introduce a novel context-aware attentive multilevel feature fusion (CAMFF) model based on AMFF, to fully leverage document-level features from all the previous inputs. The obtained multilevel features are then fused and fed into a bidirectional long short-term memory (BiLSTM)-conditional random field (CRF) network for the final sequence labeling. Extensive experiments on four benchmark datasets demonstrate that our proposed AMFF and CAMFF models outperform a set of state-of-the-art baseline methods and the features learned from multiple levels are complementary."
9679914,Unsupervised graph-clustering learning framework for financial news summarization,"Financial news shows significant influence on the inflection point of stock market. To condense the news texts with exponential growth, Automatic Text Summarization(ATS) becomes urgent. However, ATS tailored for financial news summarization has not been explored. In this paper, we propose a Graph-Clustering framework(FinGC) to extract financial news summarization. Our framework jointly learns the graph embedding and performs clustering in an unsupervised way. By combining graph structure which contains cross-sentence relations into text embedding, FinGC enriches the representation of financial news and highlights important sentences that point to specific events. Clustering is incorporated to group the financial news in terms of diversity and non-redundancy. We evaluated our best FinGC model on Fin-News and standard Multi-News. Experiments demonstrate it achieves state-of-the-art performance on standard datasets by ROUGE scores. Finally, we validated our results with human evaluation and show that our model achieves human performance on financial news."
9172835,Two-stage encoding Extractive Summarization,"Pre-trained language model can express the semantics of word or text span, is widely applied in many NLP tasks, and text summarization is no exception. It is created using fine-tuning or feature-based method on pre-training model. Since Bidirectional Encoder Representations from Transformers (BERT; Devlin et al. 2019), many works model text summarization based on BERT, and fine tune all the parameters end-to-end. Notably, multiple research proposed different strategies to create enhanced versions of BERT further, which achieve the state-of-the-art performance in many NLP tasks. In this paper, we explore the potential of multiple versions of BERT to handle text summarization. We present a two-stage encoder model (TSEM) for extractive summarization. The first stage applies A Lite BERT (ALBERT; Lan et al. 2019) to secure sentence-level embedding, identify valuable content based on A Lite BERT (ALBERT; Lan et al. 2019). The second stage proposes a new strategy to fine-tune BERT deriving meaningful document embedding, then select the best-matched combination of important sentences with source document to compose summarization. Experimental result on the CNN/Daily Mail dataset demonstrates that our model is competitive with the state-of-the-art result."
9428547,Named Entity Aware Transfer Learning for Biomedical Factoid Question Answering,"Biomedical factoid question answering is an important task in biomedical question answering application. It has attracted much attention because of its reliability of the answer. In question answering system, better representation of word is of much importance and a proper word embedding usually can improve the performance of system significantly. With the success of pre-trained models in general natural language process tasks, pretrained model has been widely used in biomedical area as well and a lot of pretrained model based approaches have been proven effective in biomedical question answering task. Besides the proper word embedding, name entity is also important information for biomedical question answering. Inspired by the concept of transfer learning, in this research we developed a mechanism to finetune BioBERT with name entity dataset to improve the question answering performance. Furthermore, we also apply BiLSTM to encode the question text to obtain sentence level information. To better combine the question level and token level information, we use bagging to further improve the overall performance. The proposed framework has been evaluated on BioASQ 6b and 7b datasets and the results have shown its promising potential."
9663570,Knowledge Guided Attention and Graph Convolutional Networks for Chemical-Disease Relation Extraction,"The automatic extraction of the chemical-disease relation (CDR) from the text becomes critical because it takes a lot of time and effort to extract valuable CDR manually. Studies have shown that prior knowledge from the biomedical knowledge base is important for relation extraction. The method of combining deep learning models with prior knowledge is worthy of our study. In this paper, we propose a new model called Knowledge Guided Attention and Graph Convolutional Networks (KGAGN) for CDR extraction. First, to make full advantage of domain knowledge, we train entity embedding as a feature representation of input sequence, and relation embedding to capture weighted contextual information further through the attention mechanism. Then, to make full advantage of syntactic dependency information in cross-sentence CDR extraction, we construct document-level syntactic dependency graphs and encode them using a graph convolution network (GCN). Finally, the chemical-induced disease (CID) relation is extracted by using weighted context features and long-range dependency features both of which contain additional knowledge information We evaluated our model on the CDR dataset published by the BioCreative-V community and achieves an F1-score of 73.3%, surpassing other state-of-the-art methods. the code implemented by PyTorch 1.7.0 deep learning library can be downloaded from Github: https://github.com/sunyi123/cdr."
9194535,Dynamic Relation Extraction with A Learnable Temporal Encoding Method,"The need to judge the relations between two entities at a specific time arises in many natural language understanding and knowledge graph related tasks, where the traditional relation extraction (RE) task without considering specific time is not feasible. Therefore, it is an important task to extract the dynamic relation from sentences containing the two entities. However, existing studies focus on extracting the static relation while ignoring temporal information in sentences or encode temporal information as a sequence to infer the relation. Considering these limitations of existing studies, we propose a Learnable Temporal Encoding (LTE) model, which encodes explicit temporal information in sentences. Specifically, we introduce a key-value memory network in LTE to identify the relation between an entity pair at a specific time. Through experiments on a general temporal relation extraction dataset, we show that the proposed model outperforms other state-of-the-art baselines, which demonstrate the effectiveness of LTE for dynamic relation extraction. We also conduct visual analysis to demonstrate that our model can fully represent the temporal information in the embedding space for any time spots."
9806393,Word2Pix: Word to Pixel Cross-Attention Transformer in Visual Grounding,"Current one-stage methods for visual grounding encode the language query as one holistic sentence embedding before fusion with visual features for target localization. Such a formulation provides insufficient ability to model query at the word level, and therefore is prone to neglect words that may not be the most important ones for a sentence but are critical for the referred object. In this article, we propose Word2Pix: a one-stage visual grounding network based on the encoder–decoder transformer architecture that enables learning for textual to visual feature correspondence via word to pixel attention. Each word from the query sentence is given an equal opportunity when attending to visual pixels through multiple stacks of transformer decoder layers. In this way, the decoder can learn to model the language query and fuse language with the visual features for target prediction simultaneously. We conduct the experiments on RefCOCO, RefCOCO
+
, and RefCOCOg datasets, and the proposed Word2Pix outperforms the existing one-stage methods by a notable margin. The results obtained also show that Word2Pix surpasses the two-stage visual grounding models, while at the same time keeping the merits of the one-stage paradigm, namely, end-to-end training and fast inference speed. Code is available at https://github.com/azurerain7/Word2Pix."
8919370,An Approach of Rhetorical Status Recognition for Judgments in Court Documents using Deep Learning Models,"In a court document, the rhetorical status of a sentence conveys the intention of the sentence, whether is is a claim or contains supporting evidences, thus, is beneficial to court document processing systems, for example, court document retrieval systems. Besides, rhetorical structure analysis has high-impact applications in natural language processing, for instances, text summarization, sentiment analysis, question answering. The output structures of the analysis contain high-level relationship between clauses and so provides valuable information. Despite of a wide range of applications and the necessity for automatic court document processing, automatic rhetorical structure analysis has not been well noticed in the legal domain. We propose to use deep learning models for tackling the task of recognizing the rhetorical status of each sentence in a court document. Deep learning has been shown effective towards natural language processing tasks including discourse analysis. We have achieved promising results for the task, which suggests the applicability of artificial neural module embedding rhetorical information for other tasks, for example, summarization and information retrieval."
9784948,Memorial GAN With Joint Semantic Optimization for Unpaired Image Captioning,"Most works of image captioning are implemented under the full supervision of paired image-caption data. Limited to expensive cost of data collection, the task of unpaired image captioning has attracted researchers' attention. In this article, we propose a novel memorial GAN (MemGAN) with the joint semantic optimization for unpaired image captioning. The core idea is to explore implicit semantic correlation between disjointed images and sentences through building a multimodal semantic-aware space (SAS). Concretely, each modality is mapped into a unified multimodal SAS, where SAS includes the semantic vectors of image I, visual concepts O, unpaired sentence S, and the generated caption C. We adopt the memory unit based on multihead attention and relational gate as a backbone to preserve and transit crucial multimodal semantics in the SAS for image caption generation and sentence reconstruction. Then, the memory unit is embedded into a GAN framework to exploit the semantic similarity and relevance in SAS, that is, imposing a joint semantic-aware optimization on SAS without supervision clues. To summarize, the proposed MemGAN learns the latent semantic relevance of SAS's multimodalities in an adversarial manner. Extensive experiments and qualitative results demonstrate the effectiveness of MemGAN, achieving improvements over state of the arts on unpaired image captioning benchmarks."
9433173,Encoder–Decoder Couplet Generation Model Based on ‘Trapezoidal Context’ Character Vector,"This paper studies the couplet generation model which automatically generates the second line of a couplet by giving the first line. Unlike other sequence generation problems, couplet generation not only considers the sequential context within a sentence line but also emphasizes the relationships between the corresponding words of first and second lines. Therefore, a trapezoidal context character embedding the vector model has been developed firstly, which considers the ‘sequence context’ and the ‘corresponding word context’ simultaneously. Afterwards, we chose the typical encoder–decoder framework to solve the sequence–sequence problems, of which the encoder and decoder are used by bi-directional GRU and GRU, respectively. In order to further increase the semantic consistency of the first and second lines of couplets, the pre-trained sentence vector of the first line is added to the attention mechanism in the model. To verify the effectiveness of the method, it is applied to the real data set. Experimental results show that our proposed model can compete with the up-to-date methods, and both adding sentence vectors to attention and using trapezoidal context character vectors can improve the effectiveness of the algorithm."
9755329,Document-level Event Argument Extraction via Multispan Semantic Reinforcement,"The challenge of document-level event argument extraction (DEAE) is to extract scattered event information beyond individual sentences, which generally requires a deeper semantic understanding of a document. This paper proposes a DEAE approach that incorporates multispan semantic reinforcement, which can better obtain complete event information by capturing and integrating the contextual semantics from the word span, the sentence span and the paragraph span in the document. Specifically, to extract the word-span contextual semantics optimally, we present the word-span semantic reinforced entity (WSRE) embedding approach and investigate the effects of its fusion ratio on the DEAE model. We also creatively extract the sentence- and the paragraph-span contextual semantics with a contextual attention mechanism and a gated attention mechanism. Finally, we use the sequence annotation strategy to extract the document-level event argument and match its role type simultaneously. We performed extensive experiments with the MUC-4 event extraction dataset, and experimental results with comprehensive analyses show that the proposed model outperforms other existing methods."
9152608,Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning,"Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word ""meaning"" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks.Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the ""meaning"" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another.An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model."
9009000,Learning to Assemble Neural Module Tree Networks for Visual Grounding,"Visual grounding, a task to ground (i.e., localize) natural language in images, essentially requires composite visual reasoning. However, existing methods over-simplify the composite nature of language into a monolithic sentence embedding or a coarse composition of subject-predicate-object triplet. In this paper, we propose to ground natural language in an intuitive, explainable, and composite fashion as it should be. In particular, we develop a novel modular network called Neural Module Tree network (NMTree) that regularizes the visual grounding along the dependency parsing tree of the sentence, where each node is a neural module that calculates visual attention according to its linguistic feature, and the grounding score is accumulated in a bottom-up direction where as needed. NMTree disentangles the visual grounding from the composite reasoning, allowing the former to only focus on primitive and easy-to-generalize patterns. To reduce the impact of parsing errors, we train the modules and their assembly end-to-end by using the Gumbel-Softmax approximation and its straight-through gradient estimator, accounting for the discrete nature of module assembly. Overall, the proposed NMTree consistently outperforms the state-of-the-arts on several benchmarks. Qualitative results show explainable grounding score calculation in great detail."
8895969,Aspect Based Sentiment Analysis With Feature Enhanced Attention CNN-BiLSTM,"Previous work has recognized the importance of using the attention mechanism to obtain the interaction between aspect words and contexts for sentiment analysis. However, for the most attention mechanisms, it is unrigorous to use the average vector of the aspect words to calculate the context attention. Besides, the feature extraction ability of the model is also essential for effective analysis, the combination of CNN and LSTM can enhance the feature extraction ability and semantic expression ability of the model, which is also a popular research trend. This paper introduces an aspect level neural network for sentiment analysis named Feature Enhanced Attention CNN-BiLSTM (FEA-NN). Our method is to extract a higher-level phrase representation sequence from the embedding layer by using CNN, which provides effective support for subsequent coding tasks. In order to improve the quality of context encoding and preserve semantic information, we use BiLSTM to capture both local features of phrases as well as global and temporal sentence semantics. Besides, we add an attention mechanism to model interaction relationships between aspect words and sentences to focus on those keywords of targets to learn more effective context representation. We evaluate the proposed model on three datasets: Restaurant, Laptop, and Twitter. Extensive experiments show that the effectivess of FEA-NN."
9378482,A Comparative Study of Deep Learning based Named Entity Recognition Algorithms for Cybersecurity,"Named Entity Recognition (NER) is important in the cybersecurity domain. It helps researchers extract cyber threat information from unstructured text sources. The extracted cyber-entities or key expressions can be used to model a cyber-attack described in an open-source text. A large number of general-purpose NER algorithms have been published that work well in text analysis. These algorithms do not perform well when applied to the cybersecurity domain. In the field of cybersecurity, the open-source text available varies greatly in complexity and under-lying structure of the sentences. General-purpose NER algorithms can misrepresent domain-specific words, such as ""malicious"" and ""javascript"". In this paper, we compare the recent deep learning-based NER algorithms on a cybersecurity dataset. We created a cybersecurity dataset collected from various sources, including ""Microsoft Security Bulletin"" and ""Adobe Security Updates"". Some of these approaches proposed in literature were not used for Cybersecurity. Others are innovations proposed by us. This comparative study helps us identify the NER algorithms that are robust and can work well in sentences taken from a large number of cybersecurity sources. We tabulate their performance on the test set and identify the best NER algorithm for a cybersecurity corpus. We also discuss the different embedding strategies that aid in the process of NER for the chosen deep learning algorithms."
9642146,Multidomain Supervised Aspect-based Sentiment Analysis using CNN_Bidirectional LSTM model,"Sentiment analysis or opinion mining used to capture the community’s attitude who have experienced the specific service/product. Sentiment analysis usually concentrates to classify the opinion of whole document or sentence. However, in most comments, users often express their opinions on different aspects of the mentioned entity rather than express general sentiments on entire document. In this case, using aspect-based sentiment analysis (ABSA) is a solution. ABSA emphases on extracting and synthesizing sentiments on particular aspects of entities in opinion text. The previous studies have difficulty working with aspect extraction and sentiment polarity classification in multiple domains of review. We offer an innovative deep learning approach with the integrated construction of bidirectional Long Short Term Memory (BiLSTM) and Convolutional Neural Network (CNN) for multidomain ABSA in this article. Our system finished the following tasks: domain classification, aspect extraction and opinion determination of aspect in the document. Besides applying GloVe word embedding for input sentences from mixed Laptop_Restaurant domain of the SemEval 2016 dataset, we also use the additional layer of POS to pick out the word morphological attributes before feeding to the CNN_BiLSTM architecture to enhance the flexibility and precision of our suggested model. Through experiment, we found that our proposed model has performed the above mentioned tasks of domain classification, aspect and sentiment extraction concurrently on a mixed domain dataset and achieved the positive results compared to previous models that were performed only on separated domain dataset."
8958759,Uncovering Flaming Events on News Media in Social Media,"Social networking sites (SNSs) facilitate the sharing of ideas and information through different types of feedback including publishing posts, leaving comments and other type of reactions. However, some comments or feedback on SNSs are inconsiderate and offensive, and sometimes this type of feedback has a very negative effect on a target user. The phenomenon known as flaming goes hand-in-hand with this type of posting that can trigger almost instantly on SNSs. Most popular users such as celebrities, politicians and news media are the major victims of the flaming behaviors and so detecting these types of events will be useful and appreciated. Flaming event can be monitored and identified by analyzing negative comments received on a post. Thus, our main objective of this study is to identify a way to detect flaming events in SNS using a sentiment prediction method. We use a deep Neural Network (NN) model that can identity sentiments of variable length sentences and classifies the sentiment of SNSs content (both comments and posts) to discover flaming events. Our deep NN model uses Word 2Vec and FastText word embedding methods as its training to explore which method is the most appropriate. The labeled dataset for training the deep NN is generated using an enhanced lexicon based approach. Our deep NN model classifies the sentiment of a sentence into five classes: Very Positive, Positive, Neutral, Negative and Very Negative. To detect flaming incidents, we focus only on the comments classified into the Negative and Very Negative classes. As a use-case, we try to explore the flaming phenomena in the news media domain and therefore we focused on news items posted by three popular news media on Facebook (BBCNews, CNN and FoxNews) to train and test the model. The experimental results show that flaming events can be detected with our proposed approach, and we explored main characteristics that trigger a flaming event and topics discussed in the flaming posts.
(Show More)"
9392525,Application of Knowledge-oriented Convolutional Neural Network For Causal Relation Extraction In South China Sea Conflict Issues,"Online news articles are an important source of information for decisions makers to understand the causal relation of events that happened. However, understanding the causality of an event or between events by traditional machine learning-based techniques from natural language text is a challenging task due to the complexity of the language to be comprehended by the machines. In this study, the Knowledge-oriented convolutional neural network (K-CNN) technique is used to extract the causal relation from online news articles related to the South China Sea (SCS) dispute. The proposed K-CNN model contains a Knowledge-oriented channel that can capture the causal phrases of causal relationships. A Data-oriented channel that captures the position information was added to the K-CNN model in this phase. The online news articles were collected from the national news agency and then the sentences which contain relation such as causal, message-topic, and product-producer were extracted. Then, the extracted sentences were annotated and converted into lower form and base form followed by transformed into the vector by looking up the word embedding table. A word filter that contains causal keywords was generated and a K-CNN model was developed, trained, and tested using the collected data. Finally, different architectures of the K-CNN model were compared to find out the most suitable architecture for this study. From the study, it was found out that the most suitable architecture was the K-CNN model with a Knowledge-oriented channel and a Data-oriented channel with average pooling. This shows that the linguistic clues and the position features can improve the performance in extracting the causal relation from the SCS online news articles. Keywords-component; Convolutional Neural Network, Causal Relation Extraction, South China Sea."
9832599,TypeFormer: Multi-Scale Transformer with Type Controller for Remote Sensing Image Caption,"Image captioning in remote sensing can help us understandthe inner attributes of the objects and the outer relations between different objects. However, the existing image captioning algorithms lack the ability of global representation, and cannot obtain object relations over long distances. In addition, these algorithmics generate captions randomly without consideration of the specific demands. To this end, we propose a pure transformer architecture with caption type controller for remote sensing image captioning. Specifically, a multi-scale vision transformer is adopted for the image representation, where the global and detailed content can be captured with multi-head self-attention layers. A transformer decoder is then introduced to successively translate the image features into comprehensive sentences. The optional block called the caption type controller is designed to consider the types of captions through caption type matrix sets according to the demands, embedding the learnable sentence feature with the required type. The comparison and ablation experiments conducted on the Remote Sensing Image Captioning Dataset (RSICD) dataset demonstrate that the proposed framework outperforms the current state-of-the-art image captioning methods. The experiments conducted on the FloodNet caption dataset further illustrate that the proposed methods can effectively generate specific types of captions."
8851948,Hierarchical Intention Enhanced Network for Automatic Dialogue Coherence Assessment,"Dialogue coherence across multiple turns is still an open challenge. The entity grid model is arguably the most popular approach for coherence modeling. However, it heavily relies on the distribution of entities across adjacent sentences but ignores the emotional context embedded in non-entity text and fails to model long dependencies between speech intentions. These limitations become even more severe when applied to dialogue domain since sentences in dialogue are short, informal and colloquial, thereby, less entities could be extracted and less coherence information could be expressed in these grids. To address the limitations of entity gird methods and incorporate the structure knowledge of dialogue, we propose a new neural network architecture, Hierarchical Intention Enhance Network, to integrate semantic context and speech intention in both utterance and dialogue levels to hierarchically model the global coherence without any entity grids. Our proposed model outperforms the state-of-the-art entity-grid based coherence model on text discrimination task by 17.13% increase in accuracy, confirming the effectiveness of our hierarchical modeling in dialogue context and the crucial importance of intention information in dialogue coherence assessment."
9625008,Ensemble Multi-Channel Neural Networks for Scientific Language Editing Evaluation,"A huge and growing number of scientific papers are authored by non-native English speakers, driving increased demand for effective computer-based writing tools to help writers composing scientific articles. The Automated Evaluation of Scientific Writing (AESW) shared task promotes the use of natural language processing tools to improve the quality of scientific writing in English by predicting whether a given sentence needs language editing or not. In this study, we propose an Ensemble Multi-Channel Neural Networks (called EMC-NN) model for scientific language editing evaluation, comprised of three main parts: a multi-channel word embedding representation, a combination of Bidirectional Long Short-Term Memory and Convolutional Neural Networks, and a majority voting ensemble. Experimental results on 143,804 testing sentences show that our proposed EMC-NN achieved an F1-score of 0.6367, outperforming the winner of the AESW-2016 competition task and the recent BERT transformers. Based on a series of in- depth analyses comparing the number of channels, ensemble size and network architectures, the proposed EMC-NN model is a relatively simple, but effective approach that offers significant performance improvements for scientific writing evaluation tasks."
9747087,Type-Aware Medical Visual Question Answering,"Medical Visual Question Answering (Med-VQA) helps answer medical questions raised by patients automatically so as to relieve the shortage of experienced doctors. Cross-modal feature alignment is a major challenge of Med-VQA. Moreover, it is critical to exploit sufficient semantic features with the consideration of characteristic of medical images and language. In this paper, we propose a novel From Image type point To Sentence (FITS) method to tackle the above challenge. In particular, the type of the medical images is represented as a type point which is further considered in the question sentence representation. The combined representation aims to optimize the feature distribution in an embedding space and thus enhances the ability of semantic alignment. Type point is also used in two feature extraction modules for medical questions and images respectively, which can efficiently improve the reasoning ability of different modalities, and further enhance the applicability of the fusion method for Med-VQA. The experimental results show that FITS outperforms all the previous approaches in terms of accuracy especially in open-ended questions significantly."
9578681,Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading,"Lip reading aims to predict the spoken sentences from silent lip videos. Due to the fact that such a vision task usually performs worse than its counterpart speech recognition, one potential scheme is to distill knowledge from a teacher pretrained by audio signals. However, the latent domain gap between the cross-modal data could lead to a learning ambiguity and thus limits the performance of lip reading. In this paper, we propose a novel collaborative framework for lip reading, and two aspects of issues are considered: 1) the teacher should understand bi-modal knowledge to possibly bridge the inherent cross-modal gap; 2) the teacher should adjust teaching contents adaptively with the evolution of the student. To these ends, we introduce a trainable ""master"" network which ingests both audio signals and silent lip videos instead of a pretrained teacher. The master produces logits from three modalities of features: audio modality, video modality, and their combination. To further provide an interactive strategy to fuse these knowledge organically, we regularize the master with the task-specific feedback from the student, in which the requirement of the student is implicitly embedded. Meanwhile, we involve a couple of ""tutor"" networks into our system as guidance for emphasizing the fruitful knowledge flexibly. In addition, we incorporate a curriculum learning design to ensure a better convergence. Extensive experiments demonstrate that the proposed network outperforms the state-of-the-art methods on several benchmarks, including in both word-level and sentence-level scenarios."
9023113,Automatic Ontology Population Using Deep Learning for Triple Extraction,"Ontology is a kind of representation used to represent knowledge in a form that computers can derive the content meaning. The purpose of this work is to automatically populate an ontology using deep neural networks for updating an ontology with new facts from an input knowledge resource. In this study for automatic ontology population, a bi-LSTM-based term extraction model based on character embedding is proposed to extract the terms from a sentence. The extracted terms are regarded as the concepts of the ontology. Then, a multi-layer perception network is employed to decide the predicates between the pairs of the extracted concepts. The two concepts (one serves as subject and the other as object) along with the predicate form a triple. The number of occurrences of the dependency relations between the concepts and the predicates are estimated. The predicates with low occurrence frequency are filtered out to obtain precise triples for ontology population. For evaluation of the proposed method, we collected 46,646 sentences from Ontonotes 5.0 for training and testing the bi-LSTM-based term extraction model. We also collected 404,951 triples from ConceptNet 5 for training and testing the multilayer perceptron-based triple extraction model. From the experimental results, the proposed method could extract the triples from the documents, achieving 74.59% accuracy for ontology population."
9781694,A Hybrid Arabic Text Summarization Approach based on Transformers,"Recently, the amount of data in the world has increased tremendously. Most of the research and efforts have focused on dealing with data in the English language. Dealing with data in other languages, such as Arabic, has become a significant challenge. In this paper, we proposed a sequential hybrid model based on a transformer to summarize Arabic articles. We used two main approaches of summarization to make our model. The First one is the extractive approach which depends on the most important sentences from the articles as it is to be the summary, so we used more than word embedding techniques to help us know which sentences are more important and by using Deep Learning techniques specifically transformers such as AraBert we make our summary, The second one is abstractive, and this approach is similar to human summarization, which means that the machine can use some words which have the same meaning but are not found in the original text. We apply this kind of summary using the MT5 Arabic pre-trained transformer model. We sequentially applied these two summarization approaches to build our A3SUT hybrid model. The output of the extractive module is fed into the abstractive module. We enhanced the summary's quality to be closer to the human summary by applying this approach. We tested our model on the ESAC dataset and evaluated the extractive summary using the Rouge score technique; we got a precision of 0.5348 and a recall of 0.5515, and an f1 score of 0.4932 and the evaluation of the abstractive model is evaluated by user satisfaction. We add some features to our summary to make it more understandable and accessible by applying the metadata generation task” data about data” and classification. By applying metadata generation, we add facilities and support to our summary, identification, and summary organization. Metadata also captures and provides essential contextual details, as not all summaries are self-describing. Also, classify the original text to determine t...
(Show More)"
9260073,Exploiting BERT with Global-Local Context and Label Dependency for Aspect Term Extraction,"Aspect term extraction (ATE) is a subtask of aspect-based sentiment analysis (ABSA), which aims to extract all aspect-specific words in a sentence. Recent neural network methods ignore the problem that word may play different semantic roles in different sentences and have limitation in handling dependencies between labels. In this work, we first exploit BERT as embedding layer to obtain word-level representations and utilize BERT architecture to capture global sequence features. Then, a position-aware attention is proposed to extract local context information. Global-local context representations of words are built by merging the global sequence features and local context information, which can select related information from both sides: global sequence and local context. Finally, to model the label dependency, we construct a label dependency module based on RNN and CRF, where the previous label features are introduced as additional information for label relationship modeling. Experimental results on four benchmark datasets show that our proposed model obtains the state-of-the-art performance."
9396015,Comparative Question Answering System based on Natural Language Processing and Machine Learning,"It is very tedious for anyone to go through the whole document to get answers for their queries since there is a need of Question Answering system to make life easier. In this research, we used machine learning architectures in Question Answering field, based on the Stanford Question Answering dataset (SQuAD). In our work, build two models in which first model used unsupervised learning algorithm GloVe to get vector representation of word and trained using bidirectional-LSTM in which the training accuracy is 64.93% and testing accuracy is 60.33% scored with the model. In Second model used InferSent which is a sentence embedding method to get vector representation of data. This vector representation data is used to train model. The machine learning algorithms used are XG Boost and Multinomial Logistic Regression in which scored 70.02 percent in training and 66.03 percent in testing. The aim of this research is to build the best accuracy model using Glove and InferSent to use vector of various dimensionality to represent it numerically so that machine can interpret it."
9328431,Identifying Non-functional Requirements from Unconstrained Documents using Natural Language Processing and Machine Learning Approaches,"Requirements engineering is the first phase in software development life cycle and it also plays one of the most important and critical roles. Requirement document mainly contains both functional requirements and non-functional requirements. Non-functional requirements are significant to describe the properties and constraints of the system. Early identification of Non-functional requirement has direct impact on the system architecture and initial design decision. Practically, non-functional requirements are extracted manually from the document. This makes it tedious, time-consuming task and prone to various errors. In this paper, we propose an automatic approach to identify and classify non-functional requirements using semantic and syntactic analysis with machine learning approaches from unconstrained documents. We used A dataset of public requirements documents (PURE) that consists of 79 unconstrained requirements documents in different forms. In our approach, features were extracted from the requirement sentences using four different natural language processing methods including statistical and state-of-the-art semantic analysis presented by Google word2vec and bidirectional encoder representations from transformers models. The adopted approach can efficiently classify non-functional requirements with an accuracy between 84% and 87% using statistical vectorization method and 88% to 92% using word embedding semantic methods. Furthermore, by fusing different models trained on different features, the accuracy improves by 2.4% compared with the best individual classifier."
9225657,Comparative Study on Abstractive Text Summarization,"This paper represents a comparative study and gives an overview on some of the great research work done on abstractive text summarization. Getting a gist part from a large text document is known as text summarization. In recent years, text summarization became one of the most interesting and crucial research point in Natural Language Processing(NLP) area. Text summarization is grouped in two subparts and those are Extractive Text Summarization(ETS) and Abstractive Text Summarization(ATS). ETS is more simple than ATS. ETS is based on algorithms and it extracts the salient words or sentences from the given large text document. As opposed to, ATS generates summary by itself. It's a long time process because its includes lots of terms such as content preprocessing, word embedding, fundamental model design, discourse rules, validation of test and training data, attention mechanism, supervised, reinforcement learning and so forth. We are going to give a review on some papers about ATS and will discuss about the pros and cons of their models."
9712874,Transform Waveforms into Signature Vectors for General-purpose Incipient Fault Detection,"Power system equipment presents special signatures at the incipient stage of faults. As more renewables are integrated into the systems, these signatures are harder to detect. If faults are detected at an early stage, economical losses and power outages can be avoided in modern power grids. Many researchers and power engineers have proposed a series of signature-specific methods for one type of equipment's waveform abnormality. However, conventional methods are not designed to identify multiple types of incipient faults (IFs) signatures at the same time. Therefore, we develop a general-purpose IF detection method that detects waveform abnormality stemming from multiple types of devices. To avoid the computational burden of the general-purpose IF detection method, we embed the abnormality signatures into a vector and develop a pre-training model (PTM) for machine understanding. In the PTM, signal ""words,"" ""sentences,"" and ""dictionaries"" are designed and proposed. Through the comparison with a machine learning classifier and a simple probabilistic language model, the results show a superior detection performance and reveal that the training radius is highly related to the size of abnormal waveforms."
9823442,"Framework for Topic Modeling using BERT, LDA and K-Means","In today's advanced world, the demand and the amount of data being generated is increasing rapidly. The task of keeping similar data/documents together has also become tremendously difficult. Topic Modeling can help in solving this problem. It is a statistics-based NLP data mining approach used for grouping together similar documents with different contents under similar topics. We have worked on various trending topic modeling techniques and applied them on 20- Newsgroup dataset. We have worked on a novel state of art Topic modeling framework using BERT, LDA and K-means for classifying news articles under similar topics. We also performed our experimentation using simple word embedding models like Word2Vec, Doc2Vec and also by combining doc2vec, sentence transformer with bert summarizer. We were able to achieve the best NMI score out of these models by using our hybrid Bert and LDA model."
9745261,PANNER: POS-Aware Nested Named Entity Recognition Through Heterogeneous Graph Neural Network,"Nested named entity recognition (Nested NER) in knowledge graph (KG) aims at obtaining all meaningful entities, including nested entities for sentences in longer text region. Those obtained entities are to facilitate downstream applications, such as relation extraction, entity resolution, and coreference resolution. This task, however, is challenging not only because of the demand to detect the boundary of the entity but also due to the complexity of those hierarchically nested entities. Since a substantial amount of work has been made to Flat NER (or Nested NER), a few of them can explicitly acquire the position of the entity and utilize the grammatical construction of text. In this work, we propose PANNER, a POS-aware Nested NER model, to solve all the above issues. Specifically, we first construct a heterogeneous graph by introducing the part-of-speech (POS) information of the word. Second, we design a dilated random walk (DRW) algorithm based on a grammatical path to sample a fixed size of neighbors for each node. Third, we aggregate the message from different types of neighbors through an attention mechanism. Finally, we use a bidirectional decoding module to recognize and categorize all the flat and nested entities based on the node embedding in a layer-wise manner. Our extensive experiments show the effectiveness of PANNER in both flat and nested NER."
9825245,Exploring Multimodal Sentiment Analysis through Cartesian Product approach using BERT Embeddings and ResNet-50 encodings and comparing performance with pre-existing models,"Multimodal Sentiment Analysis has become a popular interest of research in Machine Learning. It not only provides better results and context inference but also provides a good alternative for uni-modal analysis. The past few years have seen many new datasets for this task and state-of-the-art models dealing with the intermodular and intramodular dynamics of the task. This paper aims at exploring the pre-existing research on the topic of Multimodal Sentiment analysis and combining it with the idea of Sentence Embedding and Video encoding using state-of-the-art models, using the technique of classifying cartesian-product of extracted features, discussing experiments, and doing the qualitative analysis of the results."
9599076,BERT Based Topic-Specific Crawler,"Nowadays, retrieving certain information using search engines is very popular and one of the main applications of the Internet. To speed up the process of getting the required information(web pages), having a topic-specific crawler is essential to fetch and index only the relevant ones. This paper presents a multi-thread web crawler using a Sentence Bidirectional Encoder Representations from Transformers (S-BERT). The S- BERT is used to calculate the similarity between the predefined classes and the text of the downloaded web pages. This provides a lightweight model compared to using a word embedding with deep learning for text classification."
9474853,Crime Monitor: Monitoring Criminals from Trajectory Data,"The movement of criminals is an important factor used in detecting crimes. Individuals sentenced to house arrest who wears an ankle monitor have their trajectories collected periodically. Each offender using an ankle monitor must adhere to a set of rules, for instance, be at his/her home during the night. Unfortunately, some of them break such rules, also some end up committing crimes again. In this demonstration 1 , we present a prototype system called Crime Monitor to monitor offenders in a semi-open regime. Crime Monitor reports the illegal activities to the police department in real-time based on trajectory features. Thus, the police can effectively prevent crimes from happening and handle them efficiently when they occur. We tackled the trajectory classification problem and used a deep learning model combining embedding with a recurrent neural network to classify illegal activities and learn the pattern regardless of who the criminal user is. We conduct experiments on a real dataset, and we show that DeepeST outperforms other approaches from state- of-the-art."
8681431,Serendipity—A Machine-Learning Application for Mining Serendipitous Drug Usage From Social Media,"Serendipitous drug usage refers to the unexpected relief of comorbid diseases or symptoms when taking medication for a different known indication. Historically, serendipity has contributed significantly to identifying many new drug indications. If patient-reported serendipitous drug usage in social media could be computationally identified, it could help generate and validate drug-repositioning hypotheses. We investigated deep neural network models for mining serendipitous drug usage from social media. We used the word2vec algorithm to construct word-embedding features from drug reviews posted in a WebMD patient forum. We adapted and redesigned the convolutional neural network, long short-term memory network, and convolutional long short-term memory network by adding contextual information extracted from drug-review posts, information-filtering tools, medical ontology, and medical knowledge. We trained, tuned, and evaluated our models with a gold-standard dataset of 15714 sentences (447 [2.8%] describing serendipitous drug usage). Additionally, we compared our deep neural networks to support vector machine, random forest, and AdaBoost.M1 algorithms. Context information helped to reduce the false-positive rate of deep neural network models. If we used an extremely imbalanced dataset with limited instances of serendipitous drug usage, deep neural network models did not outperform other machine-learning models with n-gram and context features. However, deep neural network models could more effectively use word embedding in feature construction, an advantage that makes them worthy of further investigation. Finally, we implemented natural-language processing and machine-learning methods in a web-based application to help scientists and software developers mine social media for serendipitous drug usage."
8830370,Convolutional Neural Networks-Based Locating Relevant Buggy Code Files for Bug Reports Affected by Data Imbalance,"Software bug localization is very important in software engineering, but it is also complicated and time consuming. To improve the efficiency of developers, researchers have developed various traditional bug localization and machine learning bug localization methods. In this paper, we propose a novel method that improves bug localization performance. First, surface lexical correlation matching between bug reports and source code files is used to obtain features by deep neural network. Second, to solve the lexical gap between bug reports and source code files, semantic correlation matching between them is used to obtain features based on word embedding and sentence embedding by deep neural network. Then, the joint features obtained by the surface lexical and semantic correlation matching are fused into a unified feature representation for bug reports and source code files. In addition, since our experimental datasets are imbalanced data, we use a focal loss function to solve the impact of data imbalance. Finally, our method obtains the relatively high bug localization performance compared to other classic methods."
9688981,Text Classification Model Based on Multi-head self-attention mechanism and BiGRU,"Deep learning promotes the development of natural language processing quickly. However, there are still many problems in natural language processing, such as semantic diversity and so on. To solve these problems, this paper proposes a text classification model based on multi-head self-attention mechanism and BiGRU. Firstly, the BERT model is used to generate input sequence of tokens for word embedding layer. Secondly, in order to capture the relation of word dependence in sentences, multi-head self-attention mechanism is used to calculate feature vector weights. And bidirectional GRU is used to extract feature and optimize classification model in order to improve calculate speed. The experiment shows that using this proposed model for text classification can improve the accuracy, recall and F1 score also were higher than traditional Benchmark algorithm, such as CNN, time consuming is lower than BERT model. And word embedding on BERT is better than word2vec."
9782583,Adversarial Spam Detector with Character Similarity Network,"More and more social platforms suffer from the spam messages and trouble in detecting them. Lizhi, one of the most famous audio APPs in China, also suffers from the spam messages. The spam detector in Lizhi faces two major challenges: adversarial actions taken by the spammers and lack of labeled data. In this paper, we propose a novel adversarial spam detector based on character similarity network for detecting adversarial spam messages in Lizhi, namely Lizhi Adversarial Spam Detector (LZASD). The character similarity network is designed to solve the adversarial actions before being taken by the spammers, and the character embedding model is proposed to learn the embeddings of all the characters in the corpus. Then the model generates the sentence embedding of the messages sent by users. At last the classifier will predict whether the message is a spam. Also, the model utilizes active learning to solve the problem of lack of labeled data. Both of offline and online experiments are conducted to confirm the effectiveness of the proposed method."
8691602,Weakly Supervised Learning with Multi-Stream CNN-LSTM-HMMs to Discover Sequential Parallelism in Sign Language Videos,"In this work we present a new approach to the field of weakly supervised learning in the video domain. Our method is relevant to sequence learning problems which can be split up into sub-problems that occur in parallel. Here, we experiment with sign language data. The approach exploits sequence constraints within each independent stream and combines them by explicitly imposing synchronisation points to make use of parallelism that all sub-problems share. We do this with multi-stream HMMs while adding intermediate synchronisation constraints among the streams. We embed powerful CNN-LSTM models in each HMM stream following the hybrid approach. This allows the discovery of attributes which on their own lack sufficient discriminative power to be identified. We apply the approach to the domain of sign language recognition exploiting the sequential parallelism to learn sign language, mouth shape and hand shape classifiers. We evaluate the classifiers on three publicly available benchmark data sets featuring challenging real-life sign language with over 1,000 classes, full sentence based lip-reading and articulated hand shape recognition on a fine-grained hand shape taxonomy featuring over 60 different hand shapes. We clearly outperform the state-of-the-art on all data sets and observe significantly faster convergence using the parallel alignment approach."
9157657,Context-Aware Attention Network for Image-Text Retrieval,"As a typical cross-modal problem, image-text bi-directional retrieval relies heavily on the joint embedding learning and similarity measure for each image-text pair. It remains challenging because prior works seldom explore semantic correspondences between modalities and semantic correlations in a single modality at the same time. In this work, we propose a unified Context-Aware Attention Network (CAAN), which selectively focuses on critical local fragments (regions and words) by aggregating the global context. Specifically, it simultaneously utilizes global inter-modal alignments and intra-modal correlations to discover latent semantic relations. Considering the interactions between images and sentences in the retrieval process, intra-modal correlations are derived from the second-order attention of region-word alignments instead of intuitively comparing the distance between original features. Our method achieves fairly competitive results on two generic image-text retrieval datasets Flickr30K and MS-COCO."
8934592,Doly: Bengali Chatbot for Bengali Education,"This Scientific Research paper is a procedure of an automated system ""Doly: Bengali Chatbot"" which gives a reply to a user query on behalf of a human for the education system in the Bengali language. This is an AI-based Chatbot, mainly based on machine learning algorithms and Bengali Natural Language Processing (BNLP). The machine gets embedded with this knowledge to identify the desired sentences and making a decision within itself, as a response to answer questions. There are many English Chatbot's which used in education, web query, banking sector & various sectors. In this research, we have propounded a complete data-driven retrieval based closed domain Chatbot which is easily colloquy in the Bengali language with the users. We've created the train function adapter to train the Doly by encoding (encoding=""utf8"") our corpus from bot data. An input adapter has been created to take input and for output, an output adapter has been created to generate automated responses to a user's input. We have also used a machine learning algorithm like search algorithm for finding an appropriate list of matching results from the corpus and use Naïve Bayesian algorithm to generate the right answer from data. The main aim of this Chatbot based system is to bridge the gap between the knowledge sources by providing instant replies to the questions and queries that have to ask in the Bengali language."
8960358,Variable Convolution and Pooling Convolutional Neural Network for Text Sentiment Classification,"With the popularity of the internet, the expression of emotions and methods of communication are becoming increasingly abundant, and most of these emotions are transmitted in text form. Text sentiment classification research mainly includes three methods based on sentiment dictionaries, machine learning and deep learning. In recent years, many deep learning-based works have used TextCNN (text convolution neural network) to extract text semantic information for text sentiment analysis. However, TextCNN only considers the length of the sentence when extracting semantic information. It ignores the semantic features between word vectors and only considers the maximum feature value of the feature image in the pooling layer without considering other information. Therefore, in this paper, we propose a convolutional neural network based on multiple convolutions and pooling for text sentiment classification (variable convolution and pooling convolution neural network, VCPCNN). There are three contributions in this paper. First, a multiconvolution and pooling neural network is proposed for the TextCNN network structure. Second, four convolution operations are introduced in the word embedding dimension or direction, which are helpful for mining the local features on the semantic dimensions of word vectors. Finally, average pooling is introduced in the pooling layer, which is beneficial for saving the important feature information of the extracted features. The verification test was carried out on four emotional datasets, including English emotional polarity, Chinese emotional polarity, Chinese subjective and objective emotion and Chinese multicategory. Our approach is effective in that its result was up to 1.97% higher than that of the TextCNN network."
9054337,Improving Prosody with Linguistic and Bert Derived Features in Multi-Speaker Based Mandarin Chinese Neural TTS,"Recent advances of neural TTS have made ""human parity"" synthesized speech possible when a large amount of studio-quality training data from a voice talent is available. However, with only limited, casual recordings from an ordinary speaker, human-like TTS is still a big challenge, in addition to other artifacts like incomplete sentences, repetition of words, etc. Chinese, a language, of which the text is different from that of other roman-letter based languages like English, has no blank space between adjacent words, hence word segmentation errors can cause serious semantic confusions and unnatural prosody. In this study, with a multi-speaker TTS to accommodate the insufficient training data of a target speaker, we investigate linguistic features and Bert-derived information to improve the prosody of our Mandarin Chinese TTS. Three factors are studied: phone-related and prosody-related linguistic features; better predicted breaks with a refined Bert-CRF model; augmented phoneme sequence with character embedding derived from a Bert model. Subjective tests on in- and out-domain tasks of News, Chat and Audiobook, have shown that all factors are effective for improving prosody of our Mandarin TTS. The model with additional character embeddings from Bert is the best one, which outperforms the baseline by 0.17 MOS gain."
8715481,Chinese NER Using Dynamic Meta-Embeddings,"Named entity recognition (NER) is one of the basic techniques in natural language processing tasks. Chinese NER is complicated and difficult which remains a major challenge. One of the main reasons is that the boundaries of entities in Chinese are blurred and closely related to word segmentation results. Previous studies for this task have broadly divided into two categories, word-based, and character-based methods. However, the former class suffers from the word segmentation errors, and the latter cannot make full use of the information on multiple granularities. To address these problems, we investigate a new dynamic meta-embeddings method and apply it to Chinese NER task, which utilizes attention mechanism to combine features of both character and word granularity in embedding layer. The meta-embeddings created by our method are dynamic, data-specific, and task-specific, since the meta-embeddings for same characters in different sentence sequences are distinct. The experiments on MSRA and LiteratureNER datasets validate the effectiveness of our model, and this method achieves the state-of-the-art results on LiteratureNER."
9055024,Identifying Emotion Labels From Psychiatric Social Texts Using a Bi-Directional LSTM-CNN Model,"Discussion features in online communities can be effectively used to diagnose depression and allow other users or experts to provide self-help resources to those in need. Automatic emotion identification models can quickly and effectively highlight indicators of emotional stress in the text of such discussions. Such communities also provide patients with important knowledge to help better understand their condition. This study proposes a deep learning framework combining word embeddings, bi-directional long short-term memory (Bi-LSTM), and convolutional neural networks (CNN) to identify emotion labels from psychiatric social texts. The Bi-LSTM is a powerful mechanism for extracting features from sequential data in which a sentence consists of multiple words in a particular sequence. CNN is another powerful feature extractor which can convolute many blocks to capture important features. Our proposed deep learning framework also applies word representation techniques to represent semantic relationships between words. The paper thus combines two powerful feature extraction methods with word embedding to automatically identify indicators of emotional stress. Experimental results show that our proposed framework outperformed other models using traditional feature extraction such as bag-of-words (BOW), latent semantic analysis (LSA), independent component analysis (ICA), and LSA+ICA."
8683758,A Hierarchical Neural Summarization Framework for Spoken Documents,"Extractive text or speech summarization seeks to select indicative sentences from a source document and assemble them together to form a succinct summary, so as to help people to browse and understand the main theme of the document efficiently. A more recent trend is towards developing supervised deep learning based methods for extractive summarization. This paper extends and contextualizes this line of research for spoken document summarization, while its contributions are at least three-fold. First, we propose a neural summarization framework with the flexibility to incorporate extra acoustic/prosodic and lexical features, for which the ROUGE evaluation metric is embedded into the training objective function and can be optimized with reinforcement learning. Second, disparate ways to integrate acoustic features into this framework are investigated. Third, the utility of our proposed summarization methods and several widely-used state-of-the-art ones are extensively compared and evaluated. A series of empirical experiments seem to demonstrate the effectiveness of our summarization methods."
8878136,Capsule Network With Identifying Transferable Knowledge for Cross-Domain Sentiment Classification,"Domain adaptation tasks have raised much attention in recent years, especially, the task of cross-domain sentiment classification, and remarkable success has been achieved on specific domains with large amounts of labeled data. However, annotating enough data in each domain is still expensive and time-consuming, which will produce difficulty in the application of domain adaptation. In this paper, we proposed a Capsule network method with Identifying Transferable Knowledge (CITK) as common knowledge for cross-domain sentiment classification. CITK model uses capsule network to encode the intrinsic spatial part-whole relationship constituting domain invariant knowledge, which bridges the knowledge gap between the source and target domains. In addition, we use Bidirectional Encoder Representations from Transformers (BERT) to convert sentences to equal length, which is called pre-training, in order to obtain more complete semantic embedded representation, so that Significant Consistent Polarity (SCP) words can be more accurate. Extensive experiments are conducted to evaluate the effectiveness of the proposed CITK model on a real world data set of four domains. Experimental results demonstrate that CITK can significantly outperform the state-of-the-art methods for the cross-domain sentiment classification task."
9412167,Adversarial Training for Aspect-Based Sentiment Analysis with BERT,"Aspect-Based Sentiment Analysis (ABSA) studies the extraction of sentiments and their targets. Collecting labeled data for this task in order to help neural networks generalize better can be laborious and time-consuming. As an alternative, similar data to the real-world examples can be produced artificially through an adversarial process which is carried out in the embedding space. Although these examples are not real sentences, they have been shown to act as a regularization method which can make neural networks more robust. In this work, we fine- tune the general purpose BERT and domain specific post-trained BERT (BERT-PT) using adversarial training. After improving the results of post-trained BERT with different hyperparameters, we propose a novel architecture called BERT Adversarial Training (BAT) to utilize adversarial training for the two major tasks of Aspect Extraction and Aspect Sentiment Classification in sentiment analysis. The proposed model outperforms the general BERT as well as the in-domain post-trained BERT in both tasks. To the best of our knowledge, this is the first study on the application of adversarial training in ABSA. The code is publicly available on a GitHub repository at https://github.com/IMPLabUniPr/Adversarial-Training-for-ABSA."
8691665,CCHAN: An End to End Model for Cross Domain Sentiment Classification,"Cross domain sentiment classification (CDSC) aims to adopt a model trained by a source domain to a target domain. It has received considerable attention in recent years. Most existing models mainly focus on learning representations that are domain independent in both the source domain and the target domain. However, domain specific features, which should also be informative are ignored by these models. In this paper, we propose an end to end model. It can capture both the source domain and target domain features at the same time. This model includes two parts; one is a cloze task network (CTN), we use it as an auxiliary task to fine-tune words embedding in both domains. Another is a Convolutional hierarchical attention networks (CHAN), we use it for sentiment classification. The CHAN can capture important words and sentences concerning sentiment based on its two stages of attention mechanism. The CTN and CHAN conduct jointly learning we abbreviate this model as CCHAN. The experiments on the Amazon review datasets demonstrate that the proposed CCHAN can significantly outperform the state-of-the-art methods."
8718588,An Annotation Model on End-to-End Chest Radiology Reports,"Annotating radiographic images with tags is an indispensable preliminary work in computer-aided medical research, which requires professional physician participated in and is quite time-consuming. Therefore, how to automatically annotate radiographic images has become the focus of researchers. However, image report texts, containing crucial radiologic information, have not to be given enough attention for images annotation. In this paper, we propose a neural sequence-to-sequence annotation model. Especially, in the decoding phase, a probability is first learned to copy existing words from report texts or generate new words. Second, to incorporate the patient's background information, “indication” section of the report is encoded as a sentence embedding, and concatenated with the decoder neural unit input. What's more, we devise a more reasonable evaluation metric for this annotation task, aiming at assessing the importance of different words. On the Open-i dataset, our model outperforms existing non-neural and neural baselines under the BLEU-4 metrics. To our best knowledge, we are the first to use sequence-to-sequence model for radiographic image annotation."
8711427,Term Specific TF-IDF Boosting for Detection of Rumours in Social Networks,"The spread of rumours on a social event affects the propagation of true information regarding the event. Separating a rumour from an informative post is of great importance nowadays because posts which contain rumours, try to provide an information which sounds similar to the actual happening. Rumour detection in social media can be treated as a text classification problem. However, the problem involves several challenges. Both rumours and non-rumours may contain a similar form of textual items like words, sentences etc. about an actual happening. The context and way of representing those textual items make a rumour different from a non-rumour post. In our work, we show that in many such cases, standard ways of feature construction does not always capture these patterns. Therefore, we propose a novel approach of feature construction by reweighting the TF-IDF score of some particular terms taking into account the label information of training data. This leads to a better construction of features than the standard TF-IDF representation leading to a more separable set of features. A classffier with TF-IDF boosted features performs better than the combination of standard TF-IDF and state-of-the-art machine learning algorithms using standard TF-IDF score like LightGBM, Gradient Boosting, SVM etc. This is experimentally validated on three different social events which created rumours. We also show that our model gives a comparable performance to a deep learning model like LSTM with Glove word embedding in a much lesser training time, making our model a convenient one for detecting rumours at an early stage."
9101658,Improving Neural Relation Extraction with Implicit Mutual Relations,"Relation extraction (RE) aims at extracting the relation between two entities from the text corpora. It is a crucial task for Knowledge Graph (KG) construction. Most existing methods predict the relation between an entity pair by learning the relation from the training sentences, which contain the targeted entity pair. In contrast to existing distant supervision approaches that suffer from insufficient training corpora to extract relations, our proposal of mining implicit mutual relation from the massive unlabeled corpora transfers the semantic information of entity pairs into the RE model, which is more expressive and semantically plausible. After constructing an entity proximity graph based on the implicit mutual relations, we preserve the semantic relations of entity pairs via embedding each vertex of the graph into a low-dimensional space. As a result, we can easily and flexibly integrate the implicit mutual relations and other entity information, such as entity types, into the existing RE methods.Our experimental results on a New York Times and another Google Distant Supervision datasets suggest that our proposed neural RE framework provides a promising improvement for the RE task, and significantly outperforms the state-of-the-art methods. Moreover, the component for mining implicit mutual relations is so flexible that can help to improve the performance of both CNN-based and RNN-based RE models significant."
9218193,Towards Causality Extraction from Requirements,"System behavior is often based on causal relations between certain events (e.g. If event 1 , then event 2 ). Consequently, those causal relations are also textually embedded in requirements. We want to extract this causal knowledge and utilize it to derive test cases automatically and to reason about dependencies between requirements. Existing NLP approaches fail to extract causality from natural language (NL) with reasonable performance. In this paper, we describe first steps towards building a new approach for causality extraction and contribute: (1) an NLP architecture based on Tree Recursive Neural Networks (TRNN) that we will train to identify causal relations in NL requirements and (2) an annotation scheme and a dataset that is suitable for training TRNNs. Our dataset contains 212,186 sentences from 463 publicly available requirement documents and is a first step towards a gold standard corpus for causality extraction. We encourage fellow researchers to contribute to our dataset and help us in finalizing the causality annotation process. Additionally, the dataset can also be annotated further to serve as a benchmark for other RE-relevant NLP tasks such as requirements classification."
8784747,REVnet: Bring Reviewing Into Video Captioning for a Better Description,"Recently, the task of automatically generating a textual description of a video is attracting increasing interest. The attention-based encoder-decoder framework has been extensively applied in this domain. However, compared with other captioning tasks, such as image captioning, video captioning is more challenging because semantic information among frames is hard to be extracted. In this paper, we propose a reviewing network (REVnet) to reconstruct the previous hidden state, which is combined with the conventional encoder-decoder framework. REVnet brings backward flow into the caption generation process, which encourages the hidden state embedding more information and enables the semantics of the generated sentence more coherent. Furthermore, REVnet can regularize the attention mechanism within the framework, which encourages the model better utilizing the semantic information extracted from multiple different frames. Our experimental results on benchmark datasets demonstrate that our proposed REVnet has a significant improvement over the baseline method. Furthermore, we use a reinforcement learning method to finetune the model, and get better results than the state-of-the-art methods."
9127959,Information Extraction for Intestinal Cancer Electronic Medical Records,"The data generated by the structured electronic medical records is helpful for mining and extracting medical data, and it is an effective way to make effective use of valuable data resources. However, the hospitals have accumulated a large number of unstructured data in electronic medical records, which cannot be effectively searched, resulting in serious waste of resources. In this paper, we study the problem of extracting attribute values from the unstructured text in electronic medical records. By observing intestinal cancer diagnostic texts, our attributes have two categories - discriminative attributes and extractive attributes, which use the text classification and the sequence labeling to tackle attribute values extraction problems. For discriminative attributes, we firstly divide the text into sentences/segments as instances. Secondly, we fine-tune the pre-trained word embedding to capture domain-specific semantics/knowledge. Thirdly, we also use an attention mechanism to select the most important instance for different attribute extractors. Finally, multi-tasking learning is used to share useful information to get better experimental results. For extractive attributes, we propose a novel model to get attribute values, including the BiLSTM layer, the CNN layer and the CRF layer. In particular, we use BiLSTM and CNN to learn text features and CRF as the last layer of the model. Experiments have shown that our method is superior to several competitive baseline methods."
9025265,Enhancements to the Sequence-to-Sequence-Based Natural Answer Generation Models,"There is a great interest shown by academic researchers to continuously improve the sequence-to-sequence (Seq2Seq) model for natural answer generation (NAG) in chatbots. The Seq2Seq model shows a weakness whereby the model tends to generate answers that are generic, meaningless and inconsistent with the questions. However, a comprehensive literature review on the factors contributing to the weakness and potential solutions are still missing. Therefore, this review article fills the gap by reviewing Seq2Seq based natural answer generation-based literature to identify those factors and proposed methods to address the weakness. This literature review identified several factors such as input question is not sufficient to determine a meaningful output, usage of cross-entropy function as the loss function during training, infrequent words in training data, language model influence which generates answers not relevant to the question, utilization of teacher forcing method during training which results in exposure bias, long sentences and inability to consider dialogue history as the factors. Additionally, this literature review also identified and reviewed the methods proposed to address the weakness such as utilizing additional embedding and encoders, using different loss functions and training approaches, as well as utilizing other mechanisms like copying source word(s) and paying attention to a certain portion of the input. For discussion, these methods are categorized into four broad categories which are Structural Modifications, Augmented Learning, Beam Search and Complementary Mechanisms. Additionally, the paper highlights unexplored areas in Seq2Seq modeling and proposes potential future works for natural answer generation."
9032906,CPSX: Using AI-Machine Learning for Mapping Human-Human Interaction and Measurement of CPS Teamwork Skills,"The objective of this work is to present a machine learning (ML) -based framework to identify evidence about collaborative problem solving (CPS) cognitive (teamwork) and social-emotional learning (SEL) skills from the dyadic (human-human-HH) interactions. This work extends our previous work (Chopade et al. IEEE HST 2018, LAK2019) [1], [2]. Explicitly, we are interested in how teamwork skills and team dynamics are demonstrated as verbal and nonverbal behaviors, and how these behaviors can be captured and analyzed via passive data collection. For this work we use a two-player cooperative CPS game, Crisis in Space (CIS) from LRNG (Previously GlassLab Inc). During the summer of 2018, we implemented this CIS game for interns as a group study. A total of 34 participants played the game and provided study and survey data. During the study, we collected participants' game play data, such as audio, video and eye tracking data streams. This research involves analyzing CIS multimodal game data, and developing skill models, and machine learning techniques for CPS skills measurement. In this paper, we present our ML framework for the analysis of audio data along with preliminary results from a pilot study. The analysis of audio data uses natural language processing (NLP) techniques, such as bag-of-words and sentence embedding. Our preliminary results show that various NLP features can be used to describe successful and unsuccessful CPS performances. The ML based framework supports the development of evidence centered design for teamwork skills-mapping and aims to help teams operate effectively in a complex situation. Potential applications of this work include support for the Department of Homeland Security (DHS), and the US Army for the development of learner and team centric training, cohort, and team behavioral skill-mapping."
8659100,Improving Diversity of Image Captioning Through Variational Autoencoders and Adversarial Learning,"Learning translation from images to human-readable natural language has become a great challenge in computer vision research in recent years. Existing works explore the semantic correlation between the visual and language domains via encoder-to-decoder learning frameworks based on classifying visual features in the language domain. This approach, however, is criticized for its lacking of naturalness and diversity. In this paper, we demonstrate a novel way to learn a semantic connection between visual information and natural language directly based on a Variational Autoencoder (VAE) that is trained in an adversarial routine. Instead of using the classification based discriminator, our method directly learns to estimate the diversity between a hidden vector embedded from a text encoder and an informative feature that is sampled from a learned distribution of the autoencoders. We show that the sentences learned from this matching contains accurate semantic meaning with high diversity in the image captioning task. Our experiments on the popular MSCOCO dataset indicates that our method learns to generate high-quality natural language with competitive scores on both correctness and diversity."
8995249,A Multi-channel Neural Network for Imbalanced Emotion Recognition,"Imbalanced issue becomes one of major bottleneck for further popularizing of emotion recognition in actual applications. Recently, some resampling methods have been proposed to improve performance by balancing the training samples. However, over-sampling methods may lead to overfitting, and undersampling methods would lose useful emotion information. In this paper, we propose a multi-channel deep architecture to improve performance in both samples and features imbalance. Specifically, we design a class correction loss function to overcome the gap between majority and minority emotions. Meanwhile, emotionspecific word embedding and a fine-tuning BERT are used to increase the differentiation of emotion words and sentences. Experimental results on two Chinese micro-blog emotion classification datasets show that our proposed architecture outperforms state-of-the-art in imbalanced emotion recognition."
9440180,AutoKG - An Automotive Domain Knowledge Graph for Software Testing: A position paper,"Industries have a significant amount of data in semi-structured and unstructured formats which are typically captured in text documents, spreadsheets, images, etc. This is especially the case with the software description documents used by domain experts in the automotive domain to perform tasks at various phases of the Software Development Life Cycle (SDLC). In this paper, we propose an end-to-end pipeline to extract an Automotive Knowledge Graph (AutoKG) from textual data using Natural Language Processing (NLP) techniques with the application of automatic test case generation. The proposed pipeline primarily consists of the following components: 1) AutoOntology, an ontology that has been derived by analyzing several industry scale automotive domain software systems, 2) AutoRE, a Relation Extraction (RE) model to extract triplets from various sentence types typically found in the automotive domain, and 3) AutoVec, a neural embedding based algorithm for triplet matching and context-based search. We demonstrate the pipeline with an application of automatic test case generation from requirements using AutoKG."
8979733,Effective Use of Augmentation Degree and Language Model for Synonym-based Text Augmentation on Indonesian Text Classification,"Machine learning based text processing relies on a qualified text dataset. Text augmentation research aims to enrich text dataset in order to gain higher performance compared to the one using original text dataset. We have conducted text augmentation process on Indonesian text classification by replacing certain words with their synonyms. The process consists of determining the number of words to be substituted in the sentence and selecting the substitute word from the synonym list. The first process, determining the number of words to be substituted, is done using augmentation degree. The second process, selecting the best substitute word, is done using language model. The synonym list is built from thesaurus. We compared several options in building language model. Statistical model is built using combinations of n-gram and smoothing while simple neural model is built using gram value of 3 and 5. The neural model uses pre trained word embedding as input. 5-gram neural model excels other language model setup by significant value of perplexity. Using the best language model, augmented dataset is generated and applied on two classification task of aspect-based sentiment analysis: aspect categorization and sentiment classification. Experiments were done using augmentation degree of 0.1 to 1. The best augmentation degree yields a better 3-4% on classification model's performance."
8997176,Stacked Multi-head Attention for Multi-turn Response Selection in Retrieval-based Chatbots,"In the field of deep learning, response selection is the key to retrieval-based chatbots. Faced with the challenge of contextual meaning comprehension and semantic matching, we propose matching a response through diverse information by stacked multi-head attention. First, we construct multi-granular representations on embedded input sentences by stacked multi-head attention. Based on these representations, we build two different matching matrices for context-response segment pairs with another multi-head attention stack. Next, we use a two-layer CNN to extract hidden information from the matching matrices. The results are matching scores which measure the correlation among every context and its candidate responses. According to the matching scores, one or more proper responses will be chosen from candidate responses. Multi-head attention reinforces the model's ability to focus on different positions, as experiment on the Ubuntu Dialogue Corpus v1 and the Douban Conversation Corpus show that our model is superior to the baseline model."
9033569,Application of Summarization and Sentiment Analysis in the Tourism domain,"Text summarization helps in reducing the size of a text while preserving its information content. Text Summarization can be derived as shortening the source text in to a version that it's information content and overall meaning is preserved. It is very difficult for human beings to manually summarize large documents of text. These Text summarization methods can be classified into two broader methods such as extractive summarization and abstractive summarization. The prior method consists of selecting important sentences; paragraphs etc. from the source document and make a brief version of it by combining them. The latter method consists of understanding the overall meaning embedded in the original text and re-generates the content briefly. Sentiment analysis on the other hand is the process of computationally identifying and categorizing opinions expressed in text to clarify someone's' attitude towards a topic is positive or else negative or even neutral. In this paper, a Survey of Summarization and sentiment analysis and their application in the tourism domain has been presented."
9671919,Gun Violence News Information Retrieval using BERT as Sequence Tagging Task,"The growth in both frequency and severity of gun violence in the United States has necessitated increased research into prevention, despite the lack of funding. Comprising more than 60k gun violence media articles with a total data size of 520 MB, the gun violence database (GVDB) was developed to assist natural language processing researchers in developing and testing prevention methods. Original research based on the GVDB utilized a span-selection model to extract shooter and victim information, but their works might potentially trim out important span candidates. We proposed a new approach to improve identification accuracy and recognize every token in a sentence using a sequence tagging technique. We implemented a BIO sequence tagging model at the token-level using BERT, then further classified each token using LSTM, BiLSTM, and CRF. We found that utilizing BERT as an embedding layer, and decoding word representation as a sequence tagging task, improved shooter/victim identification compared to a span-selection model. We believe that if this improved model is combined with gun violence related keywords, automated techniques could be implemented to identify precursors/risks to gun violence on social media, allowing for intervention by law enforcement or community agencies before escalation to deaths."
8852055,Hierarchical Multi-dimensional Attention Model for Answer Selection,"Answer selection is an important subtask of the question answering domain in natural language processing(NLP) applications. In this task, attention mechanism is a widely used technique which focuses on the context information and interrelationship between different words in the sentences to allocate different weight and enhance feature. However, the natural characteristics of words themselves are not fully excavated, thus the performance may be limited to a certain extent. In this paper, we propose a novel Hierarchical Multidimensional Attention (HMDA) model to address this issue. Especially, HMDA proposes a new kind of attention mechanism, word-attention, a true individual attention which can enhance the implied meaning of the word itself to extract features from word level which are more unique. Then HMDA uses global co-attention to better utilize word-attention and capture more common similar features. In order to utilize this attention-based semantic information on different granularities differently, HMDA designs a multi-layer structure which makes full use of all attention mechanisms by embedding attention features to model hierarchically. HMDA obtains various fine-grained information between question and candidate answers and avoids information loss. Empirically, we demonstrate that our proposed model can consistently outperform the state-of-the-art baselines under different evaluation metrics on all TrecQA, WikiQA and InsuranceQA datasets."
9353756,A study on exercise recommendation method using Knowledge Graph for computer network course,"With the vast applications of massive online learning platforms during the coVID-19 outbreak, the personalized exercise recommendation methods play an import role on computer aided instruction(CAI). Most existing methods generates the exercises according to the contents and knowledge system structure, lacking semantic relationships between exercises and its knowledge. Knowledge graph is widely used to represent the semi-structured and schemaless information (nodes) and their relation (edges), and indicate the sentence embedding grammatical structure and semantic relations, thus it can be applied on computer aided instruction to automatically generate the personalized exercises. Aiming to improve the efficiency of exercise recommendation, this paper studies the feature information of computer network course, and proposes a content and knowledge graph based personalized exercise recommendation method. More specifically, knowledge graph is firstly constructed from entities and relations of computer network course, and the information vectors of exercises are generated by combining the knowledge with the exercises content. And then the learner's historical log data is analyzed, and the semantic similarity between exercises and their knowledge are generated for the wrong answers. According the semantic similarity of knowledge, the final exercises are recommended for the learners. Experimental results show that the proposed method can improve the efficiency of exercises recommendation."
9167245,Hierarchical Multi-Granularity Attention- Based Hybrid Neural Network for Text Classification,"Neural network-based approaches have become the driven forces for Natural Language Processing (NLP) tasks. Conventionally, there are two mainstream neural architectures for NLP tasks: the recurrent neural network (RNN) and the convolution neural network (ConvNet). RNNs are good at modeling long-term dependencies over input texts, but preclude parallel computation. ConvNets do not have memory capability and it has to model sequential data as un-ordered features. Therefore, ConvNets fail to learn sequential dependencies over the input texts, but it is able to carry out high-efficient parallel computation. As each neural architecture, such as RNN and ConvNets, has its own pro and con, integration of different architectures is assumed to be able to enrich the semantic representation of texts, thus enhance the performance of NLP tasks. However, few investigation explores the reconciliation of these seemingly incompatible architectures. To address this issue, we propose a hybrid architecture based on a novel hierarchical multi-granularity attention mechanism, named Multi-granularity Attention-based Hybrid Neural Network (MahNN). The attention mechanism is to assign different weights to different parts of the input sequence to increase the computation efficiency and performance of neural models. In MahNN, two types of attentions are introduced: the syntactical attention and the semantical attention. The syntactical attention computes the importance of the syntactic elements (such as words or sentence) at the lower symbolic level and the semantical attention is used to compute the importance of the embedded space dimension corresponding to the upper latent semantics. We adopt the text classification as an exemplifying way to illustrate the ability of MahNN to understand texts. The experimental results on a variety of datasets demonstrate that MahNN outperforms most of the state-of-the-arts for text classification."
9619690,A Graph Reasoning Model Based on Multiple Types of Nodes,"Multi-hop machine reading comprehension (MRC) depends on the model to answer a semantically related question based on the given supporting text. In this study, inspired by previous research about Graph Neural Network(GNN), we develop a new graph covering three kinds of nodes: Sentence, Entity, Candidate. A corresponding MRC method based on graph inference is proposed, called SEC reasoner. The advantage of SECr is that it adopts different granular information as graph nodes, which can execute a more comprehensive reasoning. Our proposed model first initialize the node representations by co-attention, multi-head attention and self-attention based on context embedding. SECr then employs GNN based information transfer algorithms to collect evidences from the node encoding, and calculates a score for choosing the answer. In experiments, we evaluate the model on two popular benchmark datasets - WikiHop and MedHop. SECr reachs accuracy rates of 71.6%, 63.1% respectively, which is a competitive result. In addition, a comprehensive analysis proves the effectiveness and interpretability of the SECr."
9658719,A Multi-Objective Optimization-based Clustering Approach for CORD-19 Scholarly Articles,"The pandemic disease COVID-19, originated from the SARS-CoV-2 virus has spread globally. Researchers are working tirelessly on areas including studying the transmission of COVID-19, promoting its identification, designing new vaccines and therapies, and recognizing its socio-economic consequences. This extensive research leads to the exploration of thousands of scientific papers related to biology, chemistry, genetics, health, and economy. Therefore, it is essential to develop an intelligent text mining technique for segregating this rich source of data to perform easy access, information retrieval, and interpretation within minimum time and resources. We propose a multi-objective optimization-based document clustering approach for the CORD-19 (COVID-19 Open Research Dataset) dataset in this paper. Here, a new technique utilizing BioBERT has been proposed, which benefits from the abstract and the document text, rather than only the brief abstract, to perceive a concise understanding of the text to generate clusters with better definitions. The main contributions of the proposed work are two-fold: in the first step, we have used BioBERT to generate the sentence embedding which is further used for the document representation. In the next step, we have developed a multi-objective optimization (MOO) based clustering algorithm for grouping the generated document vector representations. In this MOO-based clustering, we have used Non-dominated Sorting Genetic Algorithm-II and Fuzzy c-means algorithm as the underlying MOO and clustering technique, respectively. This model is evaluated using the Silhouette Score (Silhouette score) and Calinski-Harabasz index (CH index), and the clustering solutions are visualized using word clouds. The clustering results exhibit significant improvements over various other existing clustering models."
9293731,Assessment of the Relative Importance of different hyper-parameters of LSTM for an IDS,"Recurrent deep learning language models like the LSTM are often used to provide advanced cyber-defense for high-value assets. The underlying assumption for using LSTM networks for malware-detection is that the op-code sequence of a malware could be treated as a (spoken) language representation. There are differences between any spoken-language (sequence of words/sentences) and the machine-language (sequence of op-codes). In this paper we demonstrate that due to these inherent differences, an LSTM model with its default configuration as tuned for a spoken-language, may not work well to detect malware (using its op-code sequence) unless the network’s essential hyper-parameters are tuned appropriately. In the process, we also determine the relative importance of all the different hyper-parameters of an LSTM network as applied to malware detection using their op-code sequence representations. We experimented with different configurations of LSTM networks, and altered hyper-parameters like the embedding-size, number of hidden-layers, number of LSTM-units in a hidden layers, pruning/padding-length of the input-vector, activation-function, and batch-size. We discovered that owing to the enhanced complexity of the malware/machine-language, the performance of an LSTM network configured for an Intrusion Detection System, is very sensitive towards the number-of-hidden-layers, input sequence-length and the choice of the activation-function. Also, for (spoken) language-modeling, the recurrent architectures by-far outperforms their non-recurrent counterparts. Therefore, we also assess how sequential DL architectures like the LSTM compares against their non-sequential counterparts like the MLP-DNN for the purpose of malware-detection."
9763637,Performance Analysis on Deep Learning Models in Humor Detection Task,"Natural language often has multiple meanings, and simple changes to sentences can significantly affect the emotional meaning of content. Detection systems, especially those operating in media and social networks, should also identify and assess complex semantic situations. Humor detection is a good starting point for machines to understand emotion. Humor is a complex and ambiguous emotion concept unique to natural language. In this paper, we studied and analyzed the performance of models with different characteristics on humor detection. We used the dataset provided by SemEVAL-2020 and use Glove for embedding. We then pass them to three models with distinct features: Neural Network, VGG, and Transformer. We conclude from the experimental results that Transformer has shown the best performance in this experiment. VGG can be used in a limited number of epochs, but the structure can still be refined to degrade the loss rate."
9377758,Forecasting People’s Action via Social Media Data,"In this paper, we explored predicting human activity based on users' content from social media. The activities we do are related to our interests, personalities, political preferences and our future decisions. The data set we collect contains examples of social media users related to many daily business activities. Then, we use the latest tailored sentence embedding framework to recognize the semantics of human activities and automatically cluster these activities. We proposed a neural network architecture to predict all activities performed by a specific user based on previous posts and self-describing text. Besides, we explore how adding inferred user characteristics into our model contributes to this forecasting task."
9673905,Span-based Joint Extracting Subjects and Objects and Classifying Relations with Multi-head Self-attention,"Entity recognition and relation extraction are two central tasks for extracting information from unstructured text. The current popular models are to perform joint entity and relationship extraction at the span level. However, they do not take full advantage of the uneven distribution of information values in natural languages and their essence of extracting entities before classifying relations leads to entity redundancy. To solve these problems, we propose a novel span-based joint entity and relationship extraction model. We embed the multi-head self-attention layer to the existing span-based entity and relation joint extraction model to leverage valuable information such as semantic and syntactic features in the input sentences. We also propose a novel method to extract subject and object, which can alleviate the entity redundancy problem. In the ablation experiments, we demonstrate the benefits of both improvements for extracting relation triples. The micro f1 of our model on Con1104 is 72.82, the macro f1 is 74.20, and the f1 on SciERC is 52.03, which are better than the currently widely used model."
9532783,An Android based Mobile Spoken Dialog System for Telugu language to control Smart appliances,"Speech processing is an active research field in which spoken words or sentences are processed to understand the meaning. One of the significant applications is designing spoken dialog systems whose task is to interpret the message in the speech signal. IoT is such an area where spoken dialog can be handy to interact with connected devices. Home appliance management is an interactive environment where there is a scope to develop speech interfaces for the operation of the appliances. The spoken interfaces to control the home appliances have to be designed specifically to a particular language since the development of a generic system is cumbersome. Even though commercial systems are available, they are to be customized to suit the needs of a specific language. In this paper, an android based spoken dialog to control home appliances is proposed for the Telugu language. The speech processing capability is embedded into a mobile device to make it useful in poor network environments. The design and the implementation details are discussed in the paper."
9830752,Task-Oriented Multi-User Semantic Communications,"While semantic communications have shown the potential in the case of single-modal single-users, its applications to the multi-user scenario remain limited. In this paper, we investigate deep learning (DL) based multi-user semantic communication systems for transmitting single-modal data and multimodal data, respectively. We adopt three intelligent tasks, including, image retrieval, machine translation, and visual question answering (VQA) as the transmission goal of semantic communication systems.We propose a Transformer based framework to unify the structure of transmitters for different tasks. For the single-modal multi-user system, we propose two Transformer based models, named, DeepSC-IR and DeepSC-MT, to perform image retrieval and machine translation, respectively. In this case, DeepSC-IR is trained to optimize the distance in embedding space between images and DeepSC-MT is trained to minimize the semantic errors by recovering the semantic meaning of sentences. For the multimodal multi-user system, we develop a Transformer enabled model, named, DeepSC-VQA, for the VQA task by extracting text-image information at the transmitters and fusing it at the receiver. In particular, a novel layer-wise Transformer is designed to help fuse multimodal data by adding connection between each of the encoder and decoder layers. Numerical results show that the proposed models are superior to traditional communications in terms of the robustness to channels, computational complexity, transmission delay, and the task-execution performance at various task-specific metrics."
9003982,Language Model Bootstrapping Using Neural Machine Translation for Conversational Speech Recognition,"Building conversational speech recognition systems for new languages is constrained by the availability of utterances capturing user-device interactions. Data collection is expensive and limited by speed of manual transcription. In order to address this, we advocate the use of neural machine translation as a data augmentation technique for bootstrapping language models. Machine translation (MT) offers a systematic way of incorporating collections from mature, resource-rich conversational systems that may be available for a different language. However, ingesting raw translations from a general purpose MT system may not be effective owing to the presence of named entities, intra sentential code-switching and the domain mismatch between the conversational data being translated and the parallel text used for MT training. To circumvent this, we explore following domain adaptation techniques: (a) sentence embedding based data selection for MT training, (b) model finetuning, and (c) rescoring and filtering translated hypotheses. Using Hindi language as the experimental testbed, we supplement transcribed collections with translated US English utterances. We observe a relative word error rate reduction of 7.8-15.6%, depending on the bootstrapping phase. Fine grained analysis reveals that translation particularly aids the interaction scenarios underrepresented in the transcribed data."
9828432,PhysioVec: IoT Biosignal Based Search Engine for Gastrointestinal Health,"Gastrointestinal problems are major health threats to term newborn babies. There are currently no known methods for monitoring the gastrointestinal health of these babies in ICU units contributing to thousands of yearly mortality rates in Australia alone. The internet and Health Social networks (HSN) provide a large amount of useful information for patients. However, finding the right information on HSN is time-consuming and challenging because data from HSN is too large to be processed manually. We develop PhysioVec, a Bowel-Sound IoT to HSN search engine that extracts physiological measurements from bowel sounds providing an automated search of HSN. PhysioVec consists of three parts: Local Recurrent Transformer (LRT), a Multivariate radial-basis Logistic Interpreter (MLI), and a sentence embedding module. LRT combines local attention and recurrent Transformer encoder to reduce overfitting and improve the performance of bowel sound segmentation. The physiological measurements extracted from bowel sounds are used to search for relevant health information on the internet. PhysioVec achieved 100.00% precision in the top one search results for bowel sound with both vomiting and bowel obstruction. Our proposed framework allows patients and doctors to search for useful information in HSN by continuously monitoring bowel sounds with minimal discomfort."
9797846,Cross-modal Enhancement Network for Multimodal Sentiment Analysis,"Multimodal sentiment analysis (MSA) plays an important role in many applications, such as intelligent question-answering, computer-assisted psychotherapy and video understanding, and has attracted considerable attention in recent years. It leverages multimodal signals including verbal language, facial gestures, and acoustic behaviors to identify sentiments in videos. Language modality typically outperforms nonverbal modalities in MSA. Therefore, strengthening the significance of language in MSA will be a vital way to promote recognition accuracy. Considering that the meaning of a sentence often varies in different nonverbal contexts, combining nonverbal information with text representations is conducive to understanding the exact emotion conveyed by an utterance. In this paper, we propose a Cross-modal Enhancement Network (CENet) model to enhance text representations by integrating visual and acoustic information into a language model. Specifically, it embeds a Cross-modal Enhancement (CE) module, which enhances each word representation according to long-range emotional cues implied in unaligned nonverbal data, into a transformer-based pre-trained language model. Moreover, a feature transformation strategy is introduced for acoustic and visual modalities to reduce the distribution differences between the initial representations of verbal and nonverbal modalities, thereby facilitating the fusion of distinct modalities. Extensive experiments on benchmark datasets demonstrate the significant gains of CENet over state-of-the-art methods. The implementation codes are available on https://github.com/Say2L/CENet."
9044059,JEDoDF: Judicial Event Discrimination Based on Deep Forest,"With the rapid development of natural language and the implementation of the Wisdom Court, intelligent judicial assistants has become a new application of natural language processing in the judicial field. The text classification method based on word vector and deep neural network implements statistical classification of judicial documents, but it can not achieve the inherent logical interpretation of judicial cases. A method of extracting semantic logic tree from judicial case texts is proposed, and event tree can be interpreted by deep forest. Judicial documents are divided into several sub-tree fragments by sentence segmentation, and each sub-tree fragment is analyzed by dependency syntax to obtain core subject-predicate-object triples. TF-IDF algorithm is used to calculate the weights of triples, and get the core sub-event sequence, and use pruning algorithm to construct the max heap. The designed triple encoding algorithm realizes max heap vectorization of event tree, and embedded deep forest algorithm to realize classification discrimination of judicial text event tree. The experimental results show that the proposed event tree construction method combined with the deep forest algorithm can greatly improve the logical interpretation and accuracy of judicial text."
9709290,Summarization of Text and Image Captioning in Information Retrieval Using Deep Learning Techniques,"Automated information retrieval and text summarization concept is a difficult process in natural language processing because of the infrequent structure and high complexity of the documents. The text summarization process creates a summary by paraphrasing a long text. Earlier models on information retrieval and summarization are based on a massive labeled dataset by the use of handcrafted features, leveraging on knowledge for a particular domain, and concentrated on the narrow sub-domain to improve efficiency. This paper presents a new deep learning (DL) based information retrieval with a text summarization model. The proposed model involves three major processes namely information retrieval, template generation, and text summarization. Initially, the bidirectional long short term memory (BiLSTM) approach is employed for retrieving the textual data, which assumes each word in a sentence, extracts the information, and embeds it into the semantic vector. Next, the template generation process takes place using the DL model. The deep belief network (DBN) model is employed as a text summarization tool to summarize the textual content. In addition, the image description is generated for the visualized entities that exist in the images. The design of BiLSTM with the DBN model for the text summarization and image captioning process shows the novelty of the work. The performance of the presented method is validated using Giga word corpus and DUC corpus. The experimental results referred that the proposed DBN model outperformed the compared methods with the maximum precision, recall and F-score. The image captions are compared with a predefined set of captions that exists for the image and the performance is evaluated using the BLEU metric."
9728674,Optimizing Global Representation Using Convolutional Bidirectional Recurrent Network Model for Text Categorization,"Deep learning is capable of achieving remarkable performance in sentence and document modelling. Recurrent Neural Network (RNN) is the mainstream architecture for text categorization. But RNN is a biased model of which later inputs are more dominant than earlier inputs. To optimizing global representation in RNN for document modelling, the convolutional bidirectional recurrent network (CBI-RNN) is introduced to text categorization. One convolutional layer and one max pooling layer are utilized to extract phrase level local information from word embedding. The BI-LSTM with global pooling is adopted to extract the global information for selecting features which are most favorable for classification. Depending upon the global pooling scheme utilized in the model, model variants are named CBI-RNN-Max and CBI-RNN-Att. Advanced sub-sequence representation learning is also introduced in the proposed model and its performance is reported in comparison with different model variations. The proposed model is applied to the text classification data set Reuters21578-R8, WebKB. Experimental results indicate that the proposed model captures more contextual information and achieves state-of -the-art performance on both datasets."
9206863,A Multi-Task Learning Approach to Improve Sentiment Analysis with Explicit Recommendation,"When expressing sentiment towards products, customers often explicitly indicate their recommendation status. Nevertheless, most existing literature focuses on sentiment analysis but neglects the rich correlation information that may be brought by explicit recommendation classification. We argue that the two tasks are correlated and hence, the knowledge in explicit recommendation classification can also be beneficial to sentiment analysis. Consequently, in this paper, a novel bidirectional encoder representations from transformers (BERT)-enhanced multi-task learning (BeMTL) approach is proposed to improve sentiment analysis with explicit recommendation classification. Specifically, the proposed MTL approach takes contextualized word embeddings produced by the pre-trained BERT-based embedding layer. Then, it learns the sentence contextual features shared between both tasks with a convolutional multi-head attention neural network. To fully exploit the correlation information between sentiment analysis and explicit recommendation classification tasks, a novel inter-task matching layer (IML) is designed to match their representations. In nutshell, our study reveals the potential of multi-task learning models on such types of problems, and experimental results on two Amazon datasets show that our approach outperforms the state-of-the-art baseline approaches for sentiment analysis."
9137480,Detecting and Classifying Humanitarian Crisis in Arabic Tweets,"Yemen and Syria are suffering from the worst humanitarian crisis in the world. Since 2016, 80% of the population in Yemen are dying from hunger, and 3,886 died from cholera. While since 2011, 65% of the Syrian population have become refugees. During these crises, people from both countries turned to Twitter to convey their crisis-related messages. Humanitarian organizations have realized the effectiveness of gathering, analyzing, and classifying tweets' contents to enhance their crisis rescue plan. However, most of the available crisis resources are either in the English language or cover hazards and natural disasters only. Also, there is a lack of knowledge of the most common terms used for crisis description by Arabic users. So, organizations found it difficult to gather, annotate, preprocess, extract features, and classifying Arabic crisis tweets content. As a result, there is a delay in responding to famine, cholera, and refugee crisis and a lot of loss in lives. The paper aims to proposed methodologies for extracting unique crisis terms, building annotation criteria, and enhancing classification for crisis-related messages in the Arabic language. Also, we produced a humanity crisis corpus for classifying tweets in Arabic. For that, we used keywords from each topic produced by the LDA model to collect crisis tweets. Then, we built crisis annotation criteria guided by a unique word list generated from word embedding models. Finally, we combined features from topics, words, and sentences then implemented by supervised methods for classification. Results indicate that our proposed methods enhance the classification model's performance. Besides, it increases the classifier's ability to detect more positive crisis classes to the right label. On the other hand, this paper provides humanitarian organizations with tools and methods for Arabic crisis-messages classification in social media and opens new opportunities for future studies in crisis management.
(Show More)"
9289599,Myanmar Text-to-Speech System based on Tacotron-2,"Myanmar is one of the developing countries situated in South-East Asia, and there are still many areas that have been under-developed with respect to advanced natural language processing technologies, where text-to-speech is one of them. The main motivation of this paper is to improve the naturalness of Myanmar text-to-speech system that is able to generate human-like speech. In this paper, we apply the neural network architecture based on Tacotron-2 that generates a mel spectrogram for speech synthesis directly from the sequence of text. Our proposed method is composed of three steps. In the first step, we create a speech corpus of 5k sentences of text and audio pair of Myanmar text from a large set of news articles, novel books, daily usages and travel-related expressions. We segment the Myanmar text into a sequence of characters by using a syllable segmenter and text normalizer. In the second step, we utilize the recurrent sequence-to-sequence feature prediction network that maps character embedding to mel-scale spectrograms. In the final step, we use Griffin-Lim algorithm to convert the corresponding text into generate Myanmar speech output. We compare our proposed method with an end-to-end generative model based on Tacotron. Furthermore, we investigate the subjective evaluation for both methods in speech synthesis by using mean opinion score (MOS). The experimental results show that our proposed method obtains an improvement over Tacotron based speech synthesis in terms of naturalness and intelligibility."
9671907,UGCLink: User Identity Linkage by Modeling User Generated Contents with Knowledge Distillation,"User identity linkage aims to link users with the same identities across different social networks. Recently, re- searchers model the similarities of users’ behaviors such as Point of Interests(PoIs) or User Generated Contents(UGCs) to predict the identities of users. However, it is non-trivial to solve the problem due to the following challenges: 1) PoIs are always sparse in the non-location-based social platforms, and it is impractical to measure the similarities of users solely with PoIs; 2) The similarities of hierarchical are hierarchical from the view of word, phrase, and sentence. How to model the hierarchical structure remains a key challenge; 3) The unreliable semantics of words. Two different words may refer to the same physical appearance of users, indicating that users are with the same identities.To tackle the above problems, we propose UGCLink, a knowledge distillation framework that models UGCs to predict user identities. Two main components are included in the framework, where the student network aims to model the similarities of UGCs and the teacher network guides the student network to learn better word embeddings that reveal the physical appearance of users. Besides, the teacher network, a document classification model that classifies UGCs into the categories of PoIs, is trained to guide the word embedding learning process in the student network to circumvent the unreliable semantic problem. We demonstrate that our proposed method outperforms the state- of-the-art methods by more than 11% in terms of AUC score."
9362099,Text Enhancement for Paragraph Processing in End-to-End Code-switching TTS,"Current end-to-end code-switching Text-to-Speech (TTS) can already generate high quality two languages speech in the same utterance with single speaker bilingual corpora. When the speakers of the bilingual corpora are different, the naturalness and consistency of the code-switching TTS will be poor. The cross-lingual embedding layers structure we proposed makes similar syllables in different languages relevant, thus improving the naturalness and consistency of generated speech. In the end-to-end code-switching TTS, there exists problem of prosody instability when synthesizing paragraph text. The text enhancement method we proposed makes the input contain prosodic information and sentence- level context information, thus improving the prosody stability of paragraph text. Experimental results demonstrate the effectiveness of the proposed methods in the naturalness, consistency, and prosody stability. In addition to Mandarin and English, we also apply these methods to Shanghaiese and Cantonese corpora, proving that the methods we proposed can be extended to other languages to build end-to-end code- switching TTS system."
9580124,To laugh or not to laugh – LSTM based humor detection approach,"Humor holds the power to turn any mundane conversation into something more enthralling. It is an important feature of personal communication. Sentiment Analysis helps people as well as corporations to comprehend the attitudes of people towards certain words based on how they are strung together. Particularly, Humor Detection facilitates the understanding of underlying relations of words and how various combinations of strings can invoke laughter. In this paper, an approach to identify the presence of humor in sentences, using a model based on Long Short-Term Memory (LSTM), is adopted. LSTM is a specialized version of Recurrent Neural Network (RNN). While conventional RNNs involve cyclic connections for modeling sequenced data, LSTM uses additional units for retaining information for comparatively long periods. A wide-ranging, evenly distributed dataset is used in the implementation of the model. After preprocessing, the data progresses along the embedding layer, LSTM layer and the dense layer and the outcome is finally compiled. Ultimately, the model states whether a given statement is humorous or not. The proposed LSTM model gives an accuracy of 94.62%. As new sources of obtaining humorous content keep emerging through multimedia, new patterns can be discovered. Understanding such patterns through such existing Humor Detection models can promote the development of Automated Humor Generation Systems."
8728539,Paraphrase Detection using Dependency Tree Recursive Autoencoder,"An architecture is proposed based on recursive autoencoder for paraphrase detection. The proposed architecture embeds the semantic information by using word representations generated from the neural network language model and the syntactic information by implementing the dependency tree over the recursive autoencoder, where dependency tree reveals the syntactic information of the given sentence in recursive form. The proposed architecture is tested on the MSRP dataset for paraphrase detection and the results are above the baseline. The proposed system reached a moderate accuracy and F1 score for the paraphrase detection test on MSRP dataset."
9106695,Correction to “Smart Urban Living: Enabling Emotion-Guided Interaction With Next Generation Sensing Fabric”,"In the above article [1] , there is a mistake in the grant number. The sentence “This work was supported by the Deanship of Scientific Research (DSR), King Abdulaziz University, Jeddah, under Grant DF-166-611-1441” should be corrected to “This work was supported by the Deanship of Scientific Research (DSR), King Abdulaziz University, Jeddah, under Grant DF-251-611-1441.”"
9096628,Corrections to “3D Printed Elastomeric Lattices With Embedded Deformation Sensing”,"In the above article [1] , Figure 3 should read “Isometric view of two wires forming a capacitor in the measured configuration. The yellow wire is the top plate and the red is the bottom plate of a capacitor. The gray is a dielectric elastomer lattice, the deformation of which can be indirectly determined by measuring the capacitance.” Figure 4 should read “Isometric (A) and bottom view (B) of a lattice with an alternative configuration in four quadrants for selective sensing. The selectivity can be extended to any combination of cells in both vertical and horizontal configurations.” In Figure 9 , the
x
-axis should read “Min Deformation Slow; Min Deformation Fast; Max Deformation Slow; Max Deformation Fast.” A sentence in the introduction should read “Within the context of additive manufacturing, lattices are the focus of significant research since they...” The authors requested this correction for clarity purposes."
9490944,Corrections to “Structural Integrity of 3-D Metal–Insulator–Metal Capacitor Embedded in Fully Filled Cu Through-Silicon Via”,"In the above article [1] , a new reference [19] must be added to the end of the last sentence of the second paragraph in Section I: “Recently, Cu TSV filling has been completed, and therefore making it a complete structure which contains both 3-D MIM capacitor and Cu TSV core [19].” The details of [19] are as follows:"
